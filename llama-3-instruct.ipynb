{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":13608691,"sourceType":"datasetVersion","datasetId":8647923},{"sourceId":627241,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":472263,"modelId":488160}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:59:13.347232Z","iopub.execute_input":"2025-11-02T12:59:13.347498Z","iopub.status.idle":"2025-11-02T12:59:13.353178Z","shell.execute_reply.started":"2025-11-02T12:59:13.347473Z","shell.execute_reply":"2025-11-02T12:59:13.352170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pickle\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nimport re\nimport json\nimport warnings\nimport csv\nwarnings.filterwarnings(\"ignore\")\n\n# è®¾å¤‡é…ç½®ï¼ˆè‡ªåŠ¨é€‚é…GPU/CPUï¼‰\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T10:00:24.262273Z","iopub.execute_input":"2025-11-04T10:00:24.262873Z","iopub.status.idle":"2025-11-04T10:00:35.682997Z","shell.execute_reply.started":"2025-11-04T10:00:24.262849Z","shell.execute_reply":"2025-11-04T10:00:35.682214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"/kaggle/input/llama3-8b-instruct/transformers/default/1/Meta-Llama-3-8B-Instruct\"\n\n# åŠ è½½Tokenizerå’ŒModel\ntokenizer = AutoTokenizer.from_pretrained(model_path)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    output_hidden_states=True\n)\nmodel.eval()\nprint(\"Model and Tokenizer loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T10:00:35.684327Z","iopub.execute_input":"2025-11-04T10:00:35.684734Z","iopub.status.idle":"2025-11-04T10:02:12.673916Z","shell.execute_reply.started":"2025-11-04T10:00:35.684713Z","shell.execute_reply":"2025-11-04T10:02:12.673255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"XP = \"If you enter a building from the east side, walking west, and then take two rights and a left down corridors, which direction are you facing?\"\n\ndef save_hidden_states(data_dict, filename=\"hidden_states_data.pkl\"):\n    \"\"\"ä¿å­˜æ•°æ®\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n    print(f\"âœ“ æ•°æ®å·²ä¿å­˜è‡³ {filename}\")\n\ndef build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    return prompt\n\nprint(\"âœ… Cell 1å®Œæˆï¼šåˆå§‹åŒ–è®¾ç½®\")\n\n# =====================================================\n# Cell 2: ä¿®å¤çš„æ­¥éª¤çº§hidden statesæå–\n# =====================================================\n\ndef generate_step_by_step_reasoning():\n    \"\"\"è®©æ¨¡å‹è¿›è¡Œåˆ†æ­¥æ¨ç†ï¼Œè®°å½•æ¯ä¸ªæ­¥éª¤çš„hidden states\"\"\"\n\n    step_by_step_prompt = build_llama3_prompt(\n        #\"You are a helpful AI assistant that solves reasoning problems step by step. You MUST output your reasoning in the exact format: Step 1: [reasoning], Step 2: [reasoning], etc. End with 'Final Answer: [direction]'\",\n        \"You are a helpful AI assistant that solves reasoning problems step by step.\",\n        f\"Problem: {XP}\\n\\nPlease solve this step by step and and provide the final answer:\"\n    )\n    \n    print(\"ç”Ÿæˆåˆ†æ­¥æ¨ç†è¿‡ç¨‹...\")\n    inputs = tokenizer(step_by_step_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            output_hidden_states=True,\n            return_dict_in_generate=True\n        )\n    \n    # è·å–å®Œæ•´å“åº”\n    full_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    assistant_response = full_response.split(\"assistant\")[-1].strip() if \"assistant\" in full_response else full_response\n    \n    print(f\"æ¨¡å‹åˆ†æ­¥æ¨ç†å“åº”:\\n{assistant_response}\")\n    \n    # æå–æ­¥éª¤çº§hidden states\n    step_data = extract_step_level_hidden_states(outputs, inputs.input_ids.shape[1], assistant_response)\n    \n    return assistant_response, step_data\n\ndef extract_step_level_hidden_states(outputs, prompt_length, generated_text):\n    \"\"\"ç²¾ç¡®æå–æ¯ä¸ªæ­¥éª¤çš„hidden states\"\"\"\n    \n    # è§£ææ­¥éª¤ç»“æ„\n    steps = parse_step_structure(generated_text)\n    \n    step_data = []\n    generated_ids = outputs.sequences[0, prompt_length:]  # åªå–ç”Ÿæˆçš„tokenï¼ˆæ’é™¤promptï¼‰\n    full_generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    \n    print(f\"å®Œæ•´ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(full_generated_text)} å­—ç¬¦\")\n    print(f\"ç”Ÿæˆtokenæ•°é‡: {len(generated_ids)}\")\n    # ã€å…³é”®ä¿®å¤1ã€‘hidden statesé•¿åº¦ = ç”Ÿæˆçš„tokenæ•°é‡ï¼ˆgenerated_idsçš„é•¿åº¦ï¼‰\n    total_hidden_length = len(generated_ids)  \n    print(f\"ç”Ÿæˆtokenæ€»é•¿åº¦ï¼ˆhidden stateså‚è€ƒï¼‰: {total_hidden_length}\")\n    \n    # è®¡ç®—æ¯ä¸ªæ­¥éª¤çš„ç²¾ç¡®tokenä½ç½®\n    current_position = 0\n    for step_num, step_text in steps.items():\n        step_text_clean = step_text.strip()\n        step_start = full_generated_text.find(step_text_clean, current_position)\n        \n        if step_start == -1:\n            full_text_clean = \" \".join(full_generated_text.split())\n            step_text_clean_no_space = \" \".join(step_text_clean.split())\n            step_start = full_text_clean.find(step_text_clean_no_space, current_position)\n            if step_start == -1:\n                print(f\"âŒ æ— æ³•æ‰¾åˆ°æ­¥éª¤ {step_num}: {step_text[:50]}...\")\n                continue\n        \n        # è®¡ç®—æ­¥éª¤æ–‡æœ¬çš„tokené•¿åº¦\n        step_tokens = tokenizer.encode(step_text_clean, add_special_tokens=False)\n        step_token_length = len(step_tokens)\n        \n        if step_token_length == 0:\n            print(f\"âŒ æ­¥éª¤ {step_num} æ— æ³•ç¼–ç ä¸ºtokens\")\n            continue\n        \n        # ã€å…³é”®ä¿®å¤2ã€‘æ­£ç¡®è®¡ç®—tokenä½ç½®ï¼ˆåŸºäºç”Ÿæˆçš„æ–‡æœ¬ï¼Œè€Œéæ•´ä¸ªpromptï¼‰\n        text_before_step = full_generated_text[:step_start]\n        tokens_before_step = tokenizer.encode(text_before_step, add_special_tokens=False)\n        step_start_token_idx = len(tokens_before_step)\n        step_end_token_idx = step_start_token_idx + step_token_length - 1\n        \n        # ã€å…³é”®ä¿®å¤3ã€‘å½»åº•è§£å†³ç»“æŸä½ç½®<å¼€å§‹ä½ç½®çš„é—®é¢˜ï¼ˆç›´æ¥èˆå¼ƒå¼‚å¸¸æ­¥éª¤ï¼Œé¿å…å¼ºåˆ¶ä¿®æ­£ï¼‰\n        if step_end_token_idx < step_start_token_idx:\n            print(f\"âŒ æ­¥éª¤ {step_num} tokenä½ç½®è®¡ç®—å¼‚å¸¸ï¼ˆç»“æŸ<å¼€å§‹ï¼‰ï¼Œè·³è¿‡è¯¥æ­¥éª¤\")\n            current_position = step_start + len(step_text_clean)\n            continue\n        \n        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºç”Ÿæˆçš„tokenèŒƒå›´ï¼ˆçœŸæ­£çš„è¶…å‡ºåˆ¤æ–­ï¼‰\n        if step_end_token_idx >= total_hidden_length:\n            print(f\"âš ï¸ æ­¥éª¤ {step_num} è¶…å‡ºç”ŸæˆtokenèŒƒå›´ï¼ˆ{step_end_token_idx} â‰¥ {total_hidden_length}ï¼‰ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªä½ç½®\")\n            step_end_token_idx = total_hidden_length - 1\n        \n        # è·å–æ­¥éª¤ç»“æŸä½ç½®çš„hidden stateï¼ˆæ³¨æ„ï¼šoutputs.hidden_statesåŒ…å«promptçš„tokenï¼Œéœ€åç§»prompt_lengthï¼‰\n        actual_hidden_idx = prompt_length + step_end_token_idx\n        if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n            actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n        step_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n        \n        step_data.append({\n            'step_number': step_num,\n            'step_text': step_text,\n            'hidden_state': step_hidden,\n            'token_start': step_start_token_idx,\n            'token_end': step_end_token_idx,\n            'token_length': step_token_length,\n            'text_position': step_start\n        })\n        \n        print(f\"  æå–æ­¥éª¤ {step_num}:\")\n        print(f\"    æ–‡æœ¬ä½ç½®: {step_start}-{step_start + len(step_text_clean)}\")\n        print(f\"    Tokenä½ç½®: {step_start_token_idx}-{step_end_token_idx}\")\n        print(f\"    Tokené•¿åº¦: {step_token_length}\")\n        print(f\"    æ–‡æœ¬é¢„è§ˆ: {step_text[:60]}...\")\n        \n        current_position = step_start + len(step_text_clean)\n    \n    # éªŒè¯æå–ç»“æœ\n    if step_data:\n        print(f\"\\nâœ“ æˆåŠŸæå– {len(step_data)} ä¸ªæ­¥éª¤çš„hidden states\")\n        for step in step_data:\n            print(f\"  æ­¥éª¤ {step['step_number']}: tokenä½ç½®={step['token_start']}-{step['token_end']}, hidden shape={step['hidden_state'].shape}\")\n    else:\n        print(\"âŒ æœªèƒ½æå–ä»»ä½•æ­¥éª¤çš„hidden states\")\n        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æœ€åä¸€ä¸ªtokençš„hidden state\n        if len(generated_ids) > 0:\n            actual_hidden_idx = prompt_length + len(generated_ids) - 1\n            if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n                actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n            last_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n            step_data.append({\n                'step_number': 'step_1',\n                'step_text': generated_text,\n                'hidden_state': last_hidden,\n                'token_start': 0,\n                'token_end': len(generated_ids) - 1,\n                'token_length': len(generated_ids),\n                'text_position': 0\n            })\n            print(\"âœ“ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šæ•´ä¸ªå“åº”ä½œä¸ºä¸€ä¸ªæ­¥éª¤\")\n    \n    return step_data\n\ndef parse_step_structure(text):\n    \"\"\"æ›´é²æ£’çš„æ­¥éª¤ç»“æ„è§£æï¼Œæ”¯æŒå¤šç§æ ¼å¼\"\"\"\n    steps = {}\n    \n    # æ”¯æŒå¤šç§æ­¥éª¤æ ¼å¼\n    step_patterns = [\n        # æ ¼å¼1: Step 1: å†…å®¹\n        r'(Step\\s*\\d+\\s*:)(.*?)(?=Step\\s*\\d+\\s*:|Final Answer:|$)',\n        # æ ¼å¼2: 1. å†…å®¹  \n        r'(\\d+\\.\\s*)(.*?)(?=\\d+\\.\\s*|Final Answer:|$)',\n        # æ ¼å¼3: æ•°å­—) å†…å®¹\n        r'(\\d+\\)\\s*)(.*?)(?=\\d+\\)\\s*|Final Answer:|$)',\n        # æ ¼å¼4: Final Answer\n        r'(Final Answer:\\s*)(.*?)(?=$)'\n    ]\n    \n    all_matches = []\n    for pattern in step_patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n        for match in matches:\n            # è®¡ç®—åŒ¹é…é¡¹åœ¨åŸæ–‡ä¸­çš„ä½ç½®\n            header, content = match[0], match[1]\n            start_pos = text.find(header)\n            if start_pos != -1:\n                all_matches.append((start_pos, header.strip(), content.strip()))\n    \n    # æŒ‰ä½ç½®æ’åº\n    all_matches.sort(key=lambda x: x[0])\n    \n    for i, (pos, header, content) in enumerate(all_matches):\n        if not content:\n            continue\n            \n        # ç¡®å®šæ­¥éª¤é”®\n        if \"final\" in header.lower() or \"answer\" in header.lower():\n            step_key = \"final\"\n        else:\n            step_key = f\"step_{i+1}\"\n        \n        # æ‹¼æ¥å®Œæ•´æ­¥éª¤æ–‡æœ¬\n        step_text = f\"{header} {content}\".strip()\n        steps[step_key] = step_text\n    \n    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ç»“æ„åŒ–çš„æ­¥éª¤ï¼Œä½¿ç”¨åŸºäºé€»è¾‘çš„å¯å‘å¼åˆ†å‰²\n    if len(steps) <= 1:  # åªæ‰¾åˆ°final answer\n        steps = heuristic_step_splitting(text)\n    \n    print(f\"è§£æå‡º {len(steps)} ä¸ªæ­¥éª¤:\")\n    for step_key, step_text in steps.items():\n        print(f\"  {step_key}: {step_text[:80]}...\")\n    \n    return steps\n\ndef heuristic_step_splitting(text):\n    \"\"\"å¯å‘å¼æ­¥éª¤åˆ†å‰²ï¼Œå½“æ­£åˆ™åŒ¹é…å¤±è´¥æ—¶ä½¿ç”¨\"\"\"\n    steps = {}\n    \n    # åŸºäºå¸¸è§æ¨ç†æ¨¡å¼åˆ†å‰²\n    lines = text.split('\\n')\n    step_count = 1\n    \n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n            \n        # æ£€æµ‹æ­¥éª¤å¼€å§‹çš„æ¨¡å¼\n        if (re.match(r'^\\d+\\.', line) or \n            re.match(r'^Step\\s*\\d+', line, re.I) or\n            re.match(r'^\\d+\\)', line) or\n            len(line) > 20 and any(keyword in line.lower() for keyword in ['first', 'then', 'next', 'after', 'finally'])):\n            \n            if 'final' not in line.lower() and 'answer' not in line.lower():\n                steps[f'step_{step_count}'] = line\n                step_count += 1\n            else:\n                steps['final'] = line\n    \n    # å¦‚æœè¿˜æ˜¯å¤ªå°‘ï¼Œå°è¯•æŒ‰å¥å­åˆ†å‰²\n    if len(steps) <= 1:\n        sentences = re.split(r'[.!?]+', text)\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if len(sentence) > 15 and not sentence.startswith(('So', 'Therefore', 'Thus')):\n                if step_count <= 4:  # æœ€å¤š4ä¸ªæ­¥éª¤\n                    steps[f'step_{step_count}'] = sentence\n                    step_count += 1\n    \n    return steps\n\nprint(\"âœ… Cell 2å®Œæˆï¼šä¿®å¤çš„æ­¥éª¤çº§hidden statesæå–\")\n\n# =====================================================\n# Cell 3: è·³æ­¥è¯†åˆ«å‡½æ•°\n# =====================================================\n\ndef identify_skipped_steps(step_by_step_response):\n    \"\"\"ä½¿ç”¨LLM Judgeè¯†åˆ«å…·ä½“è·³è¿‡çš„æ­¥éª¤ï¼ˆä¼˜åŒ–è¯¯åˆ¤ï¼‰\"\"\"\n    \n    expected_complete_reasoning = [\n        \"Step 1: Enter from east side, initially facing east, then start walking west (new direction: west).\",\n        \"Step 2: Take first right turn (from west â†’ south).\",\n        \"Step 3: Take second right turn (from south â†’ east).\", \n        \"Step 4: Take left turn (from east â†’ north).\",\n        \"Final Answer: north\"\n    ]\n    \n    judge_prompt = build_llama3_prompt(\n        \"You are an expert reasoning analyzer. Analyze if any LOGICAL CONTENT is skipped (not just formatting differences). Allow steps to be merged if all core logic is included. Only mark steps as skipped if critical direction change logic is missing.\",\n        f\"\"\"Problem: {XP}\n\nExpected Complete Reasoning (focus on CORE LOGIC, not exact formatting/step numbers):\n{chr(10).join(expected_complete_reasoning)}\n\nActual Reasoning:\n{step_by_step_response}\n\nRules for Analysis:\n1. If the actual reasoning includes ALL direction change logic (even in merged steps), mark SKIPPED_STEPS as \"none\".\n2. Only mark steps as skipped if specific direction change logic is missing.\n3. Ignore differences in step numbering, wording, or formatting.\n\nProvide your analysis in this exact format:\nSKIPPED_STEPS: [step numbers separated by commas, or \"none\" if no skip]\nSKIPPED_CONTENT: [description of missing logic, or \"none\" if no skip]\nSKIPPING_LOCATION: [between step X and step Y, or \"none\" if no skip]\"\"\"\n    )\n    \n    inputs = tokenizer(judge_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_return_sequences=1,\n            do_sample=False,\n            temperature=0.0,  # ç¡®å®šæ€§ç”Ÿæˆï¼Œé¿å…éšæœºè¯¯åˆ¤\n            return_dict_in_generate=True\n        )\n    \n    judge_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    judge_analysis = judge_response.split(\"assistant\")[-1].strip() if \"assistant\" in judge_response else judge_response\n    \n    print(f\"LLM Judgeåˆ†æç»“æœ:\\n{judge_analysis}\")\n    \n    # è§£æåˆ†æç»“æœ\n    skipping_info = parse_skipping_analysis(judge_analysis)\n    \n    return skipping_info, judge_analysis\n\ndef parse_skipping_analysis(analysis_text):\n    \"\"\"è§£æLLM Judgeçš„è·³æ­¥åˆ†æç»“æœ\"\"\"\n    skipping_info = {\n        'skipped_steps': [],\n        'skipped_content': '',\n        'skipping_location': '',\n        'from_step': None,\n        'to_step': None\n    }\n    \n    # æå–è·³è¿‡çš„æ­¥éª¤ç¼–å·\n    steps_match = re.search(r'SKIPPED_STEPS:\\s*([^\\n]+)', analysis_text)\n    if steps_match:\n        steps_str = steps_match.group(1)\n        # æå–æ•°å­—\n        step_numbers = re.findall(r'\\d+', steps_str)\n        skipping_info['skipped_steps'] = [f\"step_{num}\" for num in step_numbers]\n    \n    # æå–è·³è¿‡å†…å®¹\n    content_match = re.search(r'SKIPPED_CONTENT:\\s*([^\\n]+)', analysis_text)\n    if content_match:\n        skipping_info['skipped_content'] = content_match.group(1).strip()\n    \n    # æå–è·³è¿‡ä½ç½®\n    location_match = re.search(r'SKIPPING_LOCATION:\\s*([^\\n]+)', analysis_text)\n    if location_match:\n        location_str = location_match.group(1)\n        skipping_info['skipping_location'] = location_str.strip()\n        \n        # è§£æä»å“ªä¸ªæ­¥éª¤è·³åˆ°å“ªä¸ªæ­¥éª¤\n        step_matches = re.findall(r'step\\s*(\\d+)', location_str.lower())\n        if len(step_matches) >= 2:\n            skipping_info['from_step'] = f\"step_{step_matches[0]}\"\n            skipping_info['to_step'] = f\"step_{step_matches[1]}\"\n    \n    return skipping_info\n\nprint(\"âœ… Cell 3å®Œæˆï¼šè·³æ­¥è¯†åˆ«å‡½æ•°å®šä¹‰\")\n\n# =====================================================\n# Cell 4: LLMå¯¹æ¯”è·¯å¾„ç”Ÿæˆ\n# =====================================================\n\ndef generate_comparison_paths_with_llm(skipping_info, step_data, tokenizer, model):\n    \"\"\"\n    âœ… æ§åˆ¶å˜é‡ç‰ˆï¼šåªåœ¨ LLM Judge åˆ¤æ–­ä¸ºâ€œæœ‰è·³æ­¥â€æ—¶å·¥ä½œ\n      - å›ºå®šä¸Šä¸‹æ–‡ï¼ˆå‰ N-1 æ­¥ï¼‰\n      - ä»…ç”Ÿæˆè¢«è·³è¿‡é‚£ä¸€æ­¥çš„ 3 é£æ ¼ Ã— 3 æ¬¡\n      - ä¸åŸå§‹â€œè·³æ­¥åâ€ä¸€æ­¥ hidden state åš cosine ç›¸ä¼¼åº¦\n      - è¿”å› mean Â± std\n    \"\"\"\n    if not skipping_info.get('skipped_steps'):\n        return {\"status\": \"no_skip\"}\n\n    skipped_step_key = skipping_info['skipped_steps'][0]          # ä¾‹ \"step_3\"\n    skipped_num = int(skipped_step_key.split('_')[1])\n    target_step_key = f\"step_{skipped_num + 1}\"                  # è·³æ­¥åçœŸå®è¾“å‡ºçš„é‚£ä¸€æ­¥\n\n    # 1. æ„å»ºå‰ N-1 æ­¥ä¸Šä¸‹æ–‡\n    context_lines = []\n    for step in step_data:\n        if step['step_number'] == skipped_step_key:\n            break\n        context_lines.append(step['step_text'])\n    context_text = \"\\n\".join(context_lines)\n\n    # 2. æå–å¯¹æ¯”ç›®æ ‡ hidden state\n    target_hidden = None\n    for step in step_data:\n        if step['step_number'] == target_step_key:\n            target_hidden = step['hidden_state']\n            break\n    if target_hidden is None:\n        return {\"status\": \"target_not_found\"}\n\n    # 3. ä¸‰ç§é£æ ¼æç¤º\n    style_prompts = {\n        \"detailed\": \"Continue with a detailed reasoning step:\",\n        \"shortcut\": \"Give only the direct result without reasoning:\",\n        \"alternative\": \"Provide an alternative concise reasoning step:\"\n    }\n\n    results = {\"status\": \"ok\", \"similarities\": {}}\n\n    # 4. æ¯é£æ ¼ 3 æ¬¡ç”Ÿæˆ\n    for style, prompt_suffix in style_prompts.items():\n        sims = []\n        for seed in [42, 43, 44]:\n            user_msg = f\"{context_text}\\n\\n{prompt_suffix}\"\n            full_prompt = build_llama3_prompt(\"You are a helpful assistant.\", user_msg)\n\n            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=50,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    seed=seed,\n                    output_hidden_states=True,\n                    return_dict_in_generate=True\n                )\n\n            # æœ€åç”Ÿæˆ token çš„ hidden state\n            last_hidden = outputs.hidden_states[-1][-1][0, -1, :].cpu().numpy()\n            sim = cosine_similarity(\n                target_hidden.reshape(1, -1),\n                last_hidden.reshape(1, -1)\n            )[0][0]\n            sims.append(float(sim))\n\n        results[\"similarities\"][style] = {\n            \"mean\": float(np.mean(sims)),\n            \"std\": float(np.std(sims)),\n            \"all\": sims\n        }\n\n    return results\n\ndef generate_llm_reasoning_path(style, context, skipped_content):\n    \"\"\"ä½¿ç”¨LLMç”ŸæˆæŒ‡å®šé£æ ¼çš„å®Œæ•´æ¨ç†è·¯å¾„\"\"\"\n    \n    if style == \"detailed\":\n        system_msg = \"You are a detailed reasoning assistant. Provide complete, logical steps for direction reasoning.\"\n        user_msg = f\"\"\"Based on this context: \"{context}\"\n\nThe skipped step was: \"{skipped_content}\"\n\nPlease provide a COMPLETE and DETAILED reasoning path that includes all logical steps. Format as:\nStep X: [reasoning]\nStep Y: [reasoning]\n...\"\"\"\n    \n    elif style == \"alternative\":\n        system_msg = \"You are an AI assistant. Provide an alternative complete reasoning path.\"\n        user_msg = f\"\"\"Context: \"{context}\"\nSkipped: \"{skipped_content}\"\n\nProvide an ALTERNATIVE but equally valid complete reasoning path with all steps:\"\"\"\n    \n    else:  # step_by_step\n        system_msg = \"You are a methodical reasoning assistant.\"\n        user_msg = f\"\"\"Given: \"{context}\"\nMissing: \"{skipped_content}\"\n\nProvide a thorough step-by-step reasoning path with no jumps:\"\"\"\n    \n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    reasoning_path = parse_llm_reasoning_steps(response)\n    \n    print(f\"  LLMç”Ÿæˆ{style}è·¯å¾„: {len(reasoning_path)}ä¸ªæ­¥éª¤\")\n    return reasoning_path\n\ndef generate_llm_shortcut_path(context, skipped_content):\n    \"\"\"ä½¿ç”¨LLMç”Ÿæˆçº¯è·³æ­¥è·¯å¾„\"\"\"\n    \n    system_msg = \"You are a concise AI assistant that provides direct answers without reasoning.\"\n    user_msg = f\"\"\"Based on: \"{context}\"\n\nProvide ONLY the final conclusion without any intermediate reasoning steps. Be direct and jump to the answer.\"\"\"\n    \n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    shortcut_path = [f\"Direct jump: {response.split('assistant')[-1].strip()}\"]\n    \n    print(f\"  LLMç”Ÿæˆçº¯è·³æ­¥è·¯å¾„: {shortcut_path[0][:50]}...\")\n    return shortcut_path\n\ndef parse_llm_reasoning_steps(response_text):\n    \"\"\"è§£æLLMç”Ÿæˆçš„æ¨ç†æ­¥éª¤ï¼ˆå»é‡ï¼Œé¿å…é‡å¤è®¡æ•°ï¼‰\"\"\"\n    \n    # æå–assistantçš„å“åº”\n    if \"assistant\" in response_text:\n        reasoning_text = response_text.split(\"assistant\")[-1].strip()\n    else:\n        reasoning_text = response_text\n    \n    steps = []\n    # ã€å…³é”®ä¿®å¤1ã€‘åªä¿ç•™ç»“æ„åŒ–çš„Stepæ ¼å¼ï¼Œè¿‡æ»¤é¡¹ç›®ç¬¦å·ï¼ˆé¿å…é‡å¤è§£æï¼‰\n    # æ¨¡å¼1: Step X: æ ¼å¼ï¼ˆä¼˜å…ˆä¿ç•™ï¼‰\n    step_pattern1 = r'(Step\\s*\\d+\\s*:)(.*?)(?=Step\\s*\\d+\\s*:|$)'\n    matches1 = re.findall(pattern1, reasoning_text, re.IGNORECASE | re.DOTALL)\n    \n    # æ¨¡å¼2: æ•°å­—. æ ¼å¼ï¼ˆæ¬¡è¦ä¿ç•™ï¼Œé¿å…é—æ¼ï¼‰\n    step_pattern2 = r'(\\d+)\\.\\s*([^\\n]*(?=\\n\\d+\\.|\\nStep|$))'\n    matches2 = re.findall(step_pattern2, reasoning_text, re.DOTALL)\n    \n    # ä¼˜å…ˆå¤„ç†Stepæ ¼å¼\n    if matches1:\n        for num, content in matches1:\n            step_content = content.strip()\n            if step_content and step_content not in [s.split(':')[-1].strip() for s in steps]:\n                steps.append(f\"{num.strip()} {step_content}\")\n    # å†å¤„ç†æ•°å­—.æ ¼å¼ï¼ˆå»é‡ï¼‰\n    elif matches2:\n        for num, content in matches2:\n            step_content = content.strip()\n            if step_content and step_content not in [s.split('.')[-1].strip() for s in steps]:\n                steps.append(f\"Step {num}: {step_content}\")\n    # å…œåº•ï¼šåªä¿ç•™éç©ºã€éé‡å¤çš„æœ‰æ•ˆæ­¥éª¤\n    else:\n        lines = reasoning_text.split('\\n')\n        for line in lines:\n            line = line.strip()\n            # è¿‡æ»¤é¡¹ç›®ç¬¦å·ã€ç©ºè¡Œã€çŸ­è¡Œ\n            if line and len(line) > 15 and not line.startswith(('*', '-', 'â€¢')):\n                if line not in steps:\n                    steps.append(line)\n    \n    # æœ€ç»ˆå»é‡ï¼ˆç¡®ä¿æ— é‡å¤æ­¥éª¤ï¼‰\n    unique_steps = []\n    seen_content = set()\n    for step in steps:\n        clean_content = \" \".join(step.lower().split())\n        if clean_content not in seen_content:\n            seen_content.add(clean_content)\n            unique_steps.append(step)\n    \n    return unique_steps\n\ndef generate_default_comparison_paths_with_llm():\n    \"\"\"å½“æ— æ³•ç¡®å®šè·³æ­¥ä½ç½®æ—¶ï¼Œç”¨LLMç”Ÿæˆé»˜è®¤å¯¹æ¯”è·¯å¾„\"\"\"\n    \n    print(\"ä½¿ç”¨LLMç”Ÿæˆé»˜è®¤å¯¹æ¯”è·¯å¾„...\")\n    \n    system_msg = \"You are an expert at direction reasoning problems.\"\n    user_msg = f\"\"\"Problem: {XP}\n\nPlease provide:\n1. A detailed complete reasoning path\n2. An alternative complete reasoning path  \n3. A direct shortcut answer without reasoning\n\nFormat as:\nDETAILED: [steps]\nALTERNATIVE: [steps]\nSHORTCUT: [answer]\"\"\"\n    \n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    llm_response = response.split(\"assistant\")[-1].strip() if \"assistant\" in response else response\n    \n    # è§£æLLMçš„å“åº”\n    paths = parse_default_llm_response(llm_response)\n    \n    return paths\n\ndef parse_default_llm_response(response_text):\n    \"\"\"è§£æLLMç”Ÿæˆçš„é»˜è®¤è·¯å¾„å“åº”\"\"\"\n    \n    paths = {\n        'full_reasoning_path_a': [],\n        'full_reasoning_path_b': [], \n        'pure_shortcut_path': [],\n        'skipped_content': \"unknown steps\",\n        'jumping_context': \"default context\"\n    }\n    \n    # å°è¯•è§£æä¸åŒéƒ¨åˆ†\n    lines = response_text.split('\\n')\n    \n    detailed_steps = []\n    alternative_steps = []\n    shortcut_answer = []\n    \n    current_section = None\n    \n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n            \n        if 'DETAILED:' in line.upper():\n            current_section = 'detailed'\n            line = line.split(':', 1)[-1].strip()\n        elif 'ALTERNATIVE:' in line.upper():\n            current_section = 'alternative'  \n            line = line.split(':', 1)[-1].strip()\n        elif 'SHORTCUT:' in line.upper():\n            current_section = 'shortcut'\n            line = line.split(':', 1)[-1].strip()\n        \n        if current_section == 'detailed' and line:\n            detailed_steps.append(line)\n        elif current_section == 'alternative' and line:\n            alternative_steps.append(line)\n        elif current_section == 'shortcut' and line:\n            shortcut_answer.append(line)\n    \n    # è®¾ç½®è·¯å¾„\n    paths['full_reasoning_path_a'] = detailed_steps if detailed_steps else [\"Step 1: Default detailed reasoning\"]\n    paths['full_reasoning_path_b'] = alternative_steps if alternative_steps else [\"Step 1: Default alternative reasoning\"]\n    paths['pure_shortcut_path'] = shortcut_answer if shortcut_answer else [\"Direct answer: north\"]\n    \n    return paths\n\nprint(\"âœ… Cell 4å®Œæˆï¼šLLMå¯¹æ¯”è·¯å¾„ç”Ÿæˆå‡½æ•°å®šä¹‰\")\n\n# =====================================================\n# Cell 5: æ”¹è¿›çš„è·¯å¾„hidden statesç”Ÿæˆå’Œç›¸ä¼¼åº¦åˆ†æ\n# =====================================================\n\ndef generate_paths_hidden_states(comparison_paths, skipping_info):\n    \"\"\"ä¸ºæ‰€æœ‰å¯¹æ¯”è·¯å¾„ç”Ÿæˆhidden states - æ”¹è¿›ç‰ˆæœ¬\"\"\"\n    \n    system_message = \"You are a helpful AI assistant that reasons about direction problems.\"\n    path_hidden_states = {}\n    \n    for path_name, path_steps in comparison_paths.items():\n        if path_name == 'skipped_content' or path_name == 'jumping_context':\n            continue\n            \n        print(f\"\\nç”Ÿæˆè·¯å¾„: {path_name}\")\n        \n        if isinstance(path_steps, list):\n            # ä¸ºè·¯å¾„ä¸­çš„æ¯ä¸ªæ­¥éª¤ç”Ÿæˆhidden states\n            step_hidden_states = []\n            \n            for step_idx, step_text in enumerate(path_steps):\n                # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡\n                previous_steps = \" \".join(path_steps[:step_idx])\n                if previous_steps:\n                    context = f\"{XP}\\n\\n{previous_steps}\\n\\nContinue: {step_text}\"\n                else:\n                    context = f\"{XP}\\n\\n{step_text}\"\n                \n                user_message = f\"Please continue this reasoning:\\n{context}\"\n                full_prompt = build_llama3_prompt(system_message, user_message)\n                \n                inputs = tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        max_new_tokens=10,  # é€‚å½“å¢åŠ ä»¥è·å–ç¨³å®šçš„hidden state\n                        num_return_sequences=1,\n                        do_sample=False,\n                        temperature=0.0,\n                        output_hidden_states=True,\n                        return_dict_in_generate=True\n                    )\n                \n                # ä½¿ç”¨ç²¾ç¡®çš„hidden stateæå–\n                generated_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n                assistant_response = generated_text.split(\"assistant\")[-1].strip() if \"assistant\" in generated_text else generated_text\n                \n                # æå–æœ€åä¸€ä¸ªtokençš„hidden state\n                last_hidden = outputs.hidden_states[-1][-1][0, -1, :].cpu().numpy()\n                step_hidden_states.append(last_hidden)\n                \n                print(f\"  æ­¥éª¤ {step_idx+1}: {step_text[:50]}...\")\n                print(f\"    Hidden stateå½¢çŠ¶: {last_hidden.shape}\")\n            \n            path_hidden_states[path_name] = step_hidden_states\n    \n    return path_hidden_states\n\ndef calculate_similarity_analysis(skipping_step_hidden, path_hidden_states, step_data, skipping_info):\n    \"\"\"æ”¹è¿›çš„ç›¸ä¼¼åº¦åˆ†æ\"\"\"\n    \n    similarity_results = {}\n    \n    # æ‰¾åˆ°è·³æ­¥ä½ç½®çš„hidden state\n    target_hidden = None\n    target_step_name = skipping_info.get('to_step', 'step_3')\n    \n    for step in step_data:\n        if step['step_number'] == target_step_name:\n            target_hidden = step['hidden_state']\n            break\n    \n    if target_hidden is None and step_data:\n        # ä½¿ç”¨æœ€åä¸€ä¸ªæ­¥éª¤ä½œä¸ºç›®æ ‡\n        target_hidden = step_data[-1]['hidden_state']\n        target_step_name = step_data[-1]['step_number']\n    \n    if target_hidden is None:\n        print(\"âŒ æ— æ³•æ‰¾åˆ°ç›®æ ‡hidden state\")\n        return similarity_results\n    \n    print(f\"\\nğŸ”¬ ç›¸ä¼¼åº¦åˆ†æ (ç›®æ ‡æ­¥éª¤: {target_step_name})\")\n    print(f\"ç›®æ ‡hidden stateå½¢çŠ¶: {target_hidden.shape}\")\n    \n    # è®¡ç®—ä¸æ¯ä¸ªè·¯å¾„çš„ç›¸ä¼¼åº¦\n    for path_name, path_hs_list in path_hidden_states.items():\n        if not path_hs_list:\n            continue\n            \n        path_similarities = []\n        \n        # å¯¹è·¯å¾„ä¸­çš„æ¯ä¸ªæ­¥éª¤éƒ½è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå–æœ€é«˜å€¼\n        for path_hs in path_hs_list:\n            if path_hs.shape != target_hidden.shape:\n                print(f\"âš ï¸ å½¢çŠ¶ä¸åŒ¹é…: {path_hs.shape} vs {target_hidden.shape}\")\n                continue\n                \n            similarity = cosine_similarity(\n                target_hidden.reshape(1, -1),\n                path_hs.reshape(1, -1)\n            )[0][0]\n            path_similarities.append(similarity)\n        \n        if path_similarities:\n            similarity_results[path_name] = {\n                'mean_similarity': np.mean(path_similarities),\n                'max_similarity': np.max(path_similarities),\n                'min_similarity': np.min(path_similarities),\n                'all_similarities': path_similarities,\n                'best_step_similarity': np.max(path_similarities)\n            }\n            \n            print(f\"  {path_name}:\")\n            print(f\"    å¹³å‡ç›¸ä¼¼åº¦ = {np.mean(path_similarities):.4f}\")\n            print(f\"    æœ€é«˜ç›¸ä¼¼åº¦ = {np.max(path_similarities):.4f}\")\n            print(f\"    æœ€ä½ç›¸ä¼¼åº¦ = {np.min(path_similarities):.4f}\")\n        else:\n            print(f\"  {path_name}: æ— æ³•è®¡ç®—ç›¸ä¼¼åº¦\")\n    \n    return similarity_results\n\nprint(\"âœ… Cell 5å®Œæˆï¼šæ”¹è¿›çš„ç›¸ä¼¼åº¦åˆ†æå‡½æ•°å®šä¹‰\")\n\n# =====================================================\n# Cell 6: ä¸»å®éªŒæµç¨‹ï¼ˆæ–°ç‰ˆï¼‰\n# =====================================================\ndef main_experiment():\n    print(\"=\" * 70)\n    print(\"æ­¥éª¤çº§åˆ«éšå¼æ¨ç†åˆ†æå®éªŒ - å®Œæ•´æµç¨‹\")\n    print(\"=\" * 70)\n\n    # é˜¶æ®µ1: ç”Ÿæˆåˆ†æ­¥æ¨ç†\n    print(\"\\nğŸ“ é˜¶æ®µ1: ç”Ÿæˆåˆ†æ­¥æ¨ç†\")\n    step_by_step_response, step_data = generate_step_by_step_reasoning()\n\n    # é˜¶æ®µ2: è¯†åˆ«è·³æ­¥ä½ç½®\n    print(\"\\nğŸ” é˜¶æ®µ2: è¯†åˆ«è·³æ­¥ä½ç½®\")\n    skipping_info, judge_analysis = identify_skipped_steps(step_by_step_response)\n\n    # é˜¶æ®µ3: æ§åˆ¶å˜é‡å¯¹æ¯”è·¯å¾„ï¼ˆä»…è·³æ­¥æ—¶è§¦å‘ï¼‰\n    print(\"\\nğŸ›£ï¸  é˜¶æ®µ3: æ§åˆ¶å˜é‡å¯¹æ¯”è·¯å¾„ï¼ˆä»…è·³æ­¥æ—¶è§¦å‘ï¼‰\")\n    comparison_results = generate_comparison_paths_with_llm(\n        skipping_info, step_data, tokenizer, model\n    )\n\n    if comparison_results[\"status\"] == \"no_skip\":\n        print(\"âœ… LLM Judge åˆ¤å®šæ— è·³æ­¥ï¼Œè·³è¿‡å¯¹æ¯”é˜¶æ®µ\")\n        comparison_paths = {}\n        similarity_results = {}\n    else:\n        print(\"ğŸ” å­˜åœ¨è·³æ­¥ï¼Œç”Ÿæˆæ§åˆ¶å˜é‡å¯¹æ¯”è·¯å¾„\")\n        comparison_paths = {}          # æœ¬ç‰ˆä¸å†éœ€å‰é¢æ—§ç‰ˆå­—å…¸\n        similarity_results = comparison_results[\"similarities\"]\n        for style, stats in similarity_results.items():\n            print(f\"  {style}: mean={stats['mean']:.4f} Â± {stats['std']:.4f}\")\n\n    # é˜¶æ®µ4: ç”Ÿæˆè·¯å¾„ hidden statesï¼ˆä»…æ—§ç‰ˆéœ€è¦ï¼Œæ–°ç‰ˆç•™ç©ºï¼‰\n    print(\"\\nğŸ§  é˜¶æ®µ4: ç”Ÿæˆè·¯å¾„hidden states\")\n    path_hidden_states = {}          # å ä½ï¼Œä¸å†ä½¿ç”¨\n\n    # é˜¶æ®µ5: ç›¸ä¼¼åº¦åˆ†æï¼ˆæ–°ç‰ˆç›´æ¥å–è‡ª comparison_resultsï¼‰\n    print(\"\\nğŸ“Š é˜¶æ®µ5: ç›¸ä¼¼åº¦åˆ†æ\")\n    # å·²åœ¨ä¸Šä¸€æ­¥å®Œæˆ\n\n    # ä¿å­˜å®Œæ•´ç»“æœ\n    complete_results = {\n        'XP': XP,\n        'step_by_step_response': step_by_step_response,\n        'step_data': step_data,\n        'skipping_info': skipping_info,\n        'judge_analysis': judge_analysis,\n        'comparison_results': comparison_results,\n        'similarity_results': similarity_results\n    }\n    save_hidden_states(complete_results, \"complete_skipping_analysis.pkl\")\n\n    # è‹¥å­˜åœ¨è·³æ­¥ï¼Œé¢å¤–ä¿å­˜è·³æ­¥ä½ç½® hidden stateï¼ˆä¾›æ–¹æ³•ä¸‰ä½¿ç”¨ï¼‰\n    if comparison_results[\"status\"] == \"ok\":\n        target_key = f\"step_{int(skipping_info['skipped_steps'][0].split('_')[1]) + 1}\"\n        for step in step_data:\n            if step['step_number'] == target_key:\n                jump_step_hidden = step['hidden_state']\n                save_hidden_states({\n                    'hs_jump': jump_step_hidden,\n                    'jump_step': target_key,\n                    'XP': XP,\n                    'skipping_info': skipping_info\n                }, \"method1_jump_hidden_state.pkl\")\n                print(f\"âœ“ è·³æ­¥ hidden state å·²ä¿å­˜ï¼Œå½¢çŠ¶: {jump_step_hidden.shape}\")\n                break\n\n    # æœ€ç»ˆæ§åˆ¶å°æ‘˜è¦\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ğŸ¯ æœ€ç»ˆåˆ†æç»“æœ\")\n    print(\"=\" * 70)\n    if comparison_results[\"status\"] == \"no_skip\":\n        print(\"âœ… æ— è·³æ­¥ï¼Œå®éªŒç»“æŸ\")\n    else:\n        print(f\"è·³æ­¥ä½ç½®: {skipping_info.get('skipping_location', 'unknown')}\")\n        print(f\"è·³è¿‡å†…å®¹: {skipping_info.get('skipped_content', 'unknown')}\")\n        print(\"\\næ§åˆ¶å˜é‡ç›¸ä¼¼åº¦ï¼ˆmean Â± stdï¼‰ï¼š\")\n        for style, stats in similarity_results.items():\n            print(f\"  {style}: {stats['mean']:.4f} Â± {stats['std']:.4f}\")\n        best_style = max(similarity_results.items(), key=lambda x: x[1]['mean'])\n        print(f\"\\nğŸ’¡ ç»“è®ºï¼šè·³æ­¥å¤„å†…éƒ¨æ¨ç†æœ€æ¥è¿‘ '{best_style[0]}' é£æ ¼\")\n    return complete_results\n\nprint(\"ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´å®éªŒ...\")\nresults = main_experiment()\nprint(\"\\nğŸ‰ å®éªŒå®Œæˆï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T08:40:44.768063Z","iopub.execute_input":"2025-11-04T08:40:44.768721Z","iopub.status.idle":"2025-11-04T08:41:00.276975Z","shell.execute_reply.started":"2025-11-04T08:40:44.768697Z","shell.execute_reply":"2025-11-04T08:41:00.276307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\ncsv_path = \"/kaggle/input/testset/testset.csv\"\nif not os.path.exists(csv_path):\n    print(f\"æ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„ï¼š{csv_path}\")\n    print(\"å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨ï¼š\", os.listdir(\"/kaggle/input/testset\"))  # æ‰“å°æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶\nelse:\n    print(\"æ–‡ä»¶å­˜åœ¨ï¼Œå¯ä»¥æ­£å¸¸åŠ è½½\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T09:13:57.272399Z","iopub.execute_input":"2025-11-04T09:13:57.272859Z","iopub.status.idle":"2025-11-04T09:13:57.277993Z","shell.execute_reply.started":"2025-11-04T09:13:57.272835Z","shell.execute_reply":"2025-11-04T09:13:57.277425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### testset","metadata":{}},{"cell_type":"code","source":"def save_hidden_states(data_dict, filename=\"hidden_states_data.pkl\"):\n    \"\"\"ä¿å­˜æ•°æ®\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n\ndef build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n# =====================================================\n# è¯»å–CSVæµ‹è¯•é›†å‡½æ•°\n# =====================================================\ndef load_testset(csv_path=\"/kaggle/input/testset/testset.csv\"):\n    \"\"\"è¯»å–CSVæ ¼å¼çš„æµ‹è¯•é›†\"\"\"\n    testset = []\n    with open(csv_path, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            expected_steps = row['expected_complete_reasoning'].split('||')\n            testset.append({\n                'id': row['id'],\n                'problem': row['problem'],\n                'expected_complete_reasoning': expected_steps\n            })\n    print(f\"âœ“ æˆåŠŸåŠ è½½ {len(testset)} ä¸ªæµ‹è¯•é—®é¢˜\")\n    return testset\n\n# =====================================================\n# æ­¥éª¤çº§hidden statesæå–\n# =====================================================\ndef extract_step_level_hidden_states(outputs, prompt_length, generated_text):\n    steps = parse_step_structure(generated_text)\n    step_data = []\n    generated_ids = outputs.sequences[0, prompt_length:]\n    full_generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    total_hidden_length = len(generated_ids)\n    \n    current_position = 0\n    for step_num, step_text in steps.items():\n        step_text_clean = step_text.strip()\n        step_start = full_generated_text.find(step_text_clean, current_position)\n        \n        if step_start == -1:\n            full_text_clean = \" \".join(full_generated_text.split())\n            step_text_clean_no_space = \" \".join(step_text_clean.split())\n            step_start = full_text_clean.find(step_text_clean_no_space, current_position)\n            if step_start == -1:\n                continue\n        \n        step_tokens = tokenizer.encode(step_text_clean, add_special_tokens=False)\n        step_token_length = len(step_tokens)\n        if step_token_length == 0:\n            continue\n        \n        text_before_step = full_generated_text[:step_start]\n        tokens_before_step = tokenizer.encode(text_before_step, add_special_tokens=False)\n        step_start_token_idx = len(tokens_before_step)\n        step_end_token_idx = step_start_token_idx + step_token_length - 1\n        \n        if step_end_token_idx < step_start_token_idx:\n            current_position = step_start + len(step_text_clean)\n            continue\n        \n        if step_end_token_idx >= total_hidden_length:\n            step_end_token_idx = total_hidden_length - 1\n        \n        actual_hidden_idx = prompt_length + step_end_token_idx\n        if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n            actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n        step_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n        \n        step_data.append({\n            'step_number': step_num,\n            'step_text': step_text,\n            'hidden_state': step_hidden,\n            'token_start': step_start_token_idx,\n            'token_end': step_end_token_idx,\n            'token_length': step_token_length,\n            'text_position': step_start\n        })\n        \n        current_position = step_start + len(step_text_clean)\n    \n    if not step_data and len(generated_ids) > 0:\n        actual_hidden_idx = prompt_length + len(generated_ids) - 1\n        if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n            actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n        last_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n        step_data.append({\n            'step_number': 'step_1',\n            'step_text': generated_text,\n            'hidden_state': last_hidden,\n            'token_start': 0,\n            'token_end': len(generated_ids) - 1,\n            'token_length': len(generated_ids),\n            'text_position': 0\n        })\n    \n    return step_data\n\ndef parse_step_structure(text):\n    steps = {}\n    step_patterns = [\n        r'(Step\\s*\\d+\\s*:)(.*?)(?=Step\\s*\\d+\\s*:|Final Answer:|$)',\n        r'(\\d+\\.\\s*)(.*?)(?=\\d+\\.\\s*|Final Answer:|$)',\n        r'(\\d+\\)\\s*)(.*?)(?=\\d+\\)\\s*|Final Answer:|$)',\n        r'(Final Answer:\\s*)(.*?)(?=$)'\n    ]\n    \n    all_matches = []\n    for pattern in step_patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n        for match in matches:\n            header, content = match[0], match[1]\n            start_pos = text.find(header)\n            if start_pos != -1:\n                all_matches.append((start_pos, header.strip(), content.strip()))\n    \n    all_matches.sort(key=lambda x: x[0])\n    \n    for i, (pos, header, content) in enumerate(all_matches):\n        if not content:\n            continue\n        if \"final\" in header.lower() or \"answer\" in header.lower():\n            step_key = \"final\"\n        else:\n            step_key = f\"step_{i+1}\"\n        step_text = f\"{header} {content}\".strip()\n        steps[step_key] = step_text\n    \n    if len(steps) <= 1:\n        steps = heuristic_step_splitting(text)\n    \n    return steps\n\ndef heuristic_step_splitting(text):\n    steps = {}\n    lines = text.split('\\n')\n    step_count = 1\n    \n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        if (re.match(r'^\\d+\\.', line) or \n            re.match(r'^Step\\s*\\d+', line, re.I) or\n            re.match(r'^\\d+\\)', line) or\n            len(line) > 20 and any(keyword in line.lower() for keyword in ['first', 'then', 'next', 'after', 'finally'])):\n            if 'final' not in line.lower() and 'answer' not in line.lower():\n                steps[f'step_{step_count}'] = line\n                step_count += 1\n            else:\n                steps['final'] = line\n    \n    if len(steps) <= 1:\n        sentences = re.split(r'[.!?]+', text)\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if len(sentence) > 15 and not sentence.startswith(('So', 'Therefore', 'Thus')):\n                if step_count <= 4:\n                    steps[f'step_{step_count}'] = sentence\n                    step_count += 1\n    \n    return steps\n\n# =====================================================\n# è·³æ­¥è¯†åˆ«å‡½æ•°\n# =====================================================\ndef identify_skipped_steps(step_by_step_response, problem, expected_complete_reasoning):\n    judge_prompt = build_llama3_prompt(\n        \"You are a strict reasoning analyzer. ONLY focus on comparing actual reasoning with expected core logic step by step. Missing any expected core logic = skipped step. No lenient judgment.\",\n        f\"\"\"Problem: {problem}\n\nExpected Complete Reasoning (EACH STEP'S CORE LOGIC IS MANDATORY):\n{chr(10).join([f\"Expected Step {i+1}: {step.strip()}\" for i, step in enumerate(expected_complete_reasoning)])}\n\nActual Reasoning:\n{step_by_step_response}\n\nStrict Rules:\n1. Compare actual reasoning with EVERY expected step's core logic (ignore wording/format differences ONLY if core logic is fully included).\n2. If ANY expected core logic is not found in actual reasoning â†’ mark as skipped step.\n3. For incomplete problems (actual reasoning asks for more info), mark SKIPPED_STEPS as \"incomplete_problem\", not \"none\".\n4. Do NOT use \"none\" if actual reasoning is incomplete or missing expected logic.\n\nMust output in this EXACT format (no extra words):\nSKIPPED_STEPS: [step numbers separated by commas / \"incomplete_problem\" / \"none\" only if 100% complete]\nSKIPPED_CONTENT: [missing core logic / \"incomplete_problem\" / \"none\"]\nSKIPPING_LOCATION: [between step X and step Y / \"incomplete_problem\" / \"none\"]\"\"\"\n    )\n    \n    inputs = tokenizer(judge_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_return_sequences=1,\n            do_sample=False,\n            return_dict_in_generate=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    judge_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    judge_analysis = judge_response.split(\"assistant\")[-1].strip() if \"assistant\" in judge_response else judge_response\n    skipping_info = parse_skipping_analysis(judge_analysis)\n    return skipping_info, judge_analysis\n\ndef parse_skipping_analysis(analysis_text):\n    skipping_info = {\n        'skipped_steps': [],\n        'skipped_content': '',\n        'skipping_location': '',\n        'from_step': None,\n        'to_step': None\n    }\n    \n    steps_match = re.search(r'SKIPPED_STEPS:\\s*([^\\n]+)', analysis_text)\n    if steps_match:\n        steps_str = steps_match.group(1)\n        step_numbers = re.findall(r'\\d+', steps_str)\n        skipping_info['skipped_steps'] = [f\"step_{num}\" for num in step_numbers]\n    \n    content_match = re.search(r'SKIPPED_CONTENT:\\s*([^\\n]+)', analysis_text)\n    if content_match:\n        skipping_info['skipped_content'] = content_match.group(1).strip()\n    \n    location_match = re.search(r'SKIPPING_LOCATION:\\s*([^\\n]+)', analysis_text)\n    if location_match:\n        location_str = location_match.group(1)\n        skipping_info['skipping_location'] = location_str.strip()\n        step_matches = re.findall(r'step\\s*(\\d+)', location_str.lower())\n        if len(step_matches) >= 2:\n            skipping_info['from_step'] = f\"step_{step_matches[0]}\"\n            skipping_info['to_step'] = f\"step_{step_matches[1]}\"\n    \n    return skipping_info\n\n# =====================================================\n# LLMå¯¹æ¯”è·¯å¾„ç”Ÿæˆ\n# =====================================================\ndef generate_comparison_paths_with_llm(skipping_info, step_data, tokenizer, model):\n    if not skipping_info.get('skipped_steps'):\n        return {\"status\": \"no_skip\"}\n\n    # è·å–è·³æ­¥å‰åçš„æ­¥éª¤ä¿¡æ¯ï¼ˆå…³é”®é”šç‚¹ï¼‰\n    skipped_step_key = skipping_info['skipped_steps'][0]\n    skipped_num = int(skipped_step_key.split('_')[1])\n    prev_step_key = f\"step_{skipped_num - 1}\" if skipped_num > 1 else None\n    target_step_key = f\"step_{skipped_num + 1}\"\n\n    # æå–è·³æ­¥å‰çš„æ­¥éª¤æ–‡æœ¬ï¼ˆprev_textï¼‰å’Œè·³æ­¥åçš„ç›®æ ‡æ­¥éª¤æ–‡æœ¬ï¼ˆtarget_textï¼‰\n    prev_text = \"\"\n    for step in step_data:\n        if step['step_number'] == prev_step_key:\n            prev_text = step['step_text'].strip()\n            break\n    # å¦‚æœæ²¡æœ‰å‰åºæ­¥éª¤ï¼Œç”¨é—®é¢˜å¼€å¤´ä½œä¸ºä¸Šä¸‹æ–‡\n    if not prev_text and skipped_num == 1:\n        prev_text = \"Initial state: Start of reasoning\"\n\n    target_text = \"\"\n    for step in step_data:\n        if step['step_number'] == target_step_key:\n            target_text = step['step_text'].strip()\n            break\n\n    # æ„å»ºæ˜ç¡®çš„ä¸Šä¸‹æ–‡ï¼šå‘ŠçŸ¥æ¨¡å‹éœ€è¦è¡¥å……â€œprev_textâ€åˆ°â€œtarget_textâ€ä¹‹é—´çš„æ¨ç†\n    context_text = f\"\"\"You need toè¡¥å…… the reasoning step between:\nPrevious step: {prev_text}\nNext step (target): {target_text}\nPlease continue the reasoning logically.\"\"\"\n\n    style_prompts = {\n        \"detailed\": \"Provide a detailed reasoning step to connect the previous and next step:\",\n        \"shortcut\": \"Give a direct result that bridges the previous and next step (no reasoning):\",\n        \"alternative\": \"Provide an alternative concise reasoning step to connect them:\"\n    }\n\n    results = {\"status\": \"ok\", \"similarities\": {}, \"paths\": {}}\n\n    for style, prompt_suffix in style_prompts.items():\n        sims = []\n        path_texts = []\n        for _ in range(3):\n            user_msg = f\"{context_text}\\n\\n{prompt_suffix}\"\n            full_prompt = build_llama3_prompt(\"You are a precise reasoning assistant. Only output the reasoning step, no extra words.\", user_msg)\n\n            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=100,  # è¶³å¤Ÿå®¹çº³å•æ­¥æ¨ç†\n                    do_sample=True,\n                    output_hidden_states=True,\n                    return_dict_in_generate=True,\n                    pad_token_id=tokenizer.eos_token_id\n                )\n\n            # è§£ç ç”Ÿæˆçš„è·¯å¾„æ–‡æœ¬ï¼ˆåªä¿ç•™æ¨ç†å†…å®¹ï¼‰\n            generated_ids = outputs.sequences[0, len(inputs.input_ids[0]):]\n            path_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n            # è¿‡æ»¤æ— å…³å›å¤ï¼ˆå¦‚è¯¢é—®ä¸Šä¸‹æ–‡çš„å†…å®¹ï¼‰\n            if \"help\" in path_text.lower() or \"context\" in path_text.lower() or \"provide\" in path_text.lower():\n                path_text = f\"[Invalid: Missing reasoning to connect steps]\"\n            path_texts.append(path_text)\n\n            # è®¡ç®—ç›¸ä¼¼åº¦\n            last_hidden = outputs.hidden_states[-1][-1][0, -1, :].cpu().numpy()\n            # ç¡®ä¿ç›®æ ‡æ­¥éª¤çš„hidden stateå­˜åœ¨\n            target_hidden = None\n            for step in step_data:\n                if step['step_number'] == target_step_key:\n                    target_hidden = step['hidden_state']\n                    break\n            if target_hidden is None:\n                sims.append(0.0)\n                continue\n            sim = cosine_similarity(\n                target_hidden.reshape(1, -1),\n                last_hidden.reshape(1, -1)\n            )[0][0]\n            sims.append(float(sim))\n\n        results[\"similarities\"][style] = {\n            \"mean\": float(np.mean(sims)),\n            \"std\": float(np.std(sims)),\n            \"all\": sims\n        }\n        results[\"paths\"][style] = path_texts\n\n    return results\n\n# =====================================================\n# å•ä¸ªé—®é¢˜å®éªŒæµç¨‹\n# =====================================================\ndef main_experiment_with_details(test_sample):\n    test_id = test_sample['id']\n    problem = test_sample['problem']\n    expected_steps = test_sample['expected_complete_reasoning']\n    \n    # ç”Ÿæˆåˆ†æ­¥æ¨ç†\n    step_by_step_prompt = build_llama3_prompt(\n        \"You solve reasoning problems in a FIXED FORMAT. Follow EXACTLY: 1. Start with 'Step 1: ...' 2. Each step has only core logic 3. End with 'Final Answer: ...' 4. No extra greetings/explanations 5. Do NOT ask for more information (try your best if problem is simple)\",\n        f\"Problem: {problem}\\n\\nSolve step by step in the FIXED FORMAT (only core logic, no extra words):\"\n    )\n    inputs = tokenizer(step_by_step_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=500,\n            num_return_sequences=1,\n            do_sample=True,\n            output_hidden_states=True,\n            return_dict_in_generate=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    full_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    step_by_step_response = full_response.split(\"assistant\")[-1].strip() if \"assistant\" in full_response else full_response\n    \n    # æå–æ­¥éª¤çº§hidden states\n    prompt_length = inputs.input_ids.shape[1]\n    step_data = extract_step_level_hidden_states(outputs, prompt_length, step_by_step_response)\n    \n    # è¯†åˆ«è·³æ­¥ä½ç½®\n    skipping_info, judge_analysis = identify_skipped_steps(step_by_step_response, problem, expected_steps)\n    \n    # ç”Ÿæˆå¯¹æ¯”è·¯å¾„ï¼ˆåŒ…å«æ–‡æœ¬ï¼‰\n    comparison_results = generate_comparison_paths_with_llm(skipping_info, step_data, tokenizer, model)\n    \n    return {\n        'test_id': test_id,\n        'step_by_step_response': step_by_step_response,\n        'judge_analysis': judge_analysis,\n        'skipping_info': skipping_info,\n        'similarity_results': comparison_results.get('similarities', {}),\n        'paths': comparison_results.get('paths', {})  # æ–°å¢ï¼šè¿”å›è·¯å¾„æ–‡æœ¬\n    }\n\n# =====================================================\n# æ‰¹é‡è¿è¡Œæµ‹è¯•é›†ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šæ‰“å°è·¯å¾„æ–‡æœ¬ï¼‰\n# =====================================================\ndef run_batch_test():\n    testset = load_testset(\"/kaggle/input/testset/testset.csv\")\n    \n    print(\"=\" * 70)\n    print(f\"æ‰¹é‡è·³æ­¥åˆ†æå®éªŒ - å…± {len(testset)} ä¸ªé—®é¢˜\")\n    print(\"=\" * 70)\n    \n    style_to_path = {\n        \"detailed\": \"A->B->C\",\n        \"shortcut\": \"A->C\",\n        \"alternative\": \"A->B1->C\"\n    }\n    \n    for test_sample in testset:\n        detailed_result = main_experiment_with_details(test_sample)\n        test_id = detailed_result['test_id']\n        skipping_info = detailed_result['skipping_info']\n        similarities = detailed_result['similarity_results']\n        paths = detailed_result['paths']  # è·å–è·¯å¾„æ–‡æœ¬\n        step_by_step_response = detailed_result['step_by_step_response']\n        judge_analysis = detailed_result['judge_analysis']\n        \n        print(f\"\\nã€{test_id} è¯¦ç»†éªŒè¯ã€‘\")\n        print(f\"æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\\n{step_by_step_response[:500]}...\")\n        print(f\"\\nLLM Judgeåˆ†æï¼š\\n{judge_analysis}\")\n        \n        # æ–°å¢ï¼šå¦‚æœæœ‰è·³æ­¥ï¼Œæ‰“å°3ç§é£æ ¼çš„è·¯å¾„æ–‡æœ¬\n        if skipping_info['skipped_steps']:\n            skip_step_num = skipping_info['skipped_steps'][0].split('_')[-1]\n            print(f\"\\nã€è·³æ­¥{skip_step_num}çš„å¯¹æ¯”è·¯å¾„ã€‘\")\n            for style, path_texts in paths.items():\n                print(f\"\\n{style_to_path[style]}ï¼ˆ{style}é£æ ¼ï¼‰ï¼š\")\n                for i, text in enumerate(path_texts, 1):\n                    print(f\"  è·¯å¾„{i}ï¼š{text[:200]}...\")  # æ‰“å°å‰200å­—ç¬¦é¿å…è¿‡é•¿\n        \n        # åŸæœ‰è¾“å‡ºæ ¼å¼\n        if skipping_info['skipped_steps']:\n            if similarities:\n                best_style = max(similarities.items(), key=lambda x: x[1]['mean'])[0]\n                similar_path = style_to_path[best_style]\n                best_similarity = round(similarities[best_style]['mean'], 4)\n                print(f\"\\n{test_id} è·³æ­¥{skip_step_num}: ç›¸ä¼¼è·¯å¾„ï¼š{similar_path} ç›¸ä¼¼åº¦ï¼š{best_similarity}\")\n            else:\n                print(f\"\\n{test_id} è·³æ­¥{skip_step_num}: ç›¸ä¼¼è·¯å¾„ï¼šæ—  ç›¸ä¼¼åº¦ï¼š0.0\")\n        else:\n            print(f\"\\n{test_id}: æ— è·³æ­¥ï¼ˆæ ¹æ®Judgeåˆ†æï¼‰\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"ğŸ‰ æ‰¹é‡å®éªŒå®Œæˆï¼\")\n\n# =====================================================\n# æ‰§è¡Œæ‰¹é‡æµ‹è¯•\n# =====================================================\nif __name__ == \"__main__\":\n    run_batch_test()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T10:02:12.675425Z","iopub.execute_input":"2025-11-04T10:02:12.675986Z","iopub.status.idle":"2025-11-04T10:06:06.115952Z","shell.execute_reply.started":"2025-11-04T10:02:12.675967Z","shell.execute_reply":"2025-11-04T10:06:06.115051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ–¹æ³•ä¸€","metadata":{}},{"cell_type":"markdown","source":"### åˆå§‹åŒ–è®¾ç½®+å·¥å…·å‡½æ•°","metadata":{}},{"cell_type":"code","source":"XP = \"If you enter a building from the east side, walking west, and then take two rights and a left down corridors, which direction are you facing?\"\n\ndef save_hidden_states(data_dict, filename=\"hidden_states_data.pkl\"):\n    \"\"\"ä¿å­˜æ•°æ®\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n    print(f\"âœ“ æ•°æ®å·²ä¿å­˜è‡³ {filename}\")\n\ndef build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    return prompt\n\nprint(\"âœ… Cell 1å®Œæˆï¼šåˆå§‹åŒ–è®¾ç½®\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:38:27.327832Z","iopub.execute_input":"2025-11-04T03:38:27.328611Z","iopub.status.idle":"2025-11-04T03:38:27.333912Z","shell.execute_reply.started":"2025-11-04T03:38:27.328585Z","shell.execute_reply":"2025-11-04T03:38:27.333123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ç”Ÿæˆåˆ†æ­¥æ¨ç†å¹¶æå–æ­¥éª¤çº§hidden states","metadata":{}},{"cell_type":"code","source":"def generate_step_by_step_reasoning():\n    \"\"\"è®©æ¨¡å‹è¿›è¡Œåˆ†æ­¥æ¨ç†ï¼Œè®°å½•æ¯ä¸ªæ­¥éª¤çš„hidden states\"\"\"\n    \n    step_by_step_prompt = build_llama3_prompt(\n        \"You are a helpful AI assistant that solves direction reasoning problems step by step. You MUST output your reasoning in the exact format: Step 1: [reasoning], Step 2: [reasoning], etc. End with 'Final Answer: [direction]'\",\n        f\"Problem: {XP}\\n\\nPlease solve this step by step and output each step clearly:\"\n    )\n    \n    print(\"ç”Ÿæˆåˆ†æ­¥æ¨ç†è¿‡ç¨‹...\")\n    inputs = tokenizer(step_by_step_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            output_hidden_states=True,\n            return_dict_in_generate=True\n        )\n    \n    # è·å–å®Œæ•´å“åº”\n    full_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    assistant_response = full_response.split(\"assistant\")[-1].strip() if \"assistant\" in full_response else full_response\n    \n    print(f\"æ¨¡å‹åˆ†æ­¥æ¨ç†å“åº”:\\n{assistant_response}\")\n    \n    # æå–æ­¥éª¤çº§hidden states\n    step_data = extract_step_level_hidden_states(outputs, inputs.input_ids.shape[1], assistant_response)\n    \n    return assistant_response, step_data\n\ndef extract_step_level_hidden_states(outputs, prompt_length, generated_text):\n    \"\"\"æå–æ¯ä¸ªæ­¥éª¤çš„hidden states\"\"\"\n    \n    # è§£ææ­¥éª¤ç»“æ„\n    steps = parse_step_structure(generated_text)\n    \n    step_data = []\n    generated_ids = outputs.sequences[0, prompt_length:]\n    full_generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    \n    # ä¸ºæ¯ä¸ªæ­¥éª¤æ‰¾åˆ°å¯¹åº”çš„tokenä½ç½®\n    for step_num, step_text in steps.items():\n        # æ‰¾åˆ°æ­¥éª¤æ–‡æœ¬åœ¨å®Œæ•´ç”Ÿæˆæ–‡æœ¬ä¸­çš„ä½ç½®\n        step_start = full_generated_text.find(step_text)\n        if step_start == -1:\n            continue\n            \n        # ä¼°ç®—tokenä½ç½®\n        approx_token_start = step_start // 4  # ç²—ç•¥ä¼°ç®—\n        approx_token_end = approx_token_start + len(step_text) // 4\n        \n        # ç¡®ä¿åœ¨èŒƒå›´å†…\n        token_pos = min(approx_token_end, len(outputs.hidden_states[-1]) - 1)\n        \n        # è·å–è¯¥ä½ç½®çš„hidden state\n        step_hidden = outputs.hidden_states[-1][token_pos][0, -1, :].cpu().numpy()\n        \n        step_data.append({\n            'step_number': step_num,\n            'step_text': step_text,\n            'hidden_state': step_hidden,\n            'token_position': token_pos\n        })\n        \n        print(f\"  æå–æ­¥éª¤ {step_num}: {step_text[:50]}...\")\n    \n    return step_data\n\ndef parse_step_structure(text):\n    \"\"\"è§£ææ–‡æœ¬ä¸­çš„æ­¥éª¤ç»“æ„\"\"\"\n    steps = {}\n    \n    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… Step X: æ ¼å¼\n    step_pattern = r'(Step\\s+\\d+:|Final Answer:)([^Step]*)'\n    matches = re.findall(step_pattern, text, re.IGNORECASE | re.DOTALL)\n    \n    for i, (step_header, step_content) in enumerate(matches):\n        step_key = f\"step_{i+1}\" if \"Final\" not in step_header else \"final\"\n        step_text = (step_header + step_content).strip()\n        steps[step_key] = step_text\n    \n    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­¥éª¤ç»“æ„ï¼Œæ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªæ­¥éª¤\n    if not steps:\n        steps['step_1'] = text.strip()\n    \n    return steps\n\nprint(\"âœ… Cell 2å®Œæˆï¼šåˆ†æ­¥æ¨ç†ç”Ÿæˆå‡½æ•°å®šä¹‰\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:38:28.211403Z","iopub.execute_input":"2025-11-04T03:38:28.211643Z","iopub.status.idle":"2025-11-04T03:38:28.223144Z","shell.execute_reply.started":"2025-11-04T03:38:28.211625Z","shell.execute_reply":"2025-11-04T03:38:28.222425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LLM Judgeè¯†åˆ«è·³æ­¥ä½ç½®","metadata":{}},{"cell_type":"code","source":"def identify_skipped_steps(step_by_step_response):\n    \"\"\"ä½¿ç”¨LLM Judgeè¯†åˆ«å…·ä½“è·³è¿‡çš„æ­¥éª¤\"\"\"\n    \n    expected_complete_reasoning = [\n        \"Step 1: Initial direction is west (entering from east walking west).\",\n        \"Step 2: First right turn: west â†’ north.\",\n        \"Step 3: Second right turn: north â†’ east.\", \n        \"Step 4: Left turn: east â†’ north.\",\n        \"Step 5: Final Answer: north\"\n    ]\n    \n    judge_prompt = build_llama3_prompt(\n        \"You are an expert reasoning analyzer. Analyze if any logical steps are skipped in the reasoning process. Be specific about which exact steps are missing.\",\n        f\"\"\"Problem: {XP}\n\nExpected Complete Reasoning Path:\n{chr(10).join(expected_complete_reasoning)}\n\nActual Reasoning:\n{step_by_step_response}\n\nPlease analyze:\n1. Which specific step numbers are skipped or missing?\n2. What is the exact content of the skipped steps?\n3. Between which steps does the skipping occur?\n\nProvide your analysis in this exact format:\nSKIPPED_STEPS: [step numbers]\nSKIPPED_CONTENT: [description of what was skipped]\nSKIPPING_LOCATION: [between step X and step Y]\"\"\"\n    )\n    \n    inputs = tokenizer(judge_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_return_sequences=1,\n            do_sample=False,\n            temperature=0.0,\n            return_dict_in_generate=True\n        )\n    \n    judge_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    judge_analysis = judge_response.split(\"assistant\")[-1].strip() if \"assistant\" in judge_response else judge_response\n    \n    print(f\"LLM Judgeåˆ†æç»“æœ:\\n{judge_analysis}\")\n    \n    # è§£æåˆ†æç»“æœ\n    skipping_info = parse_skipping_analysis(judge_analysis)\n    \n    return skipping_info, judge_analysis\n\ndef parse_skipping_analysis(analysis_text):\n    \"\"\"è§£æLLM Judgeçš„è·³æ­¥åˆ†æç»“æœ\"\"\"\n    skipping_info = {\n        'skipped_steps': [],\n        'skipped_content': '',\n        'skipping_location': '',\n        'from_step': None,\n        'to_step': None\n    }\n    \n    # æå–è·³è¿‡çš„æ­¥éª¤ç¼–å·\n    steps_match = re.search(r'SKIPPED_STEPS:\\s*([^\\n]+)', analysis_text)\n    if steps_match:\n        steps_str = steps_match.group(1)\n        # æå–æ•°å­—\n        step_numbers = re.findall(r'\\d+', steps_str)\n        skipping_info['skipped_steps'] = [f\"step_{num}\" for num in step_numbers]\n    \n    # æå–è·³è¿‡å†…å®¹\n    content_match = re.search(r'SKIPPED_CONTENT:\\s*([^\\n]+)', analysis_text)\n    if content_match:\n        skipping_info['skipped_content'] = content_match.group(1).strip()\n    \n    # æå–è·³è¿‡ä½ç½®\n    location_match = re.search(r'SKIPPING_LOCATION:\\s*([^\\n]+)', analysis_text)\n    if location_match:\n        location_str = location_match.group(1)\n        skipping_info['skipping_location'] = location_str.strip()\n        \n        # è§£æä»å“ªä¸ªæ­¥éª¤è·³åˆ°å“ªä¸ªæ­¥éª¤\n        step_matches = re.findall(r'step\\s*(\\d+)', location_str.lower())\n        if len(step_matches) >= 2:\n            skipping_info['from_step'] = f\"step_{step_matches[0]}\"\n            skipping_info['to_step'] = f\"step_{step_matches[1]}\"\n    \n    return skipping_info\n\nprint(\"âœ… Cell 3å®Œæˆï¼šè·³æ­¥è¯†åˆ«å‡½æ•°å®šä¹‰\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:38:28.853530Z","iopub.execute_input":"2025-11-04T03:38:28.853785Z","iopub.status.idle":"2025-11-04T03:38:28.862829Z","shell.execute_reply.started":"2025-11-04T03:38:28.853766Z","shell.execute_reply":"2025-11-04T03:38:28.861988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ä¸ºè·³æ­¥ä½ç½®æ„å»ºå¯¹æ¯”è·¯å¾„","metadata":{}},{"cell_type":"code","source":"def generate_comparison_paths_with_llm(skipping_info, step_data, step_by_step_response):\n    \"\"\"ä½¿ç”¨LLMç”Ÿæˆå®Œæ•´çš„å¯¹æ¯”æ¨ç†è·¯å¾„\"\"\"\n    \n    if not skipping_info.get('from_step') or not skipping_info.get('to_step'):\n        print(\"âŒ æ— æ³•ç¡®å®šè·³æ­¥ä½ç½®ï¼Œä½¿ç”¨LLMç”Ÿæˆé»˜è®¤å¯¹æ¯”è·¯å¾„\")\n        return generate_default_comparison_paths_with_llm()\n    \n    from_step = skipping_info['from_step']\n    to_step = skipping_info['to_step']\n    skipped_content = skipping_info.get('skipped_content', 'unknown steps')\n    \n    print(f\"ğŸ” åˆ†æè·³æ­¥: ä» {from_step} è·³åˆ° {to_step}\")\n    print(f\"è·³è¿‡çš„å†…å®¹: {skipped_content}\")\n    \n    # è·å–è·³æ­¥å‰çš„ä¸Šä¸‹æ–‡\n    context_steps = []\n    for step in step_data:\n        if step['step_number'] == from_step:\n            context_steps.append(step['step_text'])\n            break\n        context_steps.append(step['step_text'])\n    \n    context_text = \" \".join(context_steps)\n    \n    # ä½¿ç”¨LLMç”Ÿæˆå¤šç§å®Œæ•´çš„æ¨ç†è·¯å¾„\n    comparison_paths = {\n        'full_reasoning_path_a': generate_llm_reasoning_path(\"detailed\", context_text, skipped_content),\n        'full_reasoning_path_b': generate_llm_reasoning_path(\"alternative\", context_text, skipped_content),\n        'full_reasoning_path_c': generate_llm_reasoning_path(\"step_by_step\", context_text, skipped_content),\n        'pure_shortcut_path': generate_llm_shortcut_path(context_text, skipped_content),\n        'skipped_content': skipped_content,\n        'jumping_context': context_text\n    }\n    \n    return comparison_paths\n\ndef generate_llm_reasoning_path(style, context, skipped_content):\n    \"\"\"ä½¿ç”¨LLMç”ŸæˆæŒ‡å®šé£æ ¼çš„å®Œæ•´æ¨ç†è·¯å¾„\"\"\"\n    \n    if style == \"detailed\":\n        system_msg = \"You are a detailed reasoning assistant. Provide complete, logical steps for direction reasoning.\"\n        user_msg = f\"\"\"Based on this context: \"{context}\"\n\nThe skipped step was: \"{skipped_content}\"\n\nPlease provide a COMPLETE and DETAILED reasoning path that includes all logical steps. Format as:\nStep X: [reasoning]\nStep Y: [reasoning]\n...\"\"\"\n    \n    elif style == \"alternative\":\n        system_msg = \"You are an AI assistant. Provide an alternative complete reasoning path.\"\n        user_msg = f\"\"\"Context: \"{context}\"\nSkipped: \"{skipped_content}\"\n\nProvide an ALTERNATIVE but equally valid complete reasoning path with all steps:\"\"\"\n    \n    else:  # step_by_step\n        system_msg = \"You are a methodical reasoning assistant.\"\n        user_msg = f\"\"\"Given: \"{context}\"\nMissing: \"{skipped_content}\"\n\nProvide a thorough step-by-step reasoning path with no jumps:\"\"\"\n    \n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    reasoning_path = parse_llm_reasoning_steps(response)\n    \n    print(f\"  LLMç”Ÿæˆ{style}è·¯å¾„: {len(reasoning_path)}ä¸ªæ­¥éª¤\")\n    return reasoning_path\n\ndef generate_llm_shortcut_path(context, skipped_content):\n    \"\"\"ä½¿ç”¨LLMç”Ÿæˆçº¯è·³æ­¥è·¯å¾„\"\"\"\n    \n    system_msg = \"You are a concise AI assistant that provides direct answers without reasoning.\"\n    user_msg = f\"\"\"Based on: \"{context}\"\n\nProvide ONLY the final conclusion without any intermediate reasoning steps. Be direct and jump to the answer.\"\"\"\n    \n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    shortcut_path = [f\"Direct jump: {response.split('assistant')[-1].strip()}\"]\n    \n    print(f\"  LLMç”Ÿæˆçº¯è·³æ­¥è·¯å¾„: {shortcut_path[0][:50]}...\")\n    return shortcut_path\n\ndef parse_llm_reasoning_steps(response_text):\n    \"\"\"è§£æLLMç”Ÿæˆçš„æ¨ç†æ­¥éª¤\"\"\"\n    \n    # æå–assistantçš„å“åº”\n    if \"assistant\" in response_text:\n        reasoning_text = response_text.split(\"assistant\")[-1].strip()\n    else:\n        reasoning_text = response_text\n    \n    # ä½¿ç”¨å¤šç§æ¨¡å¼åŒ¹é…æ­¥éª¤\n    steps = []\n    \n    # æ¨¡å¼1: Step X: æ ¼å¼\n    step_pattern1 = r'(?:Step|æ­¥éª¤)\\s*(\\d+)[:\\\\.]\\\\s*([^\\\\n]+)'\n    matches1 = re.findall(step_pattern1, reasoning_text, re.IGNORECASE)\n    \n    # æ¨¡å¼2: æ•°å­—. æ ¼å¼  \n    step_pattern2 = r'(\\d+)\\\\.\\\\s*([^\\\\n]+)'\n    matches2 = re.findall(step_pattern2, reasoning_text)\n    \n    # æ¨¡å¼3: æ¢è¡Œåˆ†éš”çš„æ¨ç†æ­¥éª¤\n    lines = reasoning_text.split('\\n')\n    for line in lines:\n        line = line.strip()\n        if line and (line.startswith('-') or line.startswith('â€¢') or \n                    (len(line) > 10 and not line.startswith('Step'))):\n            steps.append(line)\n    \n    # ä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–çš„æ­¥éª¤\n    if matches1:\n        for num, content in matches1:\n            steps.append(f\"Step {num}: {content.strip()}\")\n    elif matches2:\n        for num, content in matches2:\n            steps.append(f\"Step {num}: {content.strip()}\")\n    \n    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°æ­¥éª¤ï¼Œä½¿ç”¨æ•´ä¸ªå“åº”\n    if not steps:\n        steps = [reasoning_text]\n    \n    return steps\n\ndef generate_default_comparison_paths_with_llm():\n    \"\"\"å½“æ— æ³•ç¡®å®šè·³æ­¥ä½ç½®æ—¶ï¼Œç”¨LLMç”Ÿæˆé»˜è®¤å¯¹æ¯”è·¯å¾„\"\"\"\n    \n    print(\"ä½¿ç”¨LLMç”Ÿæˆé»˜è®¤å¯¹æ¯”è·¯å¾„...\")\n    \n    system_msg = \"You are an expert at direction reasoning problems.\"\n    user_msg = f\"\"\"Problem: {XP}\n\nPlease provide:\n1. A detailed complete reasoning path\n2. An alternative complete reasoning path  \n3. A direct shortcut answer without reasoning\n\nFormat as:\nDETAILED: [steps]\nALTERNATIVE: [steps]\nSHORTCUT: [answer]\"\"\"\n    \n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    llm_response = response.split(\"assistant\")[-1].strip() if \"assistant\" in response else response\n    \n    # è§£æLLMçš„å“åº”\n    paths = parse_default_llm_response(llm_response)\n    \n    return paths\n\ndef parse_default_llm_response(response_text):\n    \"\"\"è§£æLLMç”Ÿæˆçš„é»˜è®¤è·¯å¾„å“åº”\"\"\"\n    \n    paths = {\n        'full_reasoning_path_a': [],\n        'full_reasoning_path_b': [], \n        'pure_shortcut_path': [],\n        'skipped_content': \"unknown steps\",\n        'jumping_context': \"default context\"\n    }\n    \n    # å°è¯•è§£æä¸åŒéƒ¨åˆ†\n    lines = response_text.split('\\n')\n    \n    detailed_steps = []\n    alternative_steps = []\n    shortcut_answer = []\n    \n    current_section = None\n    \n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n            \n        if 'DETAILED:' in line.upper():\n            current_section = 'detailed'\n            line = line.split(':', 1)[-1].strip()\n        elif 'ALTERNATIVE:' in line.upper():\n            current_section = 'alternative'  \n            line = line.split(':', 1)[-1].strip()\n        elif 'SHORTCUT:' in line.upper():\n            current_section = 'shortcut'\n            line = line.split(':', 1)[-1].strip()\n        \n        if current_section == 'detailed' and line:\n            detailed_steps.append(line)\n        elif current_section == 'alternative' and line:\n            alternative_steps.append(line)\n        elif current_section == 'shortcut' and line:\n            shortcut_answer.append(line)\n    \n    # è®¾ç½®è·¯å¾„\n    paths['full_reasoning_path_a'] = detailed_steps if detailed_steps else [\"Step 1: Default detailed reasoning\"]\n    paths['full_reasoning_path_b'] = alternative_steps if alternative_steps else [\"Step 1: Default alternative reasoning\"]\n    paths['pure_shortcut_path'] = shortcut_answer if shortcut_answer else [\"Direct answer: north\"]\n    \n    return paths\n\nprint(\"âœ… Cell 4å®Œæˆï¼šLLMå¯¹æ¯”è·¯å¾„ç”Ÿæˆå‡½æ•°å®šä¹‰\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:38:29.276060Z","iopub.execute_input":"2025-11-04T03:38:29.276287Z","iopub.status.idle":"2025-11-04T03:38:29.295120Z","shell.execute_reply.started":"2025-11-04T03:38:29.276272Z","shell.execute_reply":"2025-11-04T03:38:29.294493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ç”Ÿæˆè·¯å¾„hidden stateså¹¶è®¡ç®—ç›¸ä¼¼åº¦","metadata":{}},{"cell_type":"code","source":"def generate_paths_hidden_states(comparison_paths, skipping_info):\n    \"\"\"ä¸ºæ‰€æœ‰å¯¹æ¯”è·¯å¾„ç”Ÿæˆhidden states\"\"\"\n    \n    system_message = \"You are a helpful AI assistant that reasons about direction problems.\"\n    path_hidden_states = {}\n    \n    for path_name, path_steps in comparison_paths.items():\n        if path_name == 'skipped_step':\n            continue  # è·³è¿‡å†…å®¹æè¿°ï¼Œä¸æ˜¯å®é™…è·¯å¾„\n            \n        print(f\"\\nç”Ÿæˆè·¯å¾„: {path_name}\")\n        \n        if isinstance(path_steps, list):\n            # ä¸ºè·¯å¾„ä¸­çš„æ¯ä¸ªæ­¥éª¤ç”Ÿæˆhidden states\n            step_hidden_states = []\n            \n            for step_idx, step_text in enumerate(path_steps):\n                user_message = f\"{XP}\\n\\nContinue this reasoning:\\n{step_text}\"\n                full_prompt = build_llama3_prompt(system_message, user_message)\n                \n                inputs = tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n                \n                with torch.no_grad():\n                    outputs = model.generate(\n                        **inputs,\n                        max_new_tokens=5,\n                        num_return_sequences=1,\n                        do_sample=False,\n                        temperature=0.0,\n                        output_hidden_states=True,\n                        return_dict_in_generate=True\n                    )\n                \n                # è·å–hidden state\n                last_hidden = outputs.hidden_states[-1][-1][0, -1, :].cpu().numpy()\n                step_hidden_states.append(last_hidden)\n                \n                print(f\"  æ­¥éª¤ {step_idx+1}: {step_text[:50]}...\")\n            \n            path_hidden_states[path_name] = step_hidden_states\n    \n    return path_hidden_states\n\ndef calculate_similarity_analysis(skipping_step_hidden, path_hidden_states, step_data, skipping_info):\n    \"\"\"è®¡ç®—è·³æ­¥ä½ç½®ä¸å„è·¯å¾„çš„ç›¸ä¼¼åº¦\"\"\"\n    \n    similarity_results = {}\n    \n    # æ‰¾åˆ°è·³æ­¥ä½ç½®çš„hidden state\n    target_hidden = None\n    target_step_name = skipping_info.get('to_step', 'step_3')  # é»˜è®¤ç¬¬ä¸‰ä¸ªæ­¥éª¤\n    \n    for step in step_data:\n        if step['step_number'] == target_step_name:\n            target_hidden = step['hidden_state']\n            break\n    \n    if target_hidden is None and step_data:\n        # ä½¿ç”¨æœ€åä¸€ä¸ªæ­¥éª¤ä½œä¸ºç›®æ ‡\n        target_hidden = step_data[-1]['hidden_state']\n    \n    if target_hidden is None:\n        print(\"âŒ æ— æ³•æ‰¾åˆ°ç›®æ ‡hidden state\")\n        return similarity_results\n    \n    print(f\"\\nğŸ”¬ ç›¸ä¼¼åº¦åˆ†æ (ç›®æ ‡: {target_step_name})\")\n    \n    # è®¡ç®—ä¸æ¯ä¸ªè·¯å¾„çš„ç›¸ä¼¼åº¦\n    for path_name, path_hs_list in path_hidden_states.items():\n        path_similarities = []\n        \n        for path_hs in path_hs_list:\n            similarity = cosine_similarity(\n                target_hidden.reshape(1, -1),\n                path_hs.reshape(1, -1)\n            )[0][0]\n            path_similarities.append(similarity)\n        \n        similarity_results[path_name] = {\n            'mean_similarity': np.mean(path_similarities),\n            'max_similarity': np.max(path_similarities),\n            'min_similarity': np.min(path_similarities),\n            'all_similarities': path_similarities\n        }\n        \n        print(f\"  {path_name}: å¹³å‡ç›¸ä¼¼åº¦ = {np.mean(path_similarities):.4f}\")\n    \n    return similarity_results\n\nprint(\"âœ… Cell 5å®Œæˆï¼šç›¸ä¼¼åº¦åˆ†æå‡½æ•°å®šä¹‰\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:38:34.069878Z","iopub.execute_input":"2025-11-04T03:38:34.070456Z","iopub.status.idle":"2025-11-04T03:38:34.080312Z","shell.execute_reply.started":"2025-11-04T03:38:34.070432Z","shell.execute_reply":"2025-11-04T03:38:34.079572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ä¸»å®éªŒæµç¨‹","metadata":{}},{"cell_type":"code","source":"def main_experiment():\n    \"\"\"ä¸»å®éªŒæµç¨‹\"\"\"\n    \n    print(\"=\" * 70)\n    print(\"æ­¥éª¤çº§åˆ«éšå¼æ¨ç†åˆ†æå®éªŒ - å®Œæ•´æµç¨‹\")\n    print(\"=\" * 70)\n    \n    # é˜¶æ®µ1: ç”Ÿæˆåˆ†æ­¥æ¨ç†\n    print(\"\\nğŸ“ é˜¶æ®µ1: ç”Ÿæˆåˆ†æ­¥æ¨ç†\")\n    step_by_step_response, step_data = generate_step_by_step_reasoning()\n    \n    # é˜¶æ®µ2: è¯†åˆ«è·³æ­¥ä½ç½®\n    print(\"\\nğŸ” é˜¶æ®µ2: è¯†åˆ«è·³æ­¥ä½ç½®\")\n    skipping_info, judge_analysis = identify_skipped_steps(step_by_step_response)\n    \n    # é˜¶æ®µ3: ç”¨LLMæ„å»ºå¯¹æ¯”è·¯å¾„\n    print(\"\\nğŸ›£ï¸  é˜¶æ®µ3: ç”¨LLMç”Ÿæˆå¯¹æ¯”è·¯å¾„\")\n    comparison_paths = generate_comparison_paths_with_llm(skipping_info, step_data, step_by_step_response)\n    \n    print(\"LLMç”Ÿæˆçš„å¯¹æ¯”è·¯å¾„:\")\n    for path_name, path in comparison_paths.items():\n        if isinstance(path, list):\n            print(f\"  {path_name}:\")\n            for i, step in enumerate(path):\n                print(f\"    {i+1}. {step[:60]}...\" if len(step) > 60 else f\"    {i+1}. {step}\")\n        else:\n            print(f\"  {path_name}: {path[:80]}...\" if len(path) > 80 else f\"  {path_name}: {path}\")\n    \n    # é˜¶æ®µ4: ç”Ÿæˆè·¯å¾„hidden states\n    print(\"\\nğŸ§  é˜¶æ®µ4: ç”Ÿæˆè·¯å¾„hidden states\")\n    path_hidden_states = generate_paths_hidden_states(comparison_paths, skipping_info)\n    \n    # é˜¶æ®µ5: ç›¸ä¼¼åº¦åˆ†æ\n    print(\"\\nğŸ“Š é˜¶æ®µ5: ç›¸ä¼¼åº¦åˆ†æ\")\n    similarity_results = calculate_similarity_analysis(\n        None, path_hidden_states, step_data, skipping_info\n    )\n    \n    # ä¿å­˜å®Œæ•´ç»“æœ\n    complete_results = {\n        'XP': XP,\n        'step_by_step_response': step_by_step_response,\n        'step_data': step_data,\n        'skipping_info': skipping_info,\n        'judge_analysis': judge_analysis,\n        'comparison_paths': comparison_paths,\n        'path_hidden_states': path_hidden_states,\n        'similarity_results': similarity_results\n    }\n    \n    save_hidden_states(complete_results, \"complete_skipping_analysis.pkl\")\n    \n    # ç‰¹åˆ«ä¿å­˜è·³æ­¥ä½ç½®çš„hidden stateä¾›æ–¹æ³•ä¸‰ä½¿ç”¨\n    jump_step_hidden = None\n    target_step_name = skipping_info.get('to_step', 'step_3')\n    for step in step_data:\n        if step['step_number'] == target_step_name:\n            jump_step_hidden = step['hidden_state']\n            break\n    \n    if jump_step_hidden is not None:\n        hs_data = {\n            'hs_jump': jump_step_hidden,\n            'jump_step': target_step_name,\n            'XP': XP,\n            'skipping_info': skipping_info\n        }\n        save_hidden_states(hs_data, \"method1_jump_hidden_state.pkl\")\n        print(f\"âœ“ è·³æ­¥hidden stateå·²ä¿å­˜ï¼Œå½¢çŠ¶: {jump_step_hidden.shape}\")\n    \n    # è¾“å‡ºæœ€ç»ˆåˆ†æç»“æœ\n    print(\"\\n\" + \"=\" * 70)\n    print(\"ğŸ¯ æœ€ç»ˆåˆ†æç»“æœ\")\n    print(\"=\" * 70)\n    \n    print(f\"è·³æ­¥ä½ç½®: {skipping_info.get('skipping_location', 'æœªçŸ¥')}\")\n    print(f\"è·³è¿‡çš„å†…å®¹: {skipping_info.get('skipped_content', 'æœªçŸ¥')}\")\n    \n    print(\"\\nç›¸ä¼¼åº¦æ¯”è¾ƒ:\")\n    for path_name, result in similarity_results.items():\n        print(f\"  {path_name}: {result['mean_similarity']:.4f} (æœ€é«˜: {result['max_similarity']:.4f}, æœ€ä½: {result['min_similarity']:.4f})\")\n    \n    # æ‰¾å‡ºæœ€ç›¸ä¼¼çš„è·¯å¾„\n    if similarity_results:\n        best_path = max(similarity_results.items(), key=lambda x: x[1]['mean_similarity'])\n        print(f\"\\nğŸ’¡ ç»“è®º: è·³æ­¥çš„å†…éƒ¨æ¨ç†æœ€æ¥è¿‘ '{best_path[0]}' è·¯å¾„\")\n        print(f\"    ç›¸ä¼¼åº¦: {best_path[1]['mean_similarity']:.4f}\")\n    \n    return complete_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T07:14:27.539888Z","iopub.execute_input":"2025-11-04T07:14:27.540200Z","iopub.status.idle":"2025-11-04T07:14:27.560816Z","shell.execute_reply.started":"2025-11-04T07:14:27.540171Z","shell.execute_reply":"2025-11-04T07:14:27.560171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ‰§è¡Œå®éªŒ","metadata":{}},{"cell_type":"code","source":"print(\"ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´å®éªŒ...\")\nresults = main_experiment()\nprint(\"\\nğŸ‰ å®éªŒå®Œæˆï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T07:14:27.561680Z","iopub.execute_input":"2025-11-04T07:14:27.561932Z","iopub.status.idle":"2025-11-04T07:14:27.784065Z","shell.execute_reply.started":"2025-11-04T07:14:27.561916Z","shell.execute_reply":"2025-11-04T07:14:27.783014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ–¹æ³•äºŒ","metadata":{}},{"cell_type":"markdown","source":"### åŸºç¡€è®¾ç½®","metadata":{}},{"cell_type":"code","source":"def build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    return prompt\n\ndef save_results(data_dict, filename):\n    \"\"\"ä¿å­˜å®éªŒç»“æœ\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n    print(f\"âœ“ ç»“æœå·²ä¿å­˜è‡³ {filename}\")\n\nprint(\"âœ… æ–¹æ³•äºŒåŸºç¡€è®¾ç½®å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:39:56.587792Z","iopub.execute_input":"2025-11-04T03:39:56.588483Z","iopub.status.idle":"2025-11-04T03:39:56.593608Z","shell.execute_reply.started":"2025-11-04T03:39:56.588455Z","shell.execute_reply":"2025-11-04T03:39:56.592918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### logit lensæ ¸å¿ƒå‡½æ•°","metadata":{}},{"cell_type":"code","source":"# =====================================================\n# Cell 2: Logit Lensæ ¸å¿ƒå‡½æ•°ï¼ˆä¿®å¤è®¾å¤‡é—®é¢˜ï¼‰\n# =====================================================\n\ndef get_concept_probability_curve(model, tokenizer, prompt, target_concept):\n    \"\"\"\n    è·å–æ¦‚å¿µåœ¨æ¨¡å‹å„å±‚çš„å‡ºç°æ¦‚ç‡æ›²çº¿\n    \"\"\"\n    device = model.device\n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    # è·å–ç›®æ ‡æ¦‚å¿µçš„token ID\n    concept_tokens = tokenizer.encode(target_concept, add_special_tokens=False)\n    if len(concept_tokens) == 0:\n        raise ValueError(f\"æ— æ³•ç¼–ç æ¦‚å¿µ: {target_concept}\")\n    concept_token_id = concept_tokens[0]\n    \n    with torch.no_grad():\n        outputs = model(**inputs, output_hidden_states=True)\n        hidden_states = outputs.hidden_states\n        \n        lm_head = model.lm_head\n        lm_weight = lm_head.weight.to(device)  # ç¡®ä¿æƒé‡åœ¨æ­£ç¡®è®¾å¤‡ä¸Š\n        \n        probabilities = []\n        layers = []\n        \n        for layer_idx, hidden_state in enumerate(hidden_states):\n            # ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„hidden state\n            target_hidden = hidden_state[0, -1, :].to(device)  # ç¡®ä¿åœ¨æ­£ç¡®è®¾å¤‡ä¸Š\n            logits = torch.matmul(target_hidden, lm_weight.T)\n            probs = torch.softmax(logits, dim=0)\n            concept_prob = probs[concept_token_id].item()\n            \n            probabilities.append(concept_prob)\n            layers.append(layer_idx)\n    \n    return np.array(probabilities), np.array(layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:39:57.445063Z","iopub.execute_input":"2025-11-04T03:39:57.445305Z","iopub.status.idle":"2025-11-04T03:39:57.451615Z","shell.execute_reply.started":"2025-11-04T03:39:57.445287Z","shell.execute_reply":"2025-11-04T03:39:57.450921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LLMç”ŸæˆéªŒè¯é—®é¢˜","metadata":{}},{"cell_type":"code","source":"def generate_validation_question(model, tokenizer, xp, intermediate_concept, final_answer):\n    \"\"\"\n    ç”¨LLMç”Ÿæˆåªèƒ½é€šè¿‡ä¸­é—´æ¦‚å¿µæ¨ç†å¾—åˆ°çš„éªŒè¯é—®é¢˜\n    \"\"\"\n    system_msg = \"You are an expert at creating reasoning validation questions.\"\n    \n    user_msg = f\"\"\"Base problem: {xp}\n\nWe are testing whether the model implicitly reasoned about the intermediate concept: \"{intermediate_concept}\"\n\nCreate a question that:\n1. Can ONLY be answered if you know \"{intermediate_concept}\"\n2. Cannot be answered directly from the base problem or final answer \"{final_answer}\"\n3. Requires actual reasoning using the intermediate concept\n\nProvide just the question:\"\"\"\n\n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    question = response.split(\"assistant\")[-1].strip() if \"assistant\" in response else response\n    \n    print(f\"LLMç”Ÿæˆçš„éªŒè¯é—®é¢˜: {question}\")\n    return question\n\nprint(\"âœ… éªŒè¯é—®é¢˜ç”Ÿæˆå‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:39:57.887435Z","iopub.execute_input":"2025-11-04T03:39:57.887788Z","iopub.status.idle":"2025-11-04T03:39:57.893389Z","shell.execute_reply.started":"2025-11-04T03:39:57.887771Z","shell.execute_reply":"2025-11-04T03:39:57.892657Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ„å»ºæ ‡å‡†æ›²çº¿","metadata":{}},{"cell_type":"code","source":"def build_standard_curves(model, tokenizer, xp, intermediate_concept, validation_question):\n    \"\"\"\n    æ„å»ºå®Œæ•´æ¨ç†è·¯å¾„å’Œçº¯è·³æ­¥è·¯å¾„çš„æ ‡å‡†æ›²çº¿\n    \"\"\"\n    print(\"ğŸ”§ æ„å»ºæ ‡å‡†æ›²çº¿...\")\n    \n    # 1. å®Œæ•´æ¨ç†è·¯å¾„\n    full_reasoning = f\"{xp}\\n\\nStep-by-step reasoning:\\n1. Enter from east, walking west â†’ facing west\\n2. First right turn â†’ facing {intermediate_concept}\\n3. Second right turn â†’ facing east\\n4. Left turn â†’ facing north\\n\\nNow: {validation_question}\"\n    \n    # 2. çº¯è·³æ­¥è·¯å¾„  \n    shortcut_reasoning = f\"{xp}\\n\\nAfter all turns, facing north.\\n\\nNow: {validation_question}\"\n    \n    # è·å–æ¦‚ç‡æ›²çº¿\n    curves = {}\n    \n    try:\n        full_curve, layers = get_concept_probability_curve(model, tokenizer, full_reasoning, intermediate_concept)\n        curves['full_reasoning'] = (full_curve, layers)\n        print(f\"å®Œæ•´è·¯å¾„æ›²çº¿: å¹³å‡æ¦‚ç‡ = {np.mean(full_curve):.4f}\")\n    except Exception as e:\n        print(f\"å®Œæ•´è·¯å¾„é”™è¯¯: {e}\")\n    \n    try:\n        shortcut_curve, layers = get_concept_probability_curve(model, tokenizer, shortcut_reasoning, intermediate_concept)\n        curves['pure_shortcut'] = (shortcut_curve, layers)\n        print(f\"çº¯è·³æ­¥æ›²çº¿: å¹³å‡æ¦‚ç‡ = {np.mean(shortcut_curve):.4f}\")\n    except Exception as e:\n        print(f\"çº¯è·³æ­¥é”™è¯¯: {e}\")\n    \n    return curves\n\nprint(\"âœ… æ ‡å‡†æ›²çº¿æ„å»ºå‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:39:58.253639Z","iopub.execute_input":"2025-11-04T03:39:58.254201Z","iopub.status.idle":"2025-11-04T03:39:58.259831Z","shell.execute_reply.started":"2025-11-04T03:39:58.254183Z","shell.execute_reply":"2025-11-04T03:39:58.259265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æµ‹è¯•æ›²çº¿æå–","metadata":{}},{"cell_type":"code","source":"def extract_test_curve(model, tokenizer, xp, jumping_reasoning, validation_question, intermediate_concept):\n    \"\"\"\n    ä¸ºè·³æ­¥æ¨ç†æå–æµ‹è¯•æ›²çº¿\n    \"\"\"\n    print(\"ğŸ” æå–æµ‹è¯•æ›²çº¿...\")\n    \n    test_prompt = f\"{xp}\\n\\n{jumping_reasoning}\\n\\nNow: {validation_question}\"\n    \n    try:\n        test_curve, layers = get_concept_probability_curve(model, tokenizer, test_prompt, intermediate_concept)\n        print(f\"æµ‹è¯•æ›²çº¿: å¹³å‡æ¦‚ç‡ = {np.mean(test_curve):.4f}\")\n        return test_curve, layers\n    except Exception as e:\n        print(f\"æµ‹è¯•æ›²çº¿é”™è¯¯: {e}\")\n        return None, None\n\nprint(\"âœ… æµ‹è¯•æ›²çº¿æå–å‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:39:58.572650Z","iopub.execute_input":"2025-11-04T03:39:58.573129Z","iopub.status.idle":"2025-11-04T03:39:58.577853Z","shell.execute_reply.started":"2025-11-04T03:39:58.573110Z","shell.execute_reply":"2025-11-04T03:39:58.577157Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ç›¸ä¼¼åº¦åˆ†æ","metadata":{}},{"cell_type":"code","source":"def analyze_curve_similarity(test_curve, standard_curves):\n    \"\"\"\n    åˆ†ææµ‹è¯•æ›²çº¿ä¸æ ‡å‡†æ›²çº¿çš„ç›¸ä¼¼åº¦\n    \"\"\"\n    if test_curve is None:\n        return {}\n    \n    similarities = {}\n    \n    for curve_name, (std_curve, _) in standard_curves.items():\n        # ç¡®ä¿é•¿åº¦ä¸€è‡´\n        min_len = min(len(test_curve), len(std_curve))\n        test_trimmed = test_curve[:min_len]\n        std_trimmed = std_curve[:min_len]\n        \n        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n        cosine_sim = cosine_similarity(test_trimmed.reshape(1, -1), std_trimmed.reshape(1, -1))[0][0]\n        \n        # è®¡ç®—è¶‹åŠ¿ç›¸ä¼¼åº¦\n        test_diff = np.diff(test_trimmed)\n        std_diff = np.diff(std_trimmed)\n        if len(test_diff) > 0 and len(std_diff) > 0:\n            trend_sim = np.corrcoef(test_diff, std_diff)[0, 1] if not np.isnan(np.corrcoef(test_diff, std_diff)[0, 1]) else 0\n        else:\n            trend_sim = 0\n        \n        similarities[curve_name] = {\n            'cosine_similarity': cosine_sim,\n            'trend_similarity': trend_sim,\n            'overall_similarity': (cosine_sim + max(0, trend_sim)) / 2\n        }\n        \n        print(f\"ä¸ {curve_name} çš„ç›¸ä¼¼åº¦:\")\n        print(f\"  ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.4f}\")\n        print(f\"  è¶‹åŠ¿ç›¸ä¼¼åº¦: {trend_sim:.4f}\")\n        print(f\"  æ€»ä½“ç›¸ä¼¼åº¦: {similarities[curve_name]['overall_similarity']:.4f}\")\n    \n    return similarities\n\nprint(\"âœ… ç›¸ä¼¼åº¦åˆ†æå‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:40:03.383403Z","iopub.execute_input":"2025-11-04T03:40:03.383874Z","iopub.status.idle":"2025-11-04T03:40:03.391096Z","shell.execute_reply.started":"2025-11-04T03:40:03.383854Z","shell.execute_reply":"2025-11-04T03:40:03.390258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ–¹æ³•äºŒä¸»å®éªŒ","metadata":{}},{"cell_type":"code","source":"def method2_main_experiment(model, tokenizer, xp, intermediate_concept, final_answer, jumping_reasoning):\n    \"\"\"\n    æ–¹æ³•äºŒä¸»å®éªŒæµç¨‹\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"æ–¹æ³•äºŒï¼šLogit Lenséšå¼æ¨ç†éªŒè¯\")\n    print(\"=\" * 70)\n    \n    # 1. ç”ŸæˆéªŒè¯é—®é¢˜\n    print(\"\\nğŸ“ ç”ŸæˆéªŒè¯é—®é¢˜...\")\n    validation_question = generate_validation_question(model, tokenizer, xp, intermediate_concept, final_answer)\n    \n    # 2. æ„å»ºæ ‡å‡†æ›²çº¿\n    print(\"\\nğŸ“ˆ æ„å»ºæ ‡å‡†æ›²çº¿...\")\n    standard_curves = build_standard_curves(model, tokenizer, xp, intermediate_concept, validation_question)\n    \n    # 3. æå–æµ‹è¯•æ›²çº¿\n    print(\"\\nğŸ” æå–æµ‹è¯•æ›²çº¿...\")\n    test_curve, test_layers = extract_test_curve(model, tokenizer, xp, jumping_reasoning, validation_question, intermediate_concept)\n    \n    # 4. å¯è§†åŒ–å¯¹æ¯”\n    print(\"\\nğŸ“Š å¯è§†åŒ–å¯¹æ¯”...\")\n    if test_curve is not None:\n        # å‡†å¤‡ç»˜å›¾æ•°æ®\n        plot_data = {}\n        for name, (curve, layers) in standard_curves.items():\n            plot_data[name] = (curve, layers)\n        plot_data['test_jumping'] = (test_curve, test_layers)\n        \n        plot_curves(plot_data, f\"æ¦‚å¿µ '{intermediate_concept}' çš„æ¦‚ç‡æ›²çº¿å¯¹æ¯”\")\n    \n    # 5. ç›¸ä¼¼åº¦åˆ†æ\n    print(\"\\nğŸ”¬ ç›¸ä¼¼åº¦åˆ†æ...\")\n    similarities = analyze_curve_similarity(test_curve, standard_curves)\n    \n    # 6. å¾—å‡ºç»“è®º\n    print(\"\\nğŸ¯ ç»“è®º:\")\n    if similarities:\n        best_match = max(similarities.items(), key=lambda x: x[1]['overall_similarity'])\n        best_sim = best_match[1]['overall_similarity']\n        \n        print(f\"è·³æ­¥æ¨ç†æœ€æ¥è¿‘: {best_match[0]}\")\n        print(f\"ç›¸ä¼¼åº¦: {best_sim:.4f}\")\n        \n        if 'shortcut' in best_match[0]:\n            print(\"ğŸ’¡ æ¨¡å‹åœ¨è·³æ­¥æ—¶**æ²¡æœ‰**éšå¼æ¨ç†è¿™ä¸ªæ¦‚å¿µ\")\n        else:\n            print(\"ğŸ’¡ æ¨¡å‹åœ¨è·³æ­¥æ—¶**å·²ç»**éšå¼æ¨ç†äº†è¿™ä¸ªæ¦‚å¿µ\")\n    else:\n        print(\"âŒ æ— æ³•å¾—å‡ºæœ‰æ•ˆç»“è®º\")\n    \n    # ä¿å­˜ç»“æœ\n    results = {\n        'xp': xp,\n        'intermediate_concept': intermediate_concept,\n        'validation_question': validation_question,\n        'standard_curves': standard_curves,\n        'test_curve': test_curve,\n        'similarities': similarities,\n        'jumping_reasoning': jumping_reasoning\n    }\n    \n    save_results(results, \"method2_results.pkl\")\n    \n    return results\n\nprint(\"âœ… æ–¹æ³•äºŒä¸»å®éªŒå‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:40:05.726993Z","iopub.execute_input":"2025-11-04T03:40:05.727291Z","iopub.status.idle":"2025-11-04T03:40:05.735611Z","shell.execute_reply.started":"2025-11-04T03:40:05.727269Z","shell.execute_reply":"2025-11-04T03:40:05.734956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ„é€ å®éªŒ","metadata":{}},{"cell_type":"code","source":"# =====================================================\n# Cell 8: æ‰§è¡Œæ–¹æ³•äºŒå®éªŒï¼ˆä»æ–‡ä»¶åŠ è½½æ–¹æ³•ä¸€ç»“æœï¼‰\n# =====================================================\n\nprint(\"ğŸš€ å¼€å§‹æ‰§è¡Œæ–¹æ³•äºŒå®éªŒ...\")\n\ndef load_method1_results(filename=\"complete_skipping_analysis.pkl\"):\n    \"\"\"ä»æ–‡ä»¶åŠ è½½æ–¹æ³•ä¸€çš„ç»“æœ\"\"\"\n    try:\n        with open(filename, \"rb\") as f:\n            method1_results = pickle.load(f)\n        print(f\"âœ“ æˆåŠŸåŠ è½½æ–¹æ³•ä¸€ç»“æœ from {filename}\")\n        return method1_results\n    except FileNotFoundError:\n        print(f\"âŒ æ‰¾ä¸åˆ°æ–¹æ³•ä¸€ç»“æœæ–‡ä»¶: {filename}\")\n        return None\n\ndef extract_concepts_from_method1(method1_results):\n    \"\"\"ä»æ–¹æ³•ä¸€ç»“æœä¸­ç²¾ç¡®æå–ä¸­é—´æ¦‚å¿µå’Œæœ€ç»ˆç­”æ¡ˆ\"\"\"\n    \n    if method1_results is None:\n        return \"north\", \"north\"  # é»˜è®¤å€¼\n    \n    step_data = method1_results.get('step_data', [])\n    skipping_info = method1_results.get('skipping_info', {})\n    \n    intermediate_concept = None\n    final_answer = None\n    \n    # ä»è·³æ­¥ä¿¡æ¯ä¸­æå–è·³è¿‡çš„å†…å®¹\n    skipped_content = skipping_info.get('skipped_content', '')\n    print(f\"è·³è¿‡çš„å†…å®¹: {skipped_content}\")\n    \n    # åˆ†æè·³è¿‡çš„æ­¥éª¤æ¥ç¡®å®šä¸­é—´æ¦‚å¿µ\n    if 'first right turn' in skipped_content.lower():\n        # ç¬¬ä¸€ä¸ªå³è½¬ï¼šwest â†’ north\n        intermediate_concept = \"north\"\n    elif 'second right turn' in skipped_content.lower():\n        # ç¬¬äºŒä¸ªå³è½¬ï¼šnorth â†’ east  \n        intermediate_concept = \"east\"\n    elif 'left turn' in skipped_content.lower():\n        # å·¦è½¬ï¼šeast â†’ north\n        intermediate_concept = \"north\"\n    \n    # ä»æ­¥éª¤æ•°æ®ä¸­æå–æœ€ç»ˆç­”æ¡ˆ\n    for step in step_data:\n        step_text = step['step_text'].lower()\n        \n        # å¯»æ‰¾æœ€ç»ˆç­”æ¡ˆæ­¥éª¤\n        if any(keyword in step_text for keyword in ['final', 'answer', 'conclusion']):\n            if 'north' in step_text:\n                final_answer = \"north\"\n            elif 'south' in step_text:\n                final_answer = \"south\" \n            elif 'east' in step_text:\n                final_answer = \"east\"\n            elif 'west' in step_text:\n                final_answer = \"west\"\n            break\n    \n    # å¦‚æœè¿˜æ˜¯æ— æ³•ç¡®å®šï¼Œä½¿ç”¨é—®é¢˜é€»è¾‘æ¨æ–­\n    if intermediate_concept is None:\n        # æ ¹æ®æ–¹å‘æ¨ç†é—®é¢˜çš„æ ‡å‡†é€»è¾‘\n        # è¿›å…¥æ–¹å‘eastï¼Œèµ°west â†’ åˆå§‹æ–¹å‘west\n        # ç¬¬ä¸€ä¸ªå³è½¬ï¼šwest â†’ north\n        intermediate_concept = \"north\"\n    \n    if final_answer is None:\n        final_answer = \"north\"  # æ ‡å‡†ç­”æ¡ˆ\n    \n    return intermediate_concept, final_answer\n\n# åŠ è½½æ–¹æ³•ä¸€ç»“æœ\nmethod1_results = load_method1_results()\n\nif method1_results is None:\n    print(\"âŒ æ— æ³•åŠ è½½æ–¹æ³•ä¸€ç»“æœï¼Œä½¿ç”¨æ–¹æ³•äºŒå®éªŒ\")\n    # ä½¿ç”¨é»˜è®¤å€¼\n    XP = \"If you enter a building from the east side, walking west, and then take two rights and a left down corridors, which direction are you facing?\"\n    intermediate_concept = \"north\"\n    final_answer = \"north\"\n    jumping_reasoning = \"After entering from east and walking west, then taking two rights and a left, you end up facing north.\"\nelse:\n    # ä»æ–¹æ³•ä¸€ç»“æœä¸­æå–ä¿¡æ¯\n    XP = method1_results.get('XP', \"If you enter a building from the east side, walking west, and then take two rights and a left down corridors, which direction are you facing?\")\n    jumping_reasoning = method1_results.get('step_by_step_response', \"After entering from east and walking west, then taking two rights and a left, you end up facing north.\")\n    intermediate_concept, final_answer = extract_concepts_from_method1(method1_results)\n\nprint(f\"ä½¿ç”¨çš„XP: {XP}\")\nprint(f\"æå–çš„ä¸­é—´æ¦‚å¿µ: {intermediate_concept}\")\nprint(f\"æå–çš„æœ€ç»ˆç­”æ¡ˆ: {final_answer}\")\nprint(f\"è·³æ­¥æ¨ç†æ–‡æœ¬é¢„è§ˆ: {jumping_reasoning[:100]}...\")\n\n# æ‰§è¡Œæ–¹æ³•äºŒå®éªŒ\nmethod2_results = method2_main_experiment(\n    model, tokenizer, \n    XP, intermediate_concept, final_answer, jumping_reasoning\n)\n\nprint(\"\\nğŸ‰ æ–¹æ³•äºŒå®éªŒå®Œæˆï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:40:08.983750Z","iopub.execute_input":"2025-11-04T03:40:08.984470Z","iopub.status.idle":"2025-11-04T03:40:13.983597Z","shell.execute_reply.started":"2025-11-04T03:40:08.984443Z","shell.execute_reply":"2025-11-04T03:40:13.982988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ–¹æ³•ä¸‰","metadata":{}},{"cell_type":"code","source":"# =====================================================\n# Cell 1: æ–¹æ³•ä¸‰åŸºç¡€è®¾ç½®å’Œtuned lenså¯¼å…¥\n# =====================================================\n\nimport torch\nimport pickle\nimport numpy as np\nfrom tuned_lens import TunedLens\nimport re\n\ndef build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    return prompt\n\ndef save_results(data_dict, filename):\n    \"\"\"ä¿å­˜å®éªŒç»“æœ\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n    print(f\"âœ“ æ•°æ®å·²ä¿å­˜è‡³ {filename}\")\n\nprint(\"âœ… æ–¹æ³•ä¸‰åŸºç¡€è®¾ç½®å®Œæˆ\")\n\n# =====================================================\n# Cell 2: ä¿®å¤çš„Tuned Lensæ€§èƒ½åˆ†è®¡ç®—å™¨\n# =====================================================\n\nclass FixedTunedLensScorer:\n    \"\"\"ä¿®å¤çš„Tuned Lensæ€§èƒ½åˆ†è®¡ç®—å™¨\"\"\"\n    \n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = model.device\n        \n        # åˆå§‹åŒ–tuned lens\n        print(\"åˆå§‹åŒ–Tuned Lens...\")\n        try:\n            self.tuned_lens = TunedLens.from_model(model)\n            print(\"âœ“ Tuned Lensåˆå§‹åŒ–æˆåŠŸ\")\n        except Exception as e:\n            print(f\"âŒ Tuned Lensåˆå§‹åŒ–å¤±è´¥: {e}\")\n            self.tuned_lens = None\n    \n    def ensure_tensor_on_device(self, tensor):\n        \"\"\"ç¡®ä¿tensoråœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š\"\"\"\n        if isinstance(tensor, np.ndarray):\n            tensor = torch.tensor(tensor)\n        \n        if tensor.device != self.device:\n            tensor = tensor.to(self.device)\n        \n        return tensor\n    \n    def calculate_performance_score(self, hidden_state, target_answer):\n        \"\"\"\n        ä½¿ç”¨Tuned Lensè®¡ç®—æ­£ç¡®ç­”æ¡ˆtokençš„æ¦‚ç‡\n        \"\"\"\n        try:\n            # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®è®¾å¤‡ä¸Š\n            hidden_state = self.ensure_tensor_on_device(hidden_state)\n            \n            # è·å–ç›®æ ‡ç­”æ¡ˆçš„token\n            target_tokens = self.tokenizer.encode(target_answer, add_special_tokens=False)\n            if not target_tokens:\n                return 0.0, []\n            \n            target_token = target_tokens[0]  # å–ç¬¬ä¸€ä¸ªtoken\n            \n            with torch.no_grad():\n                # é‡å¡‘hidden stateä¸º [1, 1, hidden_dim]\n                hidden_state_3d = hidden_state.unsqueeze(0).unsqueeze(0)\n                \n                if self.tuned_lens is not None:\n                    # ä¿®å¤ï¼šæ­£ç¡®ä½¿ç”¨tuned lensï¼Œä¸ä¼ é€’layer_idxå‚æ•°\n                    # tuned lensä¼šè¿”å›æ‰€æœ‰å±‚çš„logits\n                    all_layers_logits = self.tuned_lens(hidden_state_3d)\n                    # å–æœ€åä¸€å±‚çš„logits\n                    logits = all_layers_logits[-1][0, -1, :]  # [vocab_size]\n                else:\n                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨LM head\n                    lm_head = self.model.lm_head\n                    logits = lm_head(hidden_state_3d)[0, -1, :]\n                \n                # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ\n                probs = torch.softmax(logits, dim=0)\n                \n                # è·å–æ­£ç¡®ç­”æ¡ˆtokençš„æ¦‚ç‡\n                correct_token_prob = probs[target_token].item()\n                \n            return correct_token_prob, [target_token]\n            \n        except Exception as e:\n            print(f\"æ€§èƒ½åˆ†è®¡ç®—é”™è¯¯: {e}\")\n            return 0.0, []\n    \n    def get_direct_answer_probability(self, prompt, target_answer):\n        \"\"\"ç›´æ¥å¯¹å®Œæ•´promptè®¡ç®—æ­£ç¡®ç­”æ¡ˆtokençš„æ¦‚ç‡\"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs, output_hidden_states=True)\n            last_hidden = outputs.hidden_states[-1][0, -1, :]\n            \n            probability, _ = self.calculate_performance_score(last_hidden, target_answer)\n        \n        return probability\n\nprint(\"âœ… ä¿®å¤çš„Tuned Lensæ€§èƒ½åˆ†è®¡ç®—å™¨å®Œæˆ\")\n\n# =====================================================\n# Cell 3: ç”¨LLMç”Ÿæˆå¤æ‚éªŒè¯é—®é¢˜å’Œç›´æ¥ç­”æ¡ˆ\n# =====================================================\n\ndef generate_complex_problems_with_answers(model, tokenizer, base_xp, intermediate_concepts):\n    \"\"\"\n    ç”¨LLMç”Ÿæˆå¤æ‚éªŒè¯é—®é¢˜å’Œç›´æ¥ç­”æ¡ˆï¼ˆæ— æ¨ç†è¿‡ç¨‹ï¼‰\n    \"\"\"\n    system_msg = \"You are an AI assistant that provides direct answers without reasoning steps.\"\n    \n    user_msg = f\"\"\"Base scenario: {base_xp}\n\nGenerate TWO complex problems and provide ONLY the direct answers (no reasoning):\n\n1. Problem requiring knowledge of \"{intermediate_concepts[0]}\":\n   - Create a complex question that can ONLY be solved if you know \"{intermediate_concepts[0]}\"\n   - Provide ONLY the final answer\n\n2. Problem requiring knowledge of \"{intermediate_concepts[1]}\":\n   - Create a complex question that can ONLY be solved if you know \"{intermediate_concepts[1]}\"  \n   - Provide ONLY the final answer\n\nFormat as:\nPROBLEM_A: [question 1]\nANSWER_A: [direct answer 1]\nPROBLEM_B: [question 2]  \nANSWER_B: [direct answer 2]\n\nDO NOT include any reasoning steps, explanations, or step-by-step solutions.\"\"\"\n\n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    llm_response = response.split(\"assistant\")[-1].strip() if \"assistant\" in response else response\n    \n    print(\"LLMç”Ÿæˆçš„å¤æ‚é—®é¢˜å’Œç›´æ¥ç­”æ¡ˆ:\")\n    print(llm_response)\n    \n    # è§£æé—®é¢˜å’Œç­”æ¡ˆ\n    problems = {\n        'P1': {'required_concept': intermediate_concepts[0]},\n        'P2': {'required_concept': intermediate_concepts[1]}\n    }\n    \n    lines = llm_response.split('\\n')\n    \n    for line in lines:\n        line = line.strip()\n        if 'PROBLEM_A:' in line:\n            problems['P1']['problem'] = line.split('PROBLEM_A:')[-1].strip()\n        elif 'ANSWER_A:' in line:\n            problems['P1']['answer'] = line.split('ANSWER_A:')[-1].strip()\n        elif 'PROBLEM_B:' in line:\n            problems['P2']['problem'] = line.split('PROBLEM_B:')[-1].strip()\n        elif 'ANSWER_B:' in line:\n            problems['P2']['answer'] = line.split('ANSWER_B:')[-1].strip()\n    \n    # è®¾ç½®é»˜è®¤å€¼ï¼ˆå¦‚æœLLMæ²¡æœ‰ç”Ÿæˆï¼‰\n    for key in ['P1', 'P2']:\n        if 'problem' not in problems[key]:\n            concept = problems[key]['required_concept']\n            problems[key]['problem'] = f\"If you are facing {concept}, what direction is behind you?\"\n            problems[key]['answer'] = \"south\" if concept == \"north\" else \"west\"\n    \n    # æ‰“å°æœ€ç»ˆé—®é¢˜\n    for key, data in problems.items():\n        print(f\"  {key}: {data['problem']}\")\n        print(f\"    ç›´æ¥ç­”æ¡ˆ: {data['answer']}\")\n        print(f\"    éœ€è¦æ¦‚å¿µ: {data['required_concept']}\")\n    \n    return problems\n\nprint(\"âœ… å¤æ‚é—®é¢˜å’Œç›´æ¥ç­”æ¡ˆç”Ÿæˆå®Œæˆ\")\n\n# =====================================================\n# Cell 4: å¯è¡Œæ€§éªŒè¯å®éªŒ\n# =====================================================\n\ndef feasibility_validation(scorer, base_xp, complex_problems, hs_jump):\n    \"\"\"\n    è¿›è¡Œå¯è¡Œæ€§éªŒè¯ï¼šæµ‹è¯•æ­£ç¡®ç­”æ¡ˆtokençš„æ¦‚ç‡\n    \"\"\"\n    print(\"ğŸ” è¿›è¡Œå¯è¡Œæ€§éªŒè¯...\")\n    \n    feasibility_results = {}\n    \n    # æµ‹è¯•1: ç›´æ¥æä¾›ä¸­é—´æ¦‚å¿µçš„åŸºçº¿æ¦‚ç‡\n    concept = complex_problems['P1']['required_concept']\n    problem = complex_problems['P1']['problem']\n    answer = complex_problems['P1']['answer']\n    \n    baseline_prompt = f\"{base_xp}\\n\\nWe know: {concept}\\n\\nProblem: {problem}\"\n    baseline_prob = scorer.get_direct_answer_probability(baseline_prompt, answer)\n    feasibility_results['baseline_probability'] = baseline_prob\n    print(f\"åŸºçº¿æ¦‚ç‡ (æä¾›æ¦‚å¿µ{concept}): {baseline_prob:.4f}\")\n    \n    # æµ‹è¯•2: ä½¿ç”¨hidden stateçš„æ¦‚ç‡\n    try:\n        # æ„å»ºåªæœ‰é—®é¢˜çš„promptæ¥è·å–é—®é¢˜ç¼–ç çš„hidden state\n        problem_only_prompt = f\"{base_xp}\\n\\nProblem: {problem}\"\n        inputs = scorer.tokenizer(problem_only_prompt, return_tensors=\"pt\").to(scorer.device)\n        \n        with torch.no_grad():\n            outputs = scorer.model(**inputs, output_hidden_states=True)\n            problem_hidden = outputs.hidden_states[-1][0, -1, :]\n        \n        # ç»„åˆhidden states (å¹³å‡)\n        hs_jump_tensor = scorer.ensure_tensor_on_device(hs_jump)\n        combined_hidden = (hs_jump_tensor + problem_hidden) / 2\n        \n        hidden_state_prob, _ = scorer.calculate_performance_score(combined_hidden, answer)\n        feasibility_results['hidden_state_probability'] = hidden_state_prob\n        print(f\"Hidden stateæ¦‚ç‡: {hidden_state_prob:.4f}\")\n        \n    except Exception as e:\n        print(f\"Hidden stateæµ‹è¯•é”™è¯¯: {e}\")\n        feasibility_results['hidden_state_probability'] = 0.0\n    \n    # åˆ¤æ–­å¯è¡Œæ€§\n    feasibility_results['is_feasible'] = (\n        feasibility_results['baseline_probability'] > 0.1 and \n        feasibility_results['hidden_state_probability'] > 0.05\n    )\n    \n    print(f\"å®éªŒå¯è¡Œæ€§: {feasibility_results['is_feasible']}\")\n    \n    return feasibility_results\n\nprint(\"âœ… å¯è¡Œæ€§éªŒè¯å®Œæˆ\")\n\n# =====================================================\n# Cell 5: è·¯å¾„æ¨æ–­äº¤å‰æ£€éªŒ\n# =====================================================\n\ndef cross_validation_experiment(scorer, base_xp, complex_problems, hs_jump):\n    \"\"\"\n    æ‰§è¡Œè·¯å¾„æ¨æ–­çš„äº¤å‰æ£€éªŒ\n    \"\"\"\n    print(\"ğŸ”„ æ‰§è¡Œè·¯å¾„æ¨æ–­äº¤å‰æ£€éªŒ...\")\n    \n    validation_results = {}\n    test_probs = {}\n    \n    # æ£€éªŒç»„: æµ‹è¯•hs_jumpåœ¨ä¸åŒé—®é¢˜ä¸Šçš„æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡\n    for problem_key, problem_data in complex_problems.items():\n        problem_text = problem_data['problem']\n        target_answer = problem_data['answer']\n        required_concept = problem_data['required_concept']\n        \n        # æ„å»ºæµ‹è¯•prompt\n        test_prompt = f\"{base_xp}\\n\\nProblem: {problem_text}\"\n        \n        # è·å–é—®é¢˜ç¼–ç çš„hidden state\n        inputs = scorer.tokenizer(test_prompt, return_tensors=\"pt\").to(scorer.device)\n        with torch.no_grad():\n            outputs = scorer.model(**inputs, output_hidden_states=True)\n            problem_hidden = outputs.hidden_states[-1][0, -1, :]\n        \n        # ç»„åˆhidden stateså¹¶è®¡ç®—æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡\n        hs_jump_tensor = scorer.ensure_tensor_on_device(hs_jump)\n        combined_hidden = (hs_jump_tensor + problem_hidden) / 2\n        \n        correct_prob, predictions = scorer.calculate_performance_score(combined_hidden, target_answer)\n        \n        test_probs[problem_key] = {\n            'correct_probability': correct_prob,\n            'required_concept': required_concept,\n            'target_answer': target_answer,\n            'predictions': predictions\n        }\n        \n        print(f\"é—®é¢˜ {problem_key} (éœ€è¦{required_concept}) æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡: {correct_prob:.4f}\")\n    \n    validation_results['test_probs'] = test_probs\n    \n    # å¯¹ç…§ç»„: ç¡®ç«‹æ¦‚ç‡ä¸Šé™\n    print(\"\\nç¡®ç«‹æ¦‚ç‡ä¸Šé™...\")\n    control_probs = {}\n    \n    for problem_key, problem_data in complex_problems.items():\n        concept = problem_data['required_concept']\n        problem_text = problem_data['problem']\n        target_answer = problem_data['answer']\n        \n        # ç”ŸæˆåŒ…å«æ˜ç¡®æ¦‚å¿µçš„prompt\n        explicit_prompt = f\"{base_xp}\\n\\nWe know: {concept}\\n\\nProblem: {problem_text}\"\n        control_prob = scorer.get_direct_answer_probability(explicit_prompt, target_answer)\n        control_probs[concept] = control_prob\n        print(f\"æ˜ç¡®æä¾›æ¦‚å¿µ {concept} çš„æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡: {control_prob:.4f}\")\n    \n    validation_results['control_probs'] = control_probs\n    \n    return validation_results\n\nprint(\"âœ… äº¤å‰æ£€éªŒå®Œæˆ\")\n\n# =====================================================\n# Cell 6: ç»“æœåˆ†æå’Œç»“è®º\n# =====================================================\n\ndef analyze_cross_validation_results(validation_results, intermediate_concepts):\n    \"\"\"\n    åˆ†æäº¤å‰æ£€éªŒç»“æœå¹¶å¾—å‡ºç»“è®º\n    \"\"\"\n    print(\"\\nğŸ“Š åˆ†æäº¤å‰æ£€éªŒç»“æœ...\")\n    \n    test_probs = validation_results['test_probs']\n    control_probs = validation_results['control_probs']\n    \n    # æå–æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡\n    prob_E = test_probs['P1']['correct_probability']  # éœ€è¦æ¦‚å¿µA1çš„é—®é¢˜\n    prob_F = test_probs['P2']['correct_probability']  # éœ€è¦æ¦‚å¿µA2çš„é—®é¢˜\n    \n    print(f\"æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡å¯¹æ¯”:\")\n    print(f\"  é—®é¢˜P1 (éœ€è¦{intermediate_concepts[0]}): {prob_E:.4f}\")\n    print(f\"  é—®é¢˜P2 (éœ€è¦{intermediate_concepts[1]}): {prob_F:.4f}\")\n    \n    # æ¦‚ç‡ä¸Šé™å‚è€ƒ\n    control_A1 = control_probs.get(intermediate_concepts[0], 0)\n    control_A2 = control_probs.get(intermediate_concepts[1], 0)\n    print(f\"æ¦‚ç‡ä¸Šé™å‚è€ƒ:\")\n    print(f\"  æ˜ç¡®æä¾›{intermediate_concepts[0]}: {control_A1:.4f}\")\n    print(f\"  æ˜ç¡®æä¾›{intermediate_concepts[1]}: {control_A2:.4f}\")\n    \n    # å¾—å‡ºç»“è®º\n    print(\"\\nğŸ¯ å®éªŒç»“è®º:\")\n    \n    threshold = 0.05  # æ¦‚ç‡å·®å¼‚é˜ˆå€¼\n    \n    if abs(prob_E - prob_F) < threshold:\n        print(\"ğŸ’¡ è¯æ®æ”¯æŒ'çº¯è·³æ­¥'å‡è®¾\")\n        print(\"   - hs_jumpä¸­çš„ä¿¡æ¯æ˜¯æ··æ²Œæˆ–æ— æ•ˆçš„\")\n        print(\"   - åœ¨ä¸¤ä¸ªé—®é¢˜ä¸Šçš„æ­£ç¡®ç­”æ¡ˆæ¦‚ç‡æ— æ˜¾è‘—å·®å¼‚\")\n        \n    elif prob_E > prob_F + threshold:\n        print(\"ğŸ’¡ å¼ºè¯æ®è¡¨æ˜hs_jumpä¸­ç¼–ç äº†æ¦‚å¿µA1çš„ä¿¡æ¯\")\n        print(f\"   - åœ¨éœ€è¦{intermediate_concepts[0]}çš„é—®é¢˜ä¸Šæ­£ç¡®ç­”æ¡ˆæ¦‚ç‡æ›´é«˜\")\n        print(f\"   - æ¦‚ç‡å·®å¼‚: {prob_E - prob_F:.4f}\")\n        \n    elif prob_F > prob_E + threshold:\n        print(\"ğŸ’¡ å¼ºè¯æ®è¡¨æ˜hs_jumpä¸­ç¼–ç äº†æ¦‚å¿µA2çš„ä¿¡æ¯\") \n        print(f\"   - åœ¨éœ€è¦{intermediate_concepts[1]}çš„é—®é¢˜ä¸Šæ­£ç¡®ç­”æ¡ˆæ¦‚ç‡æ›´é«˜\")\n        print(f\"   - æ¦‚ç‡å·®å¼‚: {prob_F - prob_E:.4f}\")\n        \n    else:\n        print(\"âš ï¸ ç»“æœä¸æ˜ç¡®ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ\")\n    \n    return {\n        'probability_E': prob_E,\n        'probability_F': prob_F,\n        'probability_difference': prob_E - prob_F,\n        'absolute_difference': abs(prob_E - prob_F),\n        'control_A1': control_A1,\n        'control_A2': control_A2,\n        'conclusion': 'pure_shortcut' if abs(prob_E - prob_F) < threshold else \n                     'encodes_A1' if prob_E > prob_F else 'encodes_A2'\n    }\n\nprint(\"âœ… ç»“æœåˆ†æå®Œæˆ\")\n\n# =====================================================\n# Cell 7: æ–¹æ³•ä¸‰ä¸»å®éªŒ\n# =====================================================\n\ndef method3_main_experiment(model, tokenizer):\n    \"\"\"\n    æ–¹æ³•ä¸‰ä¸»å®éªŒæµç¨‹\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"æ–¹æ³•ä¸‰ï¼šåŸºäºTuned Lensçš„æ€§èƒ½å®è¯åˆ†æ\")\n    print(\"=\" * 70)\n    \n    try:\n        # åŠ è½½æ–¹æ³•ä¸€çš„ç»“æœ\n        with open(\"method1_jump_hidden_state.pkl\", \"rb\") as f:\n            method1_data = pickle.load(f)\n        \n        hs_jump = method1_data['hs_jump']\n        XP = method1_data['XP']\n        skipping_info = method1_data['skipping_info']\n        \n        print(f\"æˆåŠŸåŠ è½½æ–¹æ³•ä¸€æ•°æ®\")\n        print(f\"è·³æ­¥ä½ç½®: {skipping_info.get('skipping_location', 'æœªçŸ¥')}\")\n        print(f\"hs_jumpå½¢çŠ¶: {hs_jump.shape}\")\n        \n    except FileNotFoundError:\n        print(\"âŒ æ‰¾ä¸åˆ°æ–¹æ³•ä¸€çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ–¹æ³•ä¸€å®éªŒ\")\n        print(\"è¯·ç¡®ä¿æ–¹æ³•ä¸€å·²è¿è¡Œå¹¶ç”Ÿæˆäº† 'method1_jump_hidden_state.pkl' æ–‡ä»¶\")\n        return None\n    \n    # è®¾ç½®ä¸­é—´æ¦‚å¿µ\n    intermediate_concepts = [\"north\", \"east\"]\n    \n    # åˆå§‹åŒ–ä¿®å¤çš„Tuned Lensè®¡ç®—å™¨\n    scorer = FixedTunedLensScorer(model, tokenizer)\n    \n    # é˜¶æ®µ1: ç”Ÿæˆå¤æ‚éªŒè¯é—®é¢˜å’Œç›´æ¥ç­”æ¡ˆ\n    print(\"\\nğŸ“ ç”Ÿæˆå¤æ‚éªŒè¯é—®é¢˜å’Œç›´æ¥ç­”æ¡ˆ...\")\n    complex_problems = generate_complex_problems_with_answers(\n        model, tokenizer, XP, intermediate_concepts\n    )\n    \n    # é˜¶æ®µ2: å¯è¡Œæ€§éªŒè¯\n    print(\"\\nğŸ” è¿›è¡Œå¯è¡Œæ€§éªŒè¯...\")\n    feasibility_results = feasibility_validation(scorer, XP, complex_problems, hs_jump)\n    \n    if not feasibility_results['is_feasible']:\n        print(\"âš ï¸ å®éªŒè®¾ç½®å¯è¡Œæ€§è¾ƒä½ï¼Œä½†ç»§ç»­æ‰§è¡Œ...\")\n    \n    # é˜¶æ®µ3: è·¯å¾„æ¨æ–­äº¤å‰æ£€éªŒ\n    print(\"\\nğŸ”„ æ‰§è¡Œäº¤å‰æ£€éªŒ...\")\n    validation_results = cross_validation_experiment(scorer, XP, complex_problems, hs_jump)\n    \n    # é˜¶æ®µ4: ç»“æœåˆ†æ\n    print(\"\\nğŸ“Š åˆ†æç»“æœ...\")\n    analysis_results = analyze_cross_validation_results(validation_results, intermediate_concepts)\n    \n    # ä¿å­˜å®Œæ•´ç»“æœ\n    final_results = {\n        'XP': XP,\n        'intermediate_concepts': intermediate_concepts,\n        'complex_problems': complex_problems,\n        'feasibility_results': feasibility_results,\n        'validation_results': validation_results,\n        'analysis_results': analysis_results,\n        'hs_jump_shape': hs_jump.shape,\n        'tuned_lens_used': scorer.tuned_lens is not None\n    }\n    \n    save_results(final_results, \"method3_tuned_lens_analysis.pkl\")\n    \n    print(f\"\\nâœ… æ–¹æ³•ä¸‰å®éªŒå®Œæˆï¼\")\n    print(f\"âœ“ ä½¿ç”¨äº†Tuned Lens: {scorer.tuned_lens is not None}\")\n    print(f\"âœ“ åŸºäºæ­£ç¡®ç­”æ¡ˆtokenæ¦‚ç‡çš„åˆ†æå·²å®Œæˆ\")\n    \n    return final_results\n\nprint(\"âœ… æ–¹æ³•ä¸‰ä¸»å®éªŒå®Œæˆ\")\n\n# =====================================================\n# Cell 8: æ‰§è¡Œæ–¹æ³•ä¸‰å®éªŒ\n# =====================================================\n\nprint(\"ğŸš€ å¼€å§‹æ‰§è¡Œæ–¹æ³•ä¸‰å®éªŒ...\")\nmethod3_results = method3_main_experiment(model, tokenizer)\n\nif method3_results:\n    print(\"\\nğŸ‰ æ–¹æ³•ä¸‰å®éªŒæˆåŠŸå®Œæˆï¼\")\nelse:\n    print(\"\\nâŒ æ–¹æ³•ä¸‰å®éªŒå¤±è´¥\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:58:47.974770Z","iopub.execute_input":"2025-11-04T03:58:47.975521Z","iopub.status.idle":"2025-11-04T03:58:58.215740Z","shell.execute_reply.started":"2025-11-04T03:58:47.975493Z","shell.execute_reply":"2025-11-04T03:58:58.214934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### åŸºç¡€è®¾ç½®å’Œå·¥å…·å‡½æ•°","metadata":{}},{"cell_type":"code","source":"def build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n    return prompt\n\ndef save_results(data_dict, filename):\n    \"\"\"ä¿å­˜å®éªŒç»“æœ\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n    print(f\"âœ“ ç»“æœå·²ä¿å­˜è‡³ {filename}\")\n\nprint(\"âœ… æ–¹æ³•ä¸‰åŸºç¡€è®¾ç½®å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:25:45.279587Z","iopub.execute_input":"2025-11-04T03:25:45.279876Z","iopub.status.idle":"2025-11-04T03:25:45.285226Z","shell.execute_reply.started":"2025-11-04T03:25:45.279856Z","shell.execute_reply":"2025-11-04T03:25:45.284247Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tuned Lensæ€§èƒ½åˆ†è®¡ç®—","metadata":{}},{"cell_type":"code","source":"class PerformanceScorer:\n    \"\"\"åŸºäºTuned Lensçš„æ€§èƒ½åˆ†è®¡ç®—å™¨\"\"\"\n    \n    def __init__(self, model, tokenizer, tuned_lens=None):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = model.device\n        \n        # åˆå§‹åŒ–tuned lens\n        if tuned_lens is None:\n            print(\"åˆå§‹åŒ–é»˜è®¤Tuned Lens...\")\n            self.tuned_lens = TunedLens.from_model(model)\n        else:\n            self.tuned_lens = tuned_lens\n    \n    def calculate_performance_score(self, hidden_state, target_answer, layer_idx=-1):\n        \"\"\"\n        è®¡ç®—ç»™å®šhidden stateåœ¨ç›®æ ‡ç­”æ¡ˆä¸Šçš„æ€§èƒ½åˆ†\n        \n        Args:\n            hidden_state: éšè—çŠ¶æ€ [hidden_dim]\n            target_answer: ç›®æ ‡ç­”æ¡ˆå­—ç¬¦ä¸²\n            layer_idx: ä½¿ç”¨çš„å±‚ç´¢å¼•\n            \n        Returns:\n            performance_score: æ€§èƒ½åˆ† (0-1)\n            predictions: é¢„æµ‹çš„tokenåˆ—è¡¨\n        \"\"\"\n        # ç¡®ä¿æ•°æ®åœ¨ç›¸åŒè®¾å¤‡ä¸Š\n        hidden_state = hidden_state.to(self.device)\n        \n        # è·å–ç›®æ ‡ç­”æ¡ˆçš„token\n        target_tokens = self.tokenizer.encode(target_answer, add_special_tokens=False)\n        if not target_tokens:\n            return 0.0, []\n        \n        # ä½¿ç”¨tuned lensè¿›è¡Œå±‚é—´æŠ•å°„\n        with torch.no_grad():\n            # é‡å¡‘hidden stateä¸º [1, 1, hidden_dim]\n            hidden_state_3d = hidden_state.unsqueeze(0).unsqueeze(0)\n            \n            # ä½¿ç”¨tuned lensè·å–logits\n            logits = self.tuned_lens(hidden_state_3d, layer_idx=layer_idx)\n            logits = logits[0, -1, :]  # å–æœ€åä¸€ä¸ªä½ç½®çš„logits [vocab_size]\n            \n            # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ\n            probs = torch.softmax(logits, dim=0)\n            \n            # å¯¹ç›®æ ‡tokenåºåˆ—è®¡ç®—æ€§èƒ½åˆ†\n            correct_predictions = 0\n            predictions = []\n            \n            for i, target_token in enumerate(target_tokens):\n                # è·å–é¢„æµ‹çš„tokenï¼ˆæ¦‚ç‡æœ€é«˜çš„tokenï¼‰\n                predicted_token = torch.argmax(probs).item()\n                predictions.append(predicted_token)\n                \n                # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®\n                if predicted_token == target_token:\n                    correct_predictions += 1\n                \n                # å¯¹äºå¤štokenè¾“å‡ºï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼šåªè®¡ç®—ç¬¬ä¸€ä¸ªtokençš„æ€§èƒ½\n                # å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åºåˆ—ç”Ÿæˆé€»è¾‘\n                break  # æš‚æ—¶åªè®¡ç®—ç¬¬ä¸€ä¸ªtoken\n            \n            performance_score = correct_predictions / len(target_tokens)\n            \n        return performance_score, predictions\n    \n    def score_reasoning_chain(self, prompt, target_answer):\n        \"\"\"\n        ç›´æ¥å¯¹å®Œæ•´æ¨ç†é“¾è®¡ç®—æ€§èƒ½åˆ†\n        \"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model(**inputs, output_hidden_states=True)\n            last_hidden = outputs.hidden_states[-1][0, -1, :]  # æœ€åä¸€å±‚æœ€åä¸€ä¸ªtoken\n            \n            performance_score, predictions = self.calculate_performance_score(\n                last_hidden, target_answer\n            )\n        \n        return performance_score\n\nprint(\"âœ… Tuned Lensæ€§èƒ½åˆ†è®¡ç®—å™¨å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:25:47.876460Z","iopub.execute_input":"2025-11-04T03:25:47.877148Z","iopub.status.idle":"2025-11-04T03:25:47.891095Z","shell.execute_reply.started":"2025-11-04T03:25:47.877114Z","shell.execute_reply":"2025-11-04T03:25:47.890264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ç”¨LLMç”Ÿæˆå¤æ‚éªŒè¯é—®é¢˜","metadata":{}},{"cell_type":"code","source":"def generate_complex_validation_problems(model, tokenizer, base_xp, intermediate_concepts):\n    \"\"\"\n    ç”¨LLMç”Ÿæˆå¤æ‚çš„éªŒè¯é—®é¢˜å¯¹\n    \"\"\"\n    system_msg = \"You are an expert at creating complex reasoning validation problems.\"\n    \n    user_msg = f\"\"\"Base scenario: {base_xp}\n\nIntermediate concepts: {', '.join(intermediate_concepts)}\n\nCreate TWO complex validation problems with these properties:\n\n1. Problem P1: Should require knowledge of \"{intermediate_concepts[0]}\" to solve correctly\n2. Problem P2: Should require knowledge of \"{intermediate_concepts[1]}\" to solve correctly  \n3. Both problems should be complex and non-trivial\n4. Provide the expected answer for each problem\n5. The problems should be distinct and test different aspects\n\nFormat as:\nPROBLEM_P1: [complex problem 1]\nANSWER_P1: [expected answer 1]\nPROBLEM_P2: [complex problem 2]  \nANSWER_P2: [expected answer 2]\"\"\"\n\n    prompt = build_llama3_prompt(system_msg, user_msg)\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=300,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            return_dict_in_generate=True\n        )\n    \n    response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    llm_response = response.split(\"assistant\")[-1].strip() if \"assistant\" in response else response\n    \n    # è§£æç”Ÿæˆçš„å¤æ‚é—®é¢˜\n    problems = parse_complex_problems(llm_response, intermediate_concepts)\n    \n    return problems\n\ndef parse_complex_problems(response_text, intermediate_concepts):\n    \"\"\"è§£æLLMç”Ÿæˆçš„å¤æ‚é—®é¢˜\"\"\"\n    \n    problems = {\n        'P1': {\n            'problem': f\"Complex problem requiring {intermediate_concepts[0]}\",\n            'answer': \"Expected answer for P1\",\n            'required_concept': intermediate_concepts[0]\n        },\n        'P2': {\n            'problem': f\"Complex problem requiring {intermediate_concepts[1]}\", \n            'answer': \"Expected answer for P2\",\n            'required_concept': intermediate_concepts[1]\n        }\n    }\n    \n    # å°è¯•ä»LLMå“åº”ä¸­æå–é—®é¢˜\n    lines = response_text.split('\\n')\n    current_problem = None\n    \n    for line in lines:\n        line = line.strip()\n        if 'PROBLEM_P1:' in line:\n            problems['P1']['problem'] = line.split('PROBLEM_P1:')[-1].strip()\n            current_problem = 'P1'\n        elif 'ANSWER_P1:' in line:\n            problems['P1']['answer'] = line.split('ANSWER_P1:')[-1].strip()\n        elif 'PROBLEM_P2:' in line:\n            problems['P2']['problem'] = line.split('PROBLEM_P2:')[-1].strip()\n            current_problem = 'P2'\n        elif 'ANSWER_P2:' in line:\n            problems['P2']['answer'] = line.split('ANSWER_P2:')[-1].strip()\n    \n    print(\"ç”Ÿæˆçš„å¤æ‚éªŒè¯é—®é¢˜:\")\n    for key, problem_data in problems.items():\n        print(f\"  {key}: {problem_data['problem'][:80]}...\")\n        print(f\"    ç­”æ¡ˆ: {problem_data['answer']}\")\n        print(f\"    éœ€è¦æ¦‚å¿µ: {problem_data['required_concept']}\")\n    \n    return problems\n\nprint(\"âœ… å¤æ‚éªŒè¯é—®é¢˜ç”Ÿæˆå‡½æ•°å®Œæˆ\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:26:17.279324Z","iopub.execute_input":"2025-11-04T03:26:17.279614Z","iopub.status.idle":"2025-11-04T03:26:17.289241Z","shell.execute_reply.started":"2025-11-04T03:26:17.279594Z","shell.execute_reply":"2025-11-04T03:26:17.288515Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### å¯è¡Œæ€§éªŒè¯å®éªŒ","metadata":{}},{"cell_type":"code","source":"def feasibility_validation(scorer, base_xp, intermediate_concepts, complex_problems, hs_A):\n    \"\"\"\n    è¿›è¡Œå¯è¡Œæ€§éªŒè¯ï¼šç¡®ä¿å®éªŒè®¾ç½®æœ‰æ•ˆ\n    \"\"\"\n    print(\"ğŸ” è¿›è¡Œå¯è¡Œæ€§éªŒè¯...\")\n    \n    feasibility_results = {}\n    \n    # æµ‹è¯•1: ç›´æ¥æä¾›ä¸­é—´æ¦‚å¿µçš„åŸºçº¿æ€§èƒ½\n    concept_A1 = intermediate_concepts[0]\n    problem_P1 = complex_problems['P1']['problem']\n    answer_P1 = complex_problems['P1']['answer']\n    \n    baseline_prompt = f\"{base_xp}\\n\\nWe know: {concept_A1}\\n\\nProblem: {problem_P1}\"\n    baseline_score = scorer.score_reasoning_chain(baseline_prompt, answer_P1)\n    feasibility_results['baseline_with_concept'] = baseline_score\n    print(f\"åŸºçº¿æ€§èƒ½ (æä¾›æ¦‚å¿µ{concept_A1}): {baseline_score:.4f}\")\n    \n    # æµ‹è¯•2: ä½¿ç”¨hidden stateçš„æ€§èƒ½\n    try:\n        # æ„å»ºåªæœ‰é—®é¢˜çš„promptæ¥è·å–hidden stateä½ç½®\n        problem_only_prompt = f\"{base_xp}\\n\\nProblem: {problem_P1}\"\n        inputs = scorer.tokenizer(problem_only_prompt, return_tensors=\"pt\").to(scorer.device)\n        \n        with torch.no_grad():\n            outputs = scorer.model(**inputs, output_hidden_states=True)\n            problem_hidden = outputs.hidden_states[-1][0, -1, :]  # é—®é¢˜ç¼–ç çš„hidden state\n        \n        # ç»„åˆhidden states (ç®€åŒ–å¤„ç†)\n        combined_hidden = (hs_A + problem_hidden) / 2\n        \n        hidden_state_score, _ = scorer.calculate_performance_score(combined_hidden, answer_P1)\n        feasibility_results['hidden_state_performance'] = hidden_state_score\n        print(f\"Hidden stateæ€§èƒ½: {hidden_state_score:.4f}\")\n        \n    except Exception as e:\n        print(f\"Hidden stateæµ‹è¯•é”™è¯¯: {e}\")\n        feasibility_results['hidden_state_performance'] = 0.0\n    \n    # åˆ¤æ–­å¯è¡Œæ€§\n    feasibility_results['is_feasible'] = (\n        feasibility_results['baseline_with_concept'] > 0.7 and \n        feasibility_results['hidden_state_performance'] > 0.5\n    )\n    \n    print(f\"å®éªŒå¯è¡Œæ€§: {feasibility_results['is_feasible']}\")\n    \n    return feasibility_results\n\nprint(\"âœ… å¯è¡Œæ€§éªŒè¯å‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:26:21.632308Z","iopub.execute_input":"2025-11-04T03:26:21.632990Z","iopub.status.idle":"2025-11-04T03:26:21.640108Z","shell.execute_reply.started":"2025-11-04T03:26:21.632960Z","shell.execute_reply":"2025-11-04T03:26:21.639462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### è·¯å¾„æ¨æ–­äº¤å‰æ£€éªŒ","metadata":{}},{"cell_type":"code","source":"def cross_validation_experiment(scorer, base_xp, complex_problems, hs_A, intermediate_concepts):\n    \"\"\"\n    æ‰§è¡Œè·¯å¾„æ¨æ–­çš„äº¤å‰æ£€éªŒ\n    \"\"\"\n    print(\"ğŸ”„ æ‰§è¡Œè·¯å¾„æ¨æ–­äº¤å‰æ£€éªŒ...\")\n    \n    validation_results = {}\n    \n    # æ£€éªŒç»„: æµ‹è¯•hs_Aåœ¨ä¸åŒé—®é¢˜ä¸Šçš„æ€§èƒ½\n    test_scores = {}\n    \n    for problem_key, problem_data in complex_problems.items():\n        problem_text = problem_data['problem']\n        target_answer = problem_data['answer']\n        required_concept = problem_data['required_concept']\n        \n        # æ„å»ºæµ‹è¯•prompt\n        test_prompt = f\"{base_xp}\\n\\nProblem: {problem_text}\"\n        \n        # è·å–é—®é¢˜ç¼–ç çš„hidden state\n        inputs = scorer.tokenizer(test_prompt, return_tensors=\"pt\").to(scorer.device)\n        with torch.no_grad():\n            outputs = scorer.model(**inputs, output_hidden_states=True)\n            problem_hidden = outputs.hidden_states[-1][0, -1, :]\n        \n        # ç»„åˆhidden stateså¹¶è®¡ç®—æ€§èƒ½\n        combined_hidden = (hs_A.to(scorer.device) + problem_hidden) / 2\n        performance_score, predictions = scorer.calculate_performance_score(combined_hidden, target_answer)\n        \n        test_scores[problem_key] = {\n            'performance_score': performance_score,\n            'required_concept': required_concept,\n            'predictions': predictions\n        }\n        \n        print(f\"é—®é¢˜ {problem_key} (éœ€è¦{required_concept}) æ€§èƒ½: {performance_score:.4f}\")\n    \n    validation_results['test_scores'] = test_scores\n    \n    # å¯¹ç…§ç»„: ç¡®ç«‹æ€§èƒ½ä¸Šé™\n    print(\"\\nç¡®ç«‹æ€§èƒ½ä¸Šé™...\")\n    control_scores = {}\n    \n    for concept in intermediate_concepts:\n        # ç”ŸæˆåŒ…å«æ˜ç¡®æ¦‚å¿µçš„prompt\n        explicit_prompt = f\"{base_xp}\\n\\nWe know: {concept}\\n\\nProblem: {complex_problems['P1']['problem']}\"\n        control_score = scorer.score_reasoning_chain(explicit_prompt, complex_problems['P1']['answer'])\n        control_scores[concept] = control_score\n        print(f\"æ˜ç¡®æä¾›æ¦‚å¿µ {concept} çš„æ€§èƒ½: {control_score:.4f}\")\n    \n    validation_results['control_scores'] = control_scores\n    \n    return validation_results\n\nprint(\"âœ… äº¤å‰æ£€éªŒå‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:26:23.977595Z","iopub.execute_input":"2025-11-04T03:26:23.978242Z","iopub.status.idle":"2025-11-04T03:26:23.987174Z","shell.execute_reply.started":"2025-11-04T03:26:23.978220Z","shell.execute_reply":"2025-11-04T03:26:23.986327Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ç»“æœåˆ†æå’Œç»“è®º","metadata":{}},{"cell_type":"code","source":"def analyze_cross_validation_results(validation_results, intermediate_concepts):\n    \"\"\"\n    åˆ†æäº¤å‰æ£€éªŒç»“æœå¹¶å¾—å‡ºç»“è®º\n    \"\"\"\n    print(\"\\nğŸ“Š åˆ†æäº¤å‰æ£€éªŒç»“æœ...\")\n    \n    test_scores = validation_results['test_scores']\n    control_scores = validation_results['control_scores']\n    \n    # æå–æ€§èƒ½åˆ†æ•°\n    perf_E = test_scores['P1']['performance_score']  # éœ€è¦æ¦‚å¿µA1çš„é—®é¢˜\n    perf_F = test_scores['P2']['performance_score']  # éœ€è¦æ¦‚å¿µA2çš„é—®é¢˜\n    \n    print(f\"æ€§èƒ½åˆ†æ•°å¯¹æ¯”:\")\n    print(f\"  é—®é¢˜P1 (éœ€è¦{intermediate_concepts[0]}): {perf_E:.4f}\")\n    print(f\"  é—®é¢˜P2 (éœ€è¦{intermediate_concepts[1]}): {perf_F:.4f}\")\n    \n    # æ€§èƒ½ä¸Šé™å‚è€ƒ\n    control_A1 = control_scores.get(intermediate_concepts[0], 0)\n    control_A2 = control_scores.get(intermediate_concepts[1], 0)\n    print(f\"æ€§èƒ½ä¸Šé™å‚è€ƒ:\")\n    print(f\"  æ˜ç¡®æä¾›{intermediate_concepts[0]}: {control_A1:.4f}\")\n    print(f\"  æ˜ç¡®æä¾›{intermediate_concepts[1]}: {control_A2:.4f}\")\n    \n    # å¾—å‡ºç»“è®º\n    print(\"\\nğŸ¯ å®éªŒç»“è®º:\")\n    \n    threshold = 0.1  # æ˜¾è‘—æ€§é˜ˆå€¼\n    \n    if abs(perf_E - perf_F) < threshold:\n        print(\"ğŸ’¡ è¯æ®æ”¯æŒ'çº¯è·³æ­¥'å‡è®¾\")\n        print(\"   - hs_Aä¸­çš„ä¿¡æ¯æ˜¯æ··æ²Œæˆ–æ— æ•ˆçš„\")\n        print(\"   - åœ¨ä¸¤ä¸ªé—®é¢˜ä¸Šçš„æ€§èƒ½æ— æ˜¾è‘—å·®å¼‚\")\n        \n    elif perf_E > perf_F + threshold:\n        print(\"ğŸ’¡ å¼ºè¯æ®è¡¨æ˜hs_Aä¸­ç¼–ç äº†æ¦‚å¿µA1çš„ä¿¡æ¯\")\n        print(f\"   - åœ¨éœ€è¦{intermediate_concepts[0]}çš„é—®é¢˜ä¸Šè¡¨ç°æ›´å¥½\")\n        print(f\"   - æ€§èƒ½å·®å¼‚: {perf_E - perf_F:.4f}\")\n        \n    elif perf_F > perf_E + threshold:\n        print(\"ğŸ’¡ å¼ºè¯æ®è¡¨æ˜hs_Aä¸­ç¼–ç äº†æ¦‚å¿µA2çš„ä¿¡æ¯\") \n        print(f\"   - åœ¨éœ€è¦{intermediate_concepts[1]}çš„é—®é¢˜ä¸Šè¡¨ç°æ›´å¥½\")\n        print(f\"   - æ€§èƒ½å·®å¼‚: {perf_F - perf_E:.4f}\")\n        \n    else:\n        print(\"âš ï¸ ç»“æœä¸æ˜ç¡®ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ\")\n    \n    return {\n        'performance_difference': perf_E - perf_F,\n        'absolute_difference': abs(perf_E - perf_F),\n        'conclusion': 'pure_shortcut' if abs(perf_E - perf_F) < threshold else \n                     'encodes_A1' if perf_E > perf_F else 'encodes_A2'\n    }\n\nprint(\"âœ… ç»“æœåˆ†æå‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:26:28.978467Z","iopub.execute_input":"2025-11-04T03:26:28.979161Z","iopub.status.idle":"2025-11-04T03:26:28.986828Z","shell.execute_reply.started":"2025-11-04T03:26:28.979133Z","shell.execute_reply":"2025-11-04T03:26:28.985971Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ–¹æ³•ä¸‰ä¸»å®éªŒ","metadata":{}},{"cell_type":"code","source":"def method3_main_experiment(model, tokenizer, base_xp, hs_A, intermediate_concepts):\n    \"\"\"\n    æ–¹æ³•ä¸‰ä¸»å®éªŒæµç¨‹\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"æ–¹æ³•ä¸‰ï¼šåŸºäºè¡Œä¸ºè¡¨ç°çš„è·¯å¾„æ¨æ–­\")\n    print(\"=\" * 70)\n    \n    # åˆå§‹åŒ–æ€§èƒ½è®¡ç®—å™¨\n    scorer = PerformanceScorer(model, tokenizer)\n    \n    # é˜¶æ®µ1: ç”Ÿæˆå¤æ‚éªŒè¯é—®é¢˜\n    print(\"\\nğŸ“ ç”Ÿæˆå¤æ‚éªŒè¯é—®é¢˜...\")\n    complex_problems = generate_complex_validation_problems(\n        model, tokenizer, base_xp, intermediate_concepts\n    )\n    \n    # é˜¶æ®µ2: å¯è¡Œæ€§éªŒè¯\n    print(\"\\nğŸ” è¿›è¡Œå¯è¡Œæ€§éªŒè¯...\")\n    feasibility_results = feasibility_validation(\n        scorer, base_xp, intermediate_concepts, complex_problems, hs_A\n    )\n    \n    if not feasibility_results['is_feasible']:\n        print(\"âŒ å®éªŒè®¾ç½®ä¸å¯è¡Œï¼Œç»ˆæ­¢å®éªŒ\")\n        return None\n    \n    # é˜¶æ®µ3: è·¯å¾„æ¨æ–­äº¤å‰æ£€éªŒ\n    print(\"\\nğŸ”„ æ‰§è¡Œäº¤å‰æ£€éªŒ...\")\n    validation_results = cross_validation_experiment(\n        scorer, base_xp, complex_problems, hs_A, intermediate_concepts\n    )\n    \n    # é˜¶æ®µ4: ç»“æœåˆ†æ\n    print(\"\\nğŸ“Š åˆ†æç»“æœ...\")\n    analysis_results = analyze_cross_validation_results(validation_results, intermediate_concepts)\n    \n    # ä¿å­˜å®Œæ•´ç»“æœ\n    final_results = {\n        'base_xp': base_xp,\n        'intermediate_concepts': intermediate_concepts,\n        'complex_problems': complex_problems,\n        'feasibility_results': feasibility_results,\n        'validation_results': validation_results,\n        'analysis_results': analysis_results,\n        'hs_A_shape': hs_A.shape if hasattr(hs_A, 'shape') else str(type(hs_A))\n    }\n    \n    save_results(final_results, \"method3_performance_analysis.pkl\")\n    \n    print(f\"\\nâœ… æ–¹æ³•ä¸‰å®éªŒå®Œæˆï¼\")\n    \n    return final_results\n\nprint(\"âœ… æ–¹æ³•ä¸‰ä¸»å®éªŒå‡½æ•°å®Œæˆ\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:26:33.015290Z","iopub.execute_input":"2025-11-04T03:26:33.015786Z","iopub.status.idle":"2025-11-04T03:26:33.022580Z","shell.execute_reply.started":"2025-11-04T03:26:33.015763Z","shell.execute_reply":"2025-11-04T03:26:33.021730Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ‰§è¡Œæ–¹æ³•ä¸‰å®éªŒ","metadata":{}},{"cell_type":"code","source":"from tuned_lens import TunedLens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:30:30.777999Z","iopub.execute_input":"2025-11-04T03:30:30.778838Z","iopub.status.idle":"2025-11-04T03:30:30.791553Z","shell.execute_reply.started":"2025-11-04T03:30:30.778808Z","shell.execute_reply":"2025-11-04T03:30:30.790838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    with open(\"complete_skipping_analysis.pkl\", \"rb\") as f:\n        method1_results = pickle.load(f)\n    \n    hs_A = method1_results['hs_A']  # ä»æ–¹æ³•ä¸€è·å–çš„hidden state\n    XP = \"If you enter a building from the east side, walking west, and then take two rights and a left down corridors, which direction are you facing?\"\n    \n    # ä¸­é—´æ¦‚å¿µ\n    INTERMEDIATE_CONCEPTS = [\"north\", \"east\"]  # ç¬¬ä¸€ä¸ªå³è½¬å’Œç¬¬äºŒä¸ªå³è½¬åçš„æ–¹å‘\n    \n    print(\"ğŸš€ å¼€å§‹æ‰§è¡Œæ–¹æ³•ä¸‰å®éªŒ...\")\n    method3_results = method3_main_experiment(\n        model, tokenizer, XP, hs_A, INTERMEDIATE_CONCEPTS\n    )\n    \nexcept FileNotFoundError:\n    print(\"âŒ æ‰¾ä¸åˆ°æ–¹æ³•ä¸€çš„ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ–¹æ³•ä¸€å®éªŒ\")\nexcept Exception as e:\n    print(f\"âŒ æ‰§è¡Œæ–¹æ³•ä¸‰å®éªŒæ—¶å‡ºé”™: {e}\")\n\nprint(\"\\nğŸ‰ æ–¹æ³•ä¸‰å®éªŒæµç¨‹å®Œæˆï¼\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T03:32:40.764959Z","iopub.execute_input":"2025-11-04T03:32:40.765733Z","iopub.status.idle":"2025-11-04T03:32:40.771931Z","shell.execute_reply.started":"2025-11-04T03:32:40.765705Z","shell.execute_reply":"2025-11-04T03:32:40.771261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æš‚æ—¶åºŸå¼ƒ","metadata":{}},{"cell_type":"markdown","source":"### æ–¹æ³•ä¸‰\nä½†æ˜¯æµ‹è¯•ç»“æœ0.000ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¿®æ”¹","metadata":{}},{"cell_type":"code","source":"import torch\nimport pickle\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# --------------------------\n# 1. ç”Ÿæˆåˆ†æ­¥æ¨ç†ï¼ˆè®©æ¨¡å‹æ˜¾å¼è¾“å‡ºstep1, step2...ï¼‰\n# --------------------------\n\ndef generate_step_by_step_reasoning():\n    \"\"\"è®©æ¨¡å‹è¿›è¡Œåˆ†æ­¥æ¨ç†ï¼Œè®°å½•æ¯ä¸ªæ­¥éª¤çš„hidden states\"\"\"\n    \n    step_by_step_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful AI assistant that solves direction reasoning problems step by step. \nYou MUST output your reasoning in the exact format: Step 1: [reasoning], Step 2: [reasoning], etc.\nEnd with \"Final Answer: [direction]\"<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nProblem: {XP}\n\nPlease solve this step by step and output each step clearly:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n    \n    print(\"ç”Ÿæˆåˆ†æ­¥æ¨ç†è¿‡ç¨‹...\")\n    inputs = tokenizer(step_by_step_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            num_return_sequences=1,\n            do_sample=True,\n            temperature=0.7,\n            output_hidden_states=True,\n            return_dict_in_generate=True\n        )\n    \n    # è·å–å®Œæ•´å“åº”\n    full_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    assistant_response = full_response.split(\"assistant\")[-1].strip() if \"assistant\" in full_response else full_response\n    \n    print(f\"æ¨¡å‹åˆ†æ­¥æ¨ç†å“åº”:\\n{assistant_response}\")\n    \n    # æå–æ¯ä¸ªç”Ÿæˆæ­¥éª¤çš„hidden stateså’Œå¯¹åº”çš„æ–‡æœ¬\n    step_data = extract_step_level_hidden_states(outputs, inputs.input_ids.shape[1])\n    \n    return assistant_response, step_data\n\ndef extract_step_level_hidden_states(outputs, prompt_length):\n    \"\"\"æå–æ¯ä¸ªç”Ÿæˆæ­¥éª¤çš„hidden stateså’Œå¯¹åº”æ–‡æœ¬\"\"\"\n    \n    # outputs.hidden_states çš„ç»“æ„: tuple of (layer1_hidden_states, layer2_hidden_states, ...)\n    # æ¯ä¸ªlayer_hidden_states: tuple of (step1, step2, ...) æ¯ä¸ªstep: [batch, seq_len, hidden_dim]\n    \n    generated_ids = outputs.sequences[0, prompt_length:]  # åªå–ç”Ÿæˆçš„éƒ¨åˆ†\n    \n    step_data = []\n    current_step_text = \"\"\n    step_counter = 1\n    step_boundaries = []\n    \n    # é¦–å…ˆè¯†åˆ«æ­¥éª¤è¾¹ç•Œ\n    full_generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    \n    # æŸ¥æ‰¾æ‰€æœ‰æ­¥éª¤å¼€å§‹çš„ä½ç½®\n    step_starts = []\n    for i in range(1, 10):  # å‡è®¾æœ€å¤š10ä¸ªæ­¥éª¤\n        step_marker = f\"Step {i}:\"\n        if step_marker in full_generated_text:\n            start_idx = full_generated_text.find(step_marker)\n            step_starts.append((i, start_idx))\n    \n    # æ·»åŠ Final Answerä½œä¸ºæœ€åä¸€ä¸ªè¾¹ç•Œ\n    if \"Final Answer:\" in full_generated_text:\n        final_idx = full_generated_text.find(\"Final Answer:\")\n        step_starts.append((\"final\", final_idx))\n    \n    # æŒ‰ä½ç½®æ’åº\n    step_starts.sort(key=lambda x: x[1])\n    \n    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ­¥éª¤ç»“æ„ï¼Œæ•´ä¸ªä½œä¸ºä¸€ä¸ªæ­¥éª¤\n    if not step_starts:\n        # è·å–æœ€åä¸€ä¸ªtokençš„hidden state\n        last_layer_idx = -1\n        last_step_idx = len(outputs.hidden_states[last_layer_idx]) - 1\n        last_hidden = outputs.hidden_states[last_layer_idx][last_step_idx][0, -1, :].cpu().numpy()\n        \n        step_data.append({\n            'step_number': 1,\n            'step_text': full_generated_text,\n            'hidden_state': last_hidden,\n            'token_index': len(generated_ids) - 1\n        })\n        return step_data\n    \n    # ä¸ºæ¯ä¸ªæ­¥éª¤æå–hidden states\n    for i, (step_num, start_idx) in enumerate(step_starts):\n        # æ‰¾åˆ°æ­¥éª¤ç»“æŸçš„ä½ç½®\n        if i + 1 < len(step_starts):\n            end_idx = step_starts[i + 1][1]\n            step_text = full_generated_text[start_idx:end_idx].strip()\n        else:\n            step_text = full_generated_text[start_idx:].strip()\n        \n        # æ‰¾åˆ°è¿™ä¸ªæ­¥éª¤å¯¹åº”çš„tokenä½ç½®\n        step_tokens = tokenizer.encode(step_text, add_special_tokens=False)\n        \n        # ä¼°ç®—è¿™ä¸ªæ­¥éª¤åœ¨ç”Ÿæˆåºåˆ—ä¸­çš„å¤§è‡´ä½ç½®\n        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–æ–¹æ³•ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´ç²¾ç¡®çš„åŒ¹é…\n        approx_token_pos = min(prompt_length + start_idx // 4, len(outputs.sequences[0]) - 1)  # ç²—ç•¥ä¼°ç®—\n        \n        # è·å–è¿™ä¸ªä½ç½®çš„hidden stateï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªç”Ÿæˆæ­¥éª¤ï¼‰\n        last_layer_idx = -1\n        step_hidden_states = outputs.hidden_states[last_layer_idx]\n        \n        # ä½¿ç”¨æ­¥éª¤ç»“æŸä½ç½®çš„hidden state\n        step_end_pos = min(approx_token_pos + len(step_tokens), len(step_hidden_states) - 1)\n        step_hidden = step_hidden_states[step_end_pos][0, -1, :].cpu().numpy()\n        \n        step_data.append({\n            'step_number': step_num if step_num != \"final\" else \"final\",\n            'step_text': step_text,\n            'hidden_state': step_hidden,\n            'token_index': step_end_pos\n        })\n    \n    return step_data\n\n# --------------------------\n# 2. ç”Ÿæˆå‚è€ƒè·¯å¾„ï¼ˆå®Œæ•´æ¨ç† vs çº¯è·³æ­¥ï¼‰\n# --------------------------\n\ndef generate_reference_paths():\n    \"\"\"ç”Ÿæˆå®Œæ•´æ¨ç†è·¯å¾„å’Œçº¯è·³æ­¥è·¯å¾„çš„hidden states\"\"\"\n    \n    reference_paths = {\n        \"full_reasoning\": [\n            \"Step 1: Initial direction is west (entering from east walking west). \",\n            \"Step 2: First right turn: west â†’ north. \",\n            \"Step 3: Second right turn: north â†’ east. \",\n            \"Step 4: Left turn: east â†’ north. \",\n            \"Step 5: Final Answer: north\"\n        ],\n        \"pure_shortcut\": [\n            \"Step 1: Final Answer: north\"\n        ]\n    }\n    \n    system_message = \"You are a helpful AI assistant that solves direction reasoning problems step by step.\"\n    \n    reference_hidden_states = {}\n    \n    for path_name, steps in reference_paths.items():\n        print(f\"\\nç”Ÿæˆå‚è€ƒè·¯å¾„: {path_name}\")\n        \n        # ä¸ºæ¯ä¸ªæ­¥éª¤ç”Ÿæˆhidden states\n        step_hidden_states = []\n        \n        for step_idx, step_text in enumerate(steps):\n            # æ„å»ºåˆ°å½“å‰æ­¥éª¤çš„å®Œæ•´ä¸Šä¸‹æ–‡\n            context_steps = \"\".join(steps[:step_idx + 1])\n            user_message = f\"{XP}\\n\\nPlease continue this reasoning:\\n{context_steps}\"\n            \n            full_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_message}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_message}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            \n            inputs = tokenizer(full_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n            \n            with torch.no_grad():\n                outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=5,  # åªéœ€è¦å°‘é‡ç”Ÿæˆæ¥è·å–hidden state\n                    num_return_sequences=1,\n                    do_sample=False,\n                    temperature=0.0,\n                    output_hidden_states=True,\n                    return_dict_in_generate=True\n                )\n            \n            # è·å–æœ€åä¸€ä¸ªtokençš„hidden state\n            last_layer_idx = -1\n            last_step_idx = len(outputs.hidden_states[last_layer_idx]) - 1\n            last_hidden = outputs.hidden_states[last_layer_idx][last_step_idx][0, -1, :].cpu().numpy()\n            \n            step_hidden_states.append({\n                'step_number': step_idx + 1,\n                'step_text': step_text,\n                'hidden_state': last_hidden\n            })\n            \n            print(f\"  æ­¥éª¤ {step_idx + 1}: {step_text[:50]}...\")\n        \n        reference_hidden_states[path_name] = step_hidden_states\n    \n    return reference_hidden_states, reference_paths\n\n# --------------------------\n# 3. LLM as Judge åˆ¤æ–­æ˜¯å¦æœ‰è·³æ­¥\n# --------------------------\n\ndef detect_skipping_steps(step_by_step_response, reference_paths):\n    \"\"\"ä½¿ç”¨LLMåˆ¤æ–­æ¨ç†è¿‡ç¨‹ä¸­æ˜¯å¦æœ‰è·³æ­¥\"\"\"\n    \n    judge_prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are an expert judge analyzing reasoning processes. Your task is to detect if there are skipped reasoning steps.\nCompare the given reasoning with the expected complete reasoning path.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nProblem: {XP}\n\nExpected Complete Reasoning:\n{chr(10).join(reference_paths['full_reasoning'])}\n\nActual Reasoning:\n{step_by_step_response}\n\nAnalysis Questions:\n1. Does the actual reasoning skip any logical steps compared to the complete reasoning?\n2. If steps are skipped, which specific steps are missing?\n3. Rate the reasoning completeness on a scale of 1-5 (1=major skipping, 5=complete reasoning)\n\nPlease provide your analysis:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n    \n    inputs = tokenizer(judge_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            num_return_sequences=1,\n            do_sample=False,\n            temperature=0.0,\n            return_dict_in_generate=True\n        )\n    \n    # æ­£ç¡®è§£ç è¾“å‡º\n    judge_analysis = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    judge_response = judge_analysis.split(\"assistant\")[-1].strip() if \"assistant\" in judge_analysis else judge_analysis\n    \n    print(f\"LLM Judgeåˆ†æ:\\n{judge_response}\")\n    \n    return judge_response\n\n# --------------------------\n# 4. æ­¥éª¤çº§åˆ«ç›¸ä¼¼åº¦åˆ†æ\n# --------------------------\n\ndef analyze_step_similarities(step_data, reference_hidden_states):\n    \"\"\"åˆ†ææ¯ä¸ªæ­¥éª¤ä¸å‚è€ƒè·¯å¾„çš„ç›¸ä¼¼åº¦\"\"\"\n    \n    analysis_results = {}\n    \n    for step_info in step_data:\n        step_num = step_info['step_number']\n        step_hs = step_info['hidden_state']\n        step_text = step_info['step_text']\n        \n        print(f\"\\nåˆ†ææ­¥éª¤ {step_num}: {step_text[:100]}...\")\n        \n        step_similarities = {}\n        \n        # ä¸å®Œæ•´æ¨ç†è·¯å¾„çš„å¯¹åº”æ­¥éª¤æ¯”è¾ƒ\n        if 'full_reasoning' in reference_hidden_states:\n            full_steps = reference_hidden_states['full_reasoning']\n            \n            # æ ¹æ®æ­¥éª¤ç±»å‹é€‰æ‹©æ¯”è¾ƒå¯¹è±¡\n            if \"Final Answer\" in step_text or step_num == \"final\":\n                # æœ€ç»ˆç­”æ¡ˆä¸å®Œæ•´æ¨ç†çš„æœ€ç»ˆæ­¥éª¤æ¯”è¾ƒ\n                compare_step = full_steps[-1]  # æœ€åä¸€ä¸ªæ­¥éª¤\n            elif \"Step\" in str(step_num):\n                # æ™®é€šæ¨ç†æ­¥éª¤\n                step_idx = int(step_num) - 1 if isinstance(step_num, int) else 0\n                compare_step_idx = min(step_idx, len(full_steps) - 1)\n                compare_step = full_steps[compare_step_idx]\n            else:\n                # é»˜è®¤ä¸ç¬¬ä¸€ä¸ªæ­¥éª¤æ¯”è¾ƒ\n                compare_step = full_steps[0]\n            \n            full_step_hs = compare_step['hidden_state']\n            \n            similarity = cosine_similarity(\n                step_hs.reshape(1, -1), \n                full_step_hs.reshape(1, -1)\n            )[0][0]\n            \n            step_similarities['full_reasoning'] = {\n                'similarity': similarity,\n                'compared_with_step': compare_step['step_number'],\n                'reference_text': compare_step['step_text'][:100] + \"...\"\n            }\n            print(f\"  ä¸å®Œæ•´æ¨ç†æ­¥éª¤ {compare_step['step_number']} ç›¸ä¼¼åº¦: {similarity:.4f}\")\n        \n        # ä¸çº¯è·³æ­¥è·¯å¾„æ¯”è¾ƒ\n        if 'pure_shortcut' in reference_hidden_states:\n            shortcut_steps = reference_hidden_states['pure_shortcut']\n            shortcut_step_hs = shortcut_steps[0]['hidden_state']  # çº¯è·³æ­¥åªæœ‰ä¸€ä¸ªæ­¥éª¤\n            \n            similarity = cosine_similarity(\n                step_hs.reshape(1, -1), \n                shortcut_step_hs.reshape(1, -1)\n            )[0][0]\n            \n            step_similarities['pure_shortcut'] = {\n                'similarity': similarity,\n                'reference_text': shortcut_steps[0]['step_text'][:100] + \"...\"\n            }\n            print(f\"  ä¸çº¯è·³æ­¥ç›¸ä¼¼åº¦: {similarity:.4f}\")\n        \n        analysis_results[step_num] = {\n            'step_text': step_text,\n            'similarities': step_similarities,\n            'hidden_state': step_hs\n        }\n    \n    return analysis_results\n\n# --------------------------\n# å·¥å…·å‡½æ•°\n# --------------------------\n\ndef save_hidden_states(data_dict, filename=\"hidden_states_data.pkl\"):\n    \"\"\"ä¿å­˜æ•°æ®\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n    print(f\"âœ“ æ•°æ®å·²ä¿å­˜è‡³ {filename}\")\n\n# --------------------------\n# ä¸»æ‰§è¡Œæµç¨‹\n# --------------------------\n\ndef main_experiment():\n    \"\"\"ä¸»å®éªŒæµç¨‹\"\"\"\n    \n    print(\"=\" * 60)\n    print(\"æ­¥éª¤çº§åˆ«éšå¼æ¨ç†åˆ†æå®éªŒ\")\n    print(\"=\" * 60)\n    \n    # 1. ç”Ÿæˆåˆ†æ­¥æ¨ç†\n    step_by_step_response, step_data = generate_step_by_step_reasoning()\n    \n    # 2. ç”Ÿæˆå‚è€ƒè·¯å¾„\n    reference_hidden_states, reference_paths = generate_reference_paths()\n    \n    # 3. LLMåˆ¤æ–­æ˜¯å¦æœ‰è·³æ­¥\n    judge_analysis = detect_skipping_steps(step_by_step_response, reference_paths)\n    \n    # 4. æ­¥éª¤çº§åˆ«ç›¸ä¼¼åº¦åˆ†æ\n    similarity_analysis = analyze_step_similarities(step_data, reference_hidden_states)\n    \n    # 5. ä¿å­˜å®Œæ•´ç»“æœ\n    complete_results = {\n        'XP': XP,\n        'step_by_step_response': step_by_step_response,\n        'step_data': step_data,\n        'reference_paths': reference_paths,\n        'reference_hidden_states': reference_hidden_states,\n        'judge_analysis': judge_analysis,\n        'similarity_analysis': similarity_analysis\n    }\n    \n    save_hidden_states(complete_results, \"step_level_reasoning_analysis.pkl\")\n    \n    print(f\"\\nâœ… å®éªŒå®Œæˆï¼\")\n    print(f\"âœ… ç»“æœå·²ä¿å­˜è‡³ step_level_reasoning_analysis.pkl\")\n    \n    # æ‰“å°æ‘˜è¦\n    print(\"\\n\" + \"=\" * 60)\n    print(\"å®éªŒæ‘˜è¦\")\n    print(\"=\" * 60)\n    print(f\"æ¨ç†æ­¥éª¤æ•°é‡: {len(step_data)}\")\n    for step_num, analysis in similarity_analysis.items():\n        full_sim = analysis['similarities'].get('full_reasoning', {}).get('similarity', 0)\n        shortcut_sim = analysis['similarities'].get('pure_shortcut', {}).get('similarity', 0)\n        print(f\"æ­¥éª¤ {step_num}: å®Œæ•´æ¨ç†ç›¸ä¼¼åº¦={full_sim:.4f}, çº¯è·³æ­¥ç›¸ä¼¼åº¦={shortcut_sim:.4f}\")\n    \n    return complete_results\n\n# æ‰§è¡Œå®éªŒ\nif __name__ == \"__main__\":\n    XP = \"If you enter a building from the east side, walking west, and then take two rights and a left down corridors, which direction are you facing?\"\n    \n    results = main_experiment()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T09:55:14.861165Z","iopub.execute_input":"2025-11-03T09:55:14.861921Z","iopub.status.idle":"2025-11-03T09:55:38.341988Z","shell.execute_reply.started":"2025-11-03T09:55:14.861893Z","shell.execute_reply":"2025-11-03T09:55:38.341268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  prompt test","metadata":{}},{"cell_type":"code","source":"prompt2 = f\"\"\"### Task: Generate Standard Reasoning Paths (Non-splittable & Explicit CoT)\n### Precondition (Xp): {XP}\n### Reasoning Path Requirements:\nGenerate 3 types of reasoning paths. For paths with intermediate steps (Type 2 and 3), use EXPLICIT CoT (Chain-of-Thought) and ensure each step is a minimal logical closed loop (cannot be split).\n\n#### Type 1: Pure Skip Path (No Intermediate Steps)\n- Format: \"Initial Condition â†’ Conclusion\"\n- Requirement: Directly connect the problem context to the final fraction (no CoT).\n\n#### Type 2: Complete Path 1 (Aâ†’Câ†’Dâ†’Conclusion: 3 steps)\n- Format: \nStep 1: Initial Condition â†’ Intermediate C (Calculate Grace's fraction)\nStep 2: Intermediate C â†’ Intermediate D (Calculate remaining work after Grace)\nStep 3: Intermediate D â†’ Conclusion (Calculate John's fraction after all delegations)\n- Requirement: Each step is non-splittable (e.g., \"Calculate Grace's fraction\" = 1/3 of total work).\n\n#### Type 3: Complete Path 2 (Aâ†’Eâ†’Fâ†’Conclusion: 3 steps)\n- Format: \nStep 1: Initial Condition â†’ Intermediate E (Define total work as 1 unit)\nStep 2: Intermediate E â†’ Intermediate F (Calculate Andrew's and Beatrice's fractions)\nStep 3: Intermediate F â†’ Conclusion (Sum all delegated fractions and subtract from 1)\n- Requirement: Each step is non-splittable and logic differs from Type 2 (e.g., starts with total=1).\n\n### Your Output (Clearly label each path type):\n\"\"\"\n\n# ç”Ÿæˆæ¨¡å‹è¾“å‡º\ninputs = tokenizer(prompt2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=500,  # è¶³å¤Ÿå®¹çº³å¤šæ¡è·¯å¾„çš„æ˜¾å¼CoT\n        temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œç¡®ä¿è·¯å¾„è§„èŒƒæ€§\n        do_sample=True\n    )\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"### Your Output:\")[-1].strip()\nprint(\"Generated Result from Prompt 2:\")\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T02:21:14.202693Z","iopub.execute_input":"2025-11-03T02:21:14.203120Z","iopub.status.idle":"2025-11-03T02:21:39.537550Z","shell.execute_reply.started":"2025-11-03T02:21:14.203080Z","shell.execute_reply":"2025-11-03T02:21:39.536669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------\n# Prompt 2: ç”Ÿæˆæ ‡å‡†è·¯å¾„ï¼ˆå¤šæ¬¡ï¼‰å¹¶æå–hidden states\n# --------------------------\n# 1. Prompté…ç½®ï¼ˆæ˜ç¡®Type 1-3åŠæ˜¾å¼CoTï¼‰\nprompt2 = f\"\"\"### Task: Generate Standard Reasoning Paths (Non-splittable & Explicit CoT)\n### Precondition (Xp): {XP}\n### Reasoning Path Requirements:\nGenerate 3 types of reasoning paths. For paths with intermediate steps (Type 2 and 3), use EXPLICIT CoT and ensure each step is a minimal logical closed loop.\n\n#### Type 1: Pure Skip Path (No Intermediate Steps)\n- Format: \"Initial Condition â†’ Conclusion\"\n- Requirement: Directly connect the problem context to the final fraction (no CoT).\n\n#### Type 2: Complete Path 1 (Aâ†’Câ†’Dâ†’Conclusion: 3 steps)\n- Format: \nStep 1: Initial Condition â†’ Intermediate C (Calculate Grace's fraction)\nStep 2: Intermediate C â†’ Intermediate D (Calculate remaining work after Grace)\nStep 3: Intermediate D â†’ Conclusion (Calculate John's fraction after all delegations)\n\n#### Type 3: Complete Path 2 (Aâ†’Eâ†’Fâ†’Conclusion: 3 steps)\n- Format: \nStep 1: Initial Condition â†’ Intermediate E (Define total work as 1 unit)\nStep 2: Intermediate E â†’ Intermediate F (Calculate Andrew's and Beatrice's fractions)\nStep 3: Intermediate F â†’ Conclusion (Sum all delegated fractions and subtract from 1)\n\n### Your Output (Clearly label each path type):\n\"\"\"\n\n# 2. é…ç½®ç”Ÿæˆå‚æ•°\nnum_generations = 3  # æ¯ç§è·¯å¾„ç”Ÿæˆ3æ¬¡\ntarget_token_pos = -2  # ç›®æ ‡tokenä½ç½®ï¼ˆç»“è®ºä¸­æœ€åä¸€ä¸ªæœ‰æ•ˆtokenï¼‰\nextract_layer = -1     # æå–çš„éšè—å±‚\nhidden_states = {      # å­˜å‚¨æ‰€æœ‰éšè—çŠ¶æ€çš„å­—å…¸\n    \"Type 1\": [],  # hs_A1, hs_A1', hs_A1''\n    \"Type 2\": [],  # hs_A2, hs_A2', hs_A2''\n    \"Type 3\": []   # hs_A3, hs_A3', hs_A3''\n}\ngenerated_paths = {     # å­˜å‚¨æ‰€æœ‰ç”Ÿæˆçš„è·¯å¾„æ–‡æœ¬\n    \"Type 1\": [],\n    \"Type 2\": [],\n    \"Type 3\": []\n}\n\n# 3. å¤šæ¬¡ç”Ÿæˆå¹¶æå–\nfor gen_idx in range(num_generations):\n    print(f\"\\n=== Generation {gen_idx + 1}/{num_generations} ===\")\n    \n    # ç”Ÿæˆè·¯å¾„\n    inputs = tokenizer(prompt2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=500,\n            temperature=0.3,\n            do_sample=True,\n            output_hidden_states=True,\n            return_dict_in_generate=True\n        )\n    \n    # è§£ç ç”Ÿæˆç»“æœï¼ˆä½¿ç”¨ä½ çš„æ–¹å¼ï¼‰\n    full_text = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    path_text = full_text.split(\"### Your Output:\")[-1].strip()\n    print(\"Generated Paths:\")\n    print(path_text)\n    \n    # æå–æ¯ç§Typeçš„æ–‡æœ¬å’Œéšè—çŠ¶æ€\n    for path_type in [\"Type 1\", \"Type 2\", \"Type 3\"]:\n        # å®šä½è¯¥ç±»å‹åœ¨ç”Ÿæˆæ–‡æœ¬ä¸­çš„ä½ç½®ï¼ˆåŸºäºæ ‡ç­¾åˆ†å‰²ï¼‰\n        start_idx = path_text.find(f\"{path_type}:\")\n        if start_idx == -1:\n            print(f\"âš ï¸ {path_type} not found in generation {gen_idx + 1}\")\n            continue\n        \n        # æˆªå–è¯¥ç±»å‹çš„æ–‡æœ¬ï¼ˆåˆ°ä¸‹ä¸€ä¸ªTypeæˆ–ç»“å°¾ï¼‰\n        end_idx = len(path_text)\n        for next_type in [\"Type 1\", \"Type 2\", \"Type 3\"]:\n            if next_type != path_type and path_text.find(next_type, start_idx) != -1:\n                end_idx = path_text.find(next_type, start_idx)\n                break\n        type_text = path_text[start_idx:end_idx].strip()\n        generated_paths[path_type].append(type_text)\n        \n        # æå–è¯¥ç±»å‹å¯¹åº”çš„éšè—çŠ¶æ€\n        # é‡æ–°ç¼–ç è¯¥ç±»å‹æ–‡æœ¬ä»¥è·å–å‡†ç¡®çš„hidden statesï¼ˆé¿å…æ•´ä½“åºåˆ—å¹²æ‰°ï¼‰\n        type_inputs = tokenizer(type_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        with torch.no_grad():\n            type_outputs = model(** type_inputs, output_hidden_states=True)\n        type_hidden = type_outputs.hidden_states[extract_layer]  # [1, seq_len, hidden_dim]\n        type_hs = type_hidden[0, target_token_pos, :].cpu().numpy()\n        hidden_states[path_type].append(type_hs)\n        \n        print(f\"âœ… Extracted {path_type} hidden state (generation {gen_idx + 1})\")\n\n# 4. é‡å‘½åä¸ºè¦æ±‚çš„æ ¼å¼ï¼ˆhs_A1, hs_A1', hs_A1'' ç­‰ï¼‰\nhs_A1, hs_A1_prime, hs_A1_doubleprime = hidden_states[\"Type 1\"] if len(hidden_states[\"Type 1\"]) == 3 else [None, None, None]\nhs_A2, hs_A2_prime, hs_A2_doubleprime = hidden_states[\"Type 2\"] if len(hidden_states[\"Type 2\"]) == 3 else [None, None, None]\nhs_A3, hs_A3_prime, hs_A3_doubleprime = hidden_states[\"Type 3\"] if len(hidden_states[\"Type 3\"]) == 3 else [None, None, None]\n\nprint(\"\\n=== Hidden States Summary ===\")\nprint(f\"Type 1: hs_A1 (shape: {hs_A1.shape if hs_A1 is not None else 'N/A'}), hs_A1' (same), hs_A1'' (same)\")\nprint(f\"Type 2: hs_A2 (shape: {hs_A2.shape if hs_A2 is not None else 'N/A'}), hs_A2' (same), hs_A2'' (same)\")\nprint(f\"Type 3: hs_A3 (shape: {hs_A3.shape if hs_A3 is not None else 'N/A'}), hs_A3' (same), hs_A3'' (same)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T02:56:36.966873Z","iopub.execute_input":"2025-11-03T02:56:36.967394Z","iopub.status.idle":"2025-11-03T02:57:40.956837Z","shell.execute_reply.started":"2025-11-03T02:56:36.967372Z","shell.execute_reply":"2025-11-03T02:57:40.956081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_hidden_states = {\n    \"hs_A\": hs_A,\n    \"Type 1\": [hs_A1, hs_A1_prime, hs_A1_doubleprime],\n    \"Type 2\": [hs_A2, hs_A2_prime, hs_A2_doubleprime],\n    \"Type 3\": [hs_A3, hs_A3_prime, hs_A3_doubleprime]\n}\n\n# --------------------------\n# å®šä¹‰ç›¸ä¼¼åº¦è®¡ç®—è¾…åŠ©å‡½æ•°\n# --------------------------\ndef compute_similarity_with_mean(target_hs, reference_hs_list):\n    \"\"\"\n    è®¡ç®—ç›®æ ‡éšè—çŠ¶æ€ä¸å‚è€ƒåˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶è¿”å›å•ä¸ªå€¼å’Œå¹³å‡å€¼\n    Args:\n        target_hs: ç›®æ ‡éšè—çŠ¶æ€ï¼ˆå¦‚hs_Aï¼‰\n        reference_hs_list: å‚è€ƒéšè—çŠ¶æ€åˆ—è¡¨ï¼ˆå¦‚[hs_A1, hs_A1', hs_A1'']ï¼‰\n    Returns:\n        ç›¸ä¼¼åº¦å­—å…¸ï¼ˆå«å•ä¸ªå€¼å’Œå¹³å‡å€¼ï¼‰\n    \"\"\"\n    # è¿‡æ»¤æ— æ•ˆçš„éšè—çŠ¶æ€ï¼ˆå¦‚ç”Ÿæˆå¤±è´¥çš„Noneå€¼ï¼‰\n    valid_refs = [hs for hs in reference_hs_list if hs is not None]\n    if not valid_refs:\n        return {\"similarities\": [], \"mean_similarity\": None}\n    \n    # è®¡ç®—æ¯ä¸ªå‚è€ƒçŠ¶æ€çš„ç›¸ä¼¼åº¦\n    similarities = []\n    target_norm = target_hs / np.linalg.norm(target_hs) if np.linalg.norm(target_hs) != 0 else target_hs\n    for ref_hs in valid_refs:\n        ref_norm = ref_hs / np.linalg.norm(ref_hs) if np.linalg.norm(ref_hs) != 0 else ref_hs\n        sim = cosine_similarity(target_norm.reshape(1, -1), ref_norm.reshape(1, -1))[0][0]\n        similarities.append(round(float(sim), 4))\n    \n    # è®¡ç®—å¹³å‡å€¼\n    mean_sim = round(np.mean(similarities), 4) if similarities else None\n    return {\n        \"similarities\": similarities,\n        \"mean_similarity\": mean_sim\n    }\n\n# --------------------------\n# è®¡ç®—hs_Aä¸æ¯ç§è·¯å¾„ç±»å‹çš„ç›¸ä¼¼åº¦\n# --------------------------\n# 1. ä¸Type 1ï¼ˆçº¯è·³æ­¥è·¯å¾„ï¼‰çš„ç›¸ä¼¼åº¦\ntype1_results = compute_similarity_with_mean(\n    target_hs=hs_A,\n    reference_hs_list=all_hidden_states[\"Type 1\"]\n)\n\n# 2. ä¸Type 2ï¼ˆå®Œæ•´è·¯å¾„1ï¼‰çš„ç›¸ä¼¼åº¦\ntype2_results = compute_similarity_with_mean(\n    target_hs=hs_A,\n    reference_hs_list=all_hidden_states[\"Type 2\"]\n)\n\n# 3. ä¸Type 3ï¼ˆå®Œæ•´è·¯å¾„2ï¼‰çš„ç›¸ä¼¼åº¦\ntype3_results = compute_similarity_with_mean(\n    target_hs=hs_A,\n    reference_hs_list=all_hidden_states[\"Type 3\"]\n)\n\n# --------------------------\n# è¾“å‡ºè¯¦ç»†ç»“æœ\n# --------------------------\nprint(\"=\"*80)\nprint(\"Cosine Similarity Results (hs_A vs Each Path Type)\")\nprint(\"=\"*80)\n\n# Type 1 ç»“æœ\nprint(f\"Type 1 (Pure Skip Path):\")\nprint(f\"  Similarities with hs_A1, hs_A1', hs_A1'': {type1_results['similarities']}\")\nprint(f\"  Mean Similarity: {type1_results['mean_similarity']}\\n\")\n\n# Type 2 ç»“æœ\nprint(f\"Type 2 (Complete Path 1):\")\nprint(f\"  Similarities with hs_A2, hs_A2', hs_A2'': {type2_results['similarities']}\")\nprint(f\"  Mean Similarity: {type2_results['mean_similarity']}\\n\")\n\n# Type 3 ç»“æœ\nprint(f\"Type 3 (Complete Path 2):\")\nprint(f\"  Similarities with hs_A3, hs_A3', hs_A3'': {type3_results['similarities']}\")\nprint(f\"  Mean Similarity: {type3_results['mean_similarity']}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T03:01:27.689311Z","iopub.execute_input":"2025-11-03T03:01:27.690017Z","iopub.status.idle":"2025-11-03T03:01:27.709707Z","shell.execute_reply.started":"2025-11-03T03:01:27.689992Z","shell.execute_reply":"2025-11-03T03:01:27.708818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# ä¿å­˜æ‰€æœ‰éšè—çŠ¶æ€ï¼ˆäºŒè¿›åˆ¶æ ¼å¼ï¼Œä¿ç•™åŸå§‹æ•°ç»„ï¼‰\nwith open(\"all_hidden_states.pkl\", \"wb\") as f:\n    pickle.dump(all_hidden_states, f)\n\n# ä¿å­˜ç›¸ä¼¼åº¦ç»“æœï¼ˆæ–‡æœ¬æ ¼å¼ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰\nsimilarity_results = {\n    \"Type 1\": type1_results,\n    \"Type 2\": type2_results,\n    \"Type 3\": type3_results\n}\nwith open(\"similarity_results.json\", \"w\") as f:\n    json.dump(similarity_results, f, indent=4)\n\nprint(\"\\nâœ… All hidden states saved to 'all_hidden_states.pkl'\")\nprint(\"âœ… Similarity results saved to 'similarity_results.json'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T03:03:45.599730Z","iopub.execute_input":"2025-11-03T03:03:45.600260Z","iopub.status.idle":"2025-11-03T03:03:45.606723Z","shell.execute_reply.started":"2025-11-03T03:03:45.600236Z","shell.execute_reply":"2025-11-03T03:03:45.606036Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### hidden statesèƒ½åŠ›ç®€åŒ–æµ‹è¯•","metadata":{}},{"cell_type":"code","source":"# --------------------------\n# åŠ è½½å®éªŒæ•°æ®ï¼ˆå¤ç”¨Prompt2çš„hs_A3å’ŒA3ï¼‰\n# --------------------------\n# ä»æ–¹æ³•ä¸€ä¿å­˜çš„éšè—çŠ¶æ€æ–‡ä»¶ä¸­åŠ è½½hs_A3ï¼ˆType3çš„éšè—çŠ¶æ€ï¼‰\nwith open(\"all_hidden_states.pkl\", \"rb\") as f:\n    all_hs = pickle.load(f)\nhs_A3 = all_hs[\"Type 3\"][0]  # å–Type3ç¬¬ä¸€æ¬¡ç”Ÿæˆçš„hidden stateï¼ˆå¯æ›¿æ¢ä¸ºå…¶ä»–æ¬¡ï¼‰\nprint(f\"âœ… Loaded hs_A3 (shape: {hs_A3.shape})\")  # shape: [hidden_dim,] ä¸æ˜¯å±‚æ•°ï¼\n\n# å®šä¹‰å·²çŸ¥çš„ä¸­é—´æ¡ä»¶A3å’Œæµ‹è¯•é—®é¢˜Qï¼ˆè¡¥å……å®Œæ•´ä¸Šä¸‹æ–‡ï¼Œé¿å…æ¨¡å‹æ­§ä¹‰ï¼‰\nA3 = \"the sum of delegated fractions is 13/12\"  # ä½ çš„ä¸­é—´æ¡ä»¶A3\n# è¡¥å……åŸé¢˜èƒŒæ™¯ï¼Œè®©é—®é¢˜æ›´æ˜ç¡®ï¼ˆæ¨¡å‹éœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡æ‰èƒ½æ¨ç†ï¼‰\nQ = \"John has some work he needs to be done by the end of the week. He delegates fractions to others and completes the rest. If the sum of delegated fractions is 13/12, what fraction of the entire work does John do? (Note: Total work is 1, so John's fraction = 1 - sum of delegated fractions)\"\ncorrect_conclusion = \"John does 1/12 of the work\"  # é¢„æœŸæ­£ç¡®ç»“è®º\n\nprint(\"\\nğŸ“‹ Test Configuration:\")\nprint(f\"A3 (Intermediate Condition): {A3}\")\nprint(f\"Q (Test Question): {Q}\")\nprint(f\"Expected Correct Conclusion: {correct_conclusion}\")\n\n# --------------------------\n# æ„å»ºä¸¤ç§æµ‹è¯•è¾“å…¥ï¼ˆä¼˜åŒ–æç¤ºè¯ï¼Œå¼•å¯¼ç²¾å‡†è¾“å‡ºï¼‰\n# --------------------------\n# è¾“å…¥1ï¼šA3 + Qï¼ˆç›´æ¥æä¾›ä¸­é—´æ¡ä»¶ï¼Œä½œä¸ºåŸºå‡†ï¼‰\ninput_a3_q = f\"Given the information: {A3}. Based on this, answer the question: {Q} Output only the final conclusion in the format 'John does X of the work' (no extra text).\"\n# è¾“å…¥2ï¼šhs_A3åµŒå…¥ + Qï¼ˆä½¿ç”¨æ›´è‡ªç„¶çš„è¡¨è¿°ï¼Œä¼ é€’éšè—çŠ¶æ€ä¿¡æ¯ï¼‰\n# æ³¨ï¼šhs_A3æ˜¯[hidden_dim,]çš„å‘é‡ï¼Œå–å‰15ä¸ªæ•°å€¼æ˜¯ä¸ºäº†æ–‡æœ¬åµŒå…¥ç¤ºæ„ï¼Œéå±‚æ•°\nhs_embedding_text = f\"[Task-related semantic embedding: {np.round(hs_A3[:15], 4)}...]\"  # ä¿ç•™4ä½å°æ•°ï¼Œæ›´ç®€æ´\ninput_hs_q = f\"Use the following task-related semantic embedding: {hs_embedding_text}. Based on this embedding, answer the question: {Q} Output only the final conclusion in the format 'John does X of the work' (no extra text).\"\n\nprint(f\"\\nğŸ” Test Inputs:\")\nprint(f\"Input 1 (A3 + Q): {input_a3_q}\")\nprint(f\"Input 2 (hs_A3 + Q): {input_hs_q}\")\n\n# --------------------------\n# æ¨ç†å‡½æ•°å®šä¹‰ï¼ˆä¼˜åŒ–ç”Ÿæˆå‚æ•°ï¼Œé¿å…é‡å¤è¾“å‡ºï¼‰\n# --------------------------\ndef get_model_conclusion(input_text):\n    \"\"\"ç”Ÿæˆæ¨¡å‹å¯¹è¾“å…¥çš„ç»“è®ºè¾“å‡º\"\"\"\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=300\n    ).to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=50,\n            temperature=0.1,  # è¿›ä¸€æ­¥é™ä½éšæœºæ€§\n            do_sample=False,  # ç¦ç”¨é‡‡æ ·ï¼Œä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆ\n            pad_token_id=tokenizer.eos_token_id,\n            repetition_penalty=1.2  # æŠ‘åˆ¶é‡å¤è¾“å‡º\n        )\n    \n    # æå–æ¨¡å‹ç”Ÿæˆçš„ç»“è®ºï¼ˆæ’é™¤è¾“å…¥æ–‡æœ¬éƒ¨åˆ†ï¼‰\n    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    conclusion = full_output.replace(input_text, \"\").strip()\n    return conclusion\n\n# --------------------------\n# æ‰§è¡Œæµ‹è¯•å¹¶è¯„ä¼°\n# --------------------------\nprint(\"\\nğŸš€ Generating model conclusions...\")\nconclusion_a3_q = get_model_conclusion(input_a3_q)\nconclusion_hs_q = get_model_conclusion(input_hs_q)\n\n# ç»“è®ºæ­£ç¡®æ€§è¯„ä¼°ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼Œæ›´çµæ´»ï¼‰\ndef is_correct(conclusion, correct_answer):\n    correct_keywords = correct_answer.lower().split()\n    return all(keyword in conclusion.lower() for keyword in correct_keywords)\n\ncorrect_a3 = is_correct(conclusion_a3_q, correct_conclusion)\ncorrect_hs = is_correct(conclusion_hs_q, correct_conclusion)\n\n# --------------------------\n# è¾“å‡ºæµ‹è¯•æŠ¥å‘Š\n# --------------------------\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ“Š Hidden States Understanding Test Report\")\nprint(\"=\"*80)\nprint(f\"Input 1 (A3 + Q) â†’ Conclusion: {conclusion_a3_q}\")\nprint(f\"Input 1 Correct? â†’ {'âœ… YES' if correct_a3 else 'âŒ NO'}\")\nprint(f\"\\nInput 2 (hs_A3 + Q) â†’ Conclusion: {conclusion_hs_q}\")\nprint(f\"Input 2 Correct? â†’ {'âœ… YES' if correct_hs else 'âŒ NO'}\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-03T03:27:33.717204Z","iopub.execute_input":"2025-11-03T03:27:33.717984Z","iopub.status.idle":"2025-11-03T03:27:40.728487Z","shell.execute_reply.started":"2025-11-03T03:27:33.717951Z","shell.execute_reply":"2025-11-03T03:27:40.727812Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}