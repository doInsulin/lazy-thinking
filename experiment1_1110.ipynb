{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13608691,"sourceType":"datasetVersion","datasetId":8647923},{"sourceId":627241,"sourceType":"modelInstanceVersion","modelInstanceId":472263,"modelId":488160}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:59:13.347232Z","iopub.execute_input":"2025-11-02T12:59:13.347498Z","iopub.status.idle":"2025-11-02T12:59:13.353178Z","shell.execute_reply.started":"2025-11-02T12:59:13.347473Z","shell.execute_reply":"2025-11-02T12:59:13.352170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pickle\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport json\nimport warnings\nimport csv\nimport os\nwarnings.filterwarnings(\"ignore\")\n\n# è®¾å¤‡é…ç½®ï¼ˆè‡ªåŠ¨é€‚é…GPU/CPUï¼‰\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:10:28.135215Z","iopub.execute_input":"2025-11-10T12:10:28.135482Z","iopub.status.idle":"2025-11-10T12:10:40.583519Z","shell.execute_reply.started":"2025-11-10T12:10:28.135456Z","shell.execute_reply":"2025-11-10T12:10:40.582739Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# æ¨¡å‹è·¯å¾„ï¼ˆä¿æŒä½ åŸæ¥çš„ä¸å˜ï¼‰\nmodel_path = \"/kaggle/input/llama3-8b-instruct/transformers/default/1/Meta-Llama-3-8B-Instruct\"\n\n# ===================== åªåŠ è½½ä¸€æ¬¡Tokenizerå’ŒModelï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šåˆ é™¤é‡å¤åŠ è½½ï¼‰=====================\n# åŠ è½½Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# è®¾ç½®pad_tokenå’Œpadding_sideï¼ˆLlama 3é»˜è®¤æ²¡æœ‰pad_tokenï¼Œç”¨eos_tokenæ›¿ä»£ï¼‰\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    torch_dtype=torch.float16, \n    device_map=\"auto\", \n    low_cpu_mem_usage=True,  \n    trust_remote_code=True \n)\n\n# æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼\nmodel.eval()\n\nprint(\"âœ… Model and Tokenizer loaded successfully!\")\nprint(f\"ğŸ“Š é…ç½®ï¼šfloat16ç²¾åº¦ + è‡ªåŠ¨è®¾å¤‡åˆ†é… + hidden_stateså¯ç”¨\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:10:40.585118Z","iopub.execute_input":"2025-11-10T12:10:40.585966Z","iopub.status.idle":"2025-11-10T12:12:22.751853Z","shell.execute_reply.started":"2025-11-10T12:10:40.585933Z","shell.execute_reply":"2025-11-10T12:12:22.750971Z"}},"outputs":[{"name":"stderr","text":"2025-11-10 12:10:48.368474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762776648.555083      39 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762776648.611232      39 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"452a109c330b4a8a986d99ef6158c09e"}},"metadata":{}},{"name":"stdout","text":"âœ… Model and Tokenizer loaded successfully!\nğŸ“Š é…ç½®ï¼šfloat16ç²¾åº¦ + è‡ªåŠ¨è®¾å¤‡åˆ†é… + hidden_stateså¯ç”¨\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\n# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\ncsv_path = \"/kaggle/input/testset/testset.csv\"\nif not os.path.exists(csv_path):\n    print(f\"æ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„ï¼š{csv_path}\")\n    print(\"å½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨ï¼š\", os.listdir(\"/kaggle/input/testset\"))  # æ‰“å°æ–‡ä»¶å¤¹å†…çš„æ–‡ä»¶\nelse:\n    print(\"æ–‡ä»¶å­˜åœ¨ï¼Œå¯ä»¥æ­£å¸¸åŠ è½½\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T02:30:14.322486Z","iopub.execute_input":"2025-11-10T02:30:14.323436Z","iopub.status.idle":"2025-11-10T02:30:14.332677Z","shell.execute_reply.started":"2025-11-10T02:30:14.323410Z","shell.execute_reply":"2025-11-10T02:30:14.331937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### testset","metadata":{}},{"cell_type":"code","source":"# å…¨å±€å˜é‡ä¿å­˜å®éªŒä¸€æ•°æ®\nexperiment1_data = {}\n\n\ndef save_experiment1_data(detailed_result):\n    \"\"\"ä¿å­˜å®éªŒä¸€çš„hidden statesæ•°æ®\"\"\"\n    test_id = detailed_result['test_id']\n    \n    # æ„å»ºä¿å­˜çš„æ•°æ®ç»“æ„\n    saved_data = {\n        'test_id': test_id,\n        'problem': detailed_result.get('problem', ''),\n        'step_by_step_response': detailed_result['step_by_step_response'],\n        'step_data': [],\n        'skipping_info': detailed_result['skipping_info'],\n        'all_skip_results': detailed_result['all_skip_results'],\n        'hidden_states_info': {\n            'total_steps': len(detailed_result.get('step_data', [])),\n            'has_skipped_steps': len(detailed_result['skipping_info']['skipped_steps']) > 0,\n            'saved_timestamp': str(np.datetime64('now')),\n            'target_layer': -1  # æ–°å¢ï¼šè®°å½•æå–éšè—çŠ¶æ€çš„å±‚ï¼ˆ-1è¡¨ç¤ºæœ€åä¸€å±‚ï¼‰\n        }\n    }\n    \n    # ä¿å­˜æ­¥éª¤æ•°æ®ï¼ˆåŒ…æ‹¬hidden stateså’Œlayer_idxï¼‰\n    for step in detailed_result.get('step_data', []):\n        step_data_copy = step.copy()\n        # ç¡®ä¿hidden stateæ˜¯numpyæ•°ç»„\n        if 'hidden_state' in step_data_copy:\n            step_data_copy['hidden_state'] = step_data_copy['hidden_state']\n        # æ–°å¢ï¼šä¿å­˜å±‚ç´¢å¼•ï¼ˆä¸å®éªŒä¸‰Tuned LensåŒ¹é…ï¼‰\n        step_data_copy['layer_idx'] = step.get('layer_idx', -1)\n        saved_data['step_data'].append(step_data_copy)\n    \n    # ä¿å­˜åˆ°å…¨å±€å˜é‡\n    experiment1_data[test_id] = saved_data\n    \n    # ä¿å­˜åˆ°æ–‡ä»¶\n    filename = f\"experiment1_hidden_states_{test_id}.pkl\"\n    with open(filename, 'wb') as f:\n        pickle.dump(saved_data, f)\n    \n    # æ‰“å°ä¿å­˜ä¿¡æ¯\n    print(f\"âœ“ å·²ä¿å­˜ {test_id} çš„hidden statesæ•°æ®åˆ° {filename}\")\n    print(f\"  - æ­¥éª¤æ•°é‡: {len(saved_data['step_data'])}\")\n    print(f\"  - è·³æ­¥æ•°é‡: {len(saved_data['skipping_info']['skipped_steps'])}\")\n    if saved_data['step_data']:\n        print(f\"  - hidden stateå½¢çŠ¶: {saved_data['step_data'][0]['hidden_state'].shape}\")\n        print(f\"  - æå–å±‚ç´¢å¼•: {saved_data['step_data'][0]['layer_idx']}\")  # æ–°å¢ï¼šæ‰“å°å±‚ç´¢å¼•\n\ndef save_complete_experiment1_data():\n    \"\"\"ä¿å­˜å®Œæ•´çš„å®éªŒä¸€æ•°æ®é›†\"\"\"\n    if experiment1_data:\n        with open(\"experiment1_complete_data.pkl\", 'wb') as f:\n            pickle.dump(experiment1_data, f)\n        print(f\"\\nğŸ‰ å·²ä¿å­˜å®Œæ•´çš„å®éªŒä¸€æ•°æ®ï¼Œå…± {len(experiment1_data)} ä¸ªæµ‹è¯•æ¡ˆä¾‹\")\n        return True\n    else:\n        print(\"âŒ æ²¡æœ‰å®éªŒæ•°æ®å¯ä¿å­˜\")\n        return False\n\ndef summarize_experiment1_data():\n    \"\"\"æ±‡æ€»å®éªŒä¸€æ•°æ®\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"å®éªŒä¸€æ•°æ®æ±‡æ€»\")\n    print(\"=\"*60)\n    \n    if not experiment1_data:\n        print(\"æš‚æ— å®éªŒæ•°æ®\")\n        return\n    \n    for test_id, data in experiment1_data.items():\n        print(f\"\\n{test_id}:\")\n        print(f\"  é—®é¢˜: {data['problem'][:100]}...\")\n        print(f\"  æ¨ç†æ­¥éª¤: {len(data['step_data'])}\")\n        print(f\"  è·³æ­¥: {len(data['skipping_info']['skipped_steps'])}\")\n        print(f\"  è·³æ­¥è¯¦æƒ…: {data['skipping_info']['skipped_steps']}\")\n        if data['step_data']:\n            print(f\"  æå–å±‚ç´¢å¼•: {data['step_data'][0]['layer_idx']}\")  # æ–°å¢ï¼šæ˜¾ç¤ºå±‚ç´¢å¼•\n\ndef calculate_similarity_between_paths(actual_skip_hidden, generated_paths_hidden_states):\n    \"\"\"\n    è®¡ç®—å®é™…è·³æ­¥hidden statesä¸ç”Ÿæˆè·¯å¾„hidden statesçš„ç›¸ä¼¼åº¦\n    \n    Args:\n        actual_skip_hidden: å®é™…è·³æ­¥æ­¥éª¤çš„hidden state (numpy array)\n        generated_paths_hidden_states: ç”Ÿæˆçš„å„è·¯å¾„hidden statesåˆ—è¡¨\n    \n    Returns:\n        similarities: å„è·¯å¾„çš„ç›¸ä¼¼åº¦åˆ—è¡¨\n    \"\"\"\n    similarities = []\n    actual_vector = actual_skip_hidden.reshape(1, -1)\n    \n    for path_hidden in generated_paths_hidden_states:\n        if path_hidden is not None:\n            path_vector = path_hidden.reshape(1, -1)\n            sim = cosine_similarity(actual_vector, path_vector)[0][0]\n            similarities.append(float(sim))\n        else:\n            similarities.append(0.0)\n    \n    return similarities\n\ndef get_actual_skip_hidden_state(skip_step_key, step_data, skipping_info, outputs, prompt_length):\n    \"\"\"\n    è·å–å®é™…è·³æ­¥æ­¥éª¤çš„hidden stateï¼ˆæ–°å¢layer_idxè¿”å›ï¼‰\n    \n    Args:\n        skip_step_key: è·³æ­¥çš„æ­¥éª¤key (å¦‚ 'step_2')\n        step_data: æ­¥éª¤æ•°æ®\n        skipping_info: è·³æ­¥ä¿¡æ¯\n        outputs: æ¨¡å‹è¾“å‡º\n        prompt_length: prompté•¿åº¦\n    \n    Returns:\n        hidden_state: è·³æ­¥æ­¥éª¤çš„hidden state\n        layer_idx: å¯¹åº”çš„å±‚ç´¢å¼•\n    \"\"\"\n    # æ–¹æ³•1: å°è¯•ä»step_dataä¸­ç›´æ¥æ‰¾åˆ°è·³æ­¥æ­¥éª¤\n    for step in step_data:\n        if step['step_number'] == skip_step_key:\n            return step['hidden_state'], step.get('layer_idx', -1)  # æ–°å¢ï¼šè¿”å›layer_idx\n    \n    # æ–¹æ³•2: å¦‚æœæ‰¾ä¸åˆ°ï¼Œæ ¹æ®è·³æ­¥ä½ç½®æ¨æ–­\n    if step_data:\n        return step_data[-1]['hidden_state'], step_data[-1].get('layer_idx', -1)  # æ–°å¢ï¼šè¿”å›layer_idx\n    \n    # æ–¹æ³•3: è¿”å›æ•´ä¸ªæ¨ç†çš„æœ€åä¸€ä¸ªhidden state\n    generated_ids = outputs.sequences[0, prompt_length:]\n    layer_idx = -1  # é»˜è®¤æœ€åä¸€å±‚\n    if len(generated_ids) > 0:\n        actual_hidden_idx = prompt_length + len(generated_ids) - 1\n        if actual_hidden_idx < len(outputs.hidden_states[layer_idx]):\n            hidden_state = outputs.hidden_states[layer_idx][actual_hidden_idx][0, -1, :].cpu().numpy()\n            return hidden_state, layer_idx\n    \n    return None, -1\n\ndef save_hidden_states(data_dict, filename=\"hidden_states_data.pkl\"):\n    \"\"\"ä¿å­˜æ•°æ®\"\"\"\n    with open(filename, 'wb') as f:\n        pickle.dump(data_dict, f)\n\ndef build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n# =====================================================\n# è¯»å–CSVæµ‹è¯•é›†å‡½æ•°\n# =====================================================\ndef load_testset(csv_path=\"/kaggle/input/testset/testset.csv\"):\n    \"\"\"è¯»å–CSVæ ¼å¼çš„æµ‹è¯•é›†\"\"\"\n    testset = []\n    with open(csv_path, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            expected_steps = row['expected_complete_reasoning'].split('||')\n            testset.append({\n                'id': row['id'],\n                'problem': row['problem'],\n                'expected_complete_reasoning': expected_steps\n            })\n    print(f\"âœ“ æˆåŠŸåŠ è½½ {len(testset)} ä¸ªæµ‹è¯•é—®é¢˜\")\n    return testset\n\n# =====================================================\n# æ­¥éª¤çº§hidden statesæå–\n# =====================================================\ndef extract_step_level_hidden_states(outputs, prompt_length, generated_text):\n    steps = parse_step_structure(generated_text)\n    step_data = []\n    generated_ids = outputs.sequences[0, prompt_length:]\n    full_generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    total_hidden_length = len(generated_ids)\n\n    layer_idx = -1\n    \n    current_position = 0\n    for step_num, step_text in steps.items():\n        step_text_clean = step_text.strip()\n        step_start = full_generated_text.find(step_text_clean, current_position)\n        \n        if step_start == -1:\n            full_text_clean = \" \".join(full_generated_text.split())\n            step_text_clean_no_space = \" \".join(step_text_clean.split())\n            step_start = full_text_clean.find(step_text_clean_no_space, current_position)\n            if step_start == -1:\n                continue\n        \n        step_tokens = tokenizer.encode(step_text_clean, add_special_tokens=False)\n        step_token_length = len(step_tokens)\n        if step_token_length == 0:\n            continue\n        \n        text_before_step = full_generated_text[:step_start]\n        tokens_before_step = tokenizer.encode(text_before_step, add_special_tokens=False)\n        step_start_token_idx = len(tokens_before_step)\n        step_end_token_idx = step_start_token_idx + step_token_length - 1\n        \n        if step_end_token_idx < step_start_token_idx:\n            current_position = step_start + len(step_text_clean)\n            continue\n        \n        if step_end_token_idx >= total_hidden_length:\n            step_end_token_idx = total_hidden_length - 1\n        \n        actual_hidden_idx = prompt_length + step_end_token_idx\n        if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n            actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n        step_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n        \n        step_data.append({\n            'step_number': step_num,\n            'step_text': step_text,\n            'hidden_state': step_hidden,\n            'layer_idx': layer_idx, \n            'token_start': step_start_token_idx,\n            'token_end': step_end_token_idx,\n            'token_length': step_token_length,\n            'text_position': step_start\n        })\n        \n        current_position = step_start + len(step_text_clean)\n    \n    if not step_data and len(generated_ids) > 0:\n        actual_hidden_idx = prompt_length + len(generated_ids) - 1\n        if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n            actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n        last_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n        step_data.append({\n            'step_number': 'step_1',\n            'step_text': generated_text,\n            'hidden_state': last_hidden,\n            'layer_idx': layer_idx, \n            'token_start': 0,\n            'token_end': len(generated_ids) - 1,\n            'token_length': len(generated_ids),\n            'text_position': 0\n        })\n    \n    return step_data\n\ndef parse_step_structure(text):\n    steps = {}\n    step_patterns = [\n        r'(Step\\s*\\d+\\s*:)(.*?)(?=Step\\s*\\d+\\s*:|Final Answer:|$)',\n        r'(\\d+\\.\\s*)(.*?)(?=\\d+\\.\\s*|Final Answer:|$)',\n        r'(\\d+\\)\\s*)(.*?)(?=\\d+\\)\\s*|Final Answer:|$)',\n        r'(Final Answer:\\s*)(.*?)(?=$)'\n    ]\n    \n    all_matches = []\n    for pattern in step_patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n        for match in matches:\n            header, content = match[0], match[1]\n            start_pos = text.find(header)\n            if start_pos != -1:\n                all_matches.append((start_pos, header.strip(), content.strip()))\n    \n    all_matches.sort(key=lambda x: x[0])\n    \n    for i, (pos, header, content) in enumerate(all_matches):\n        if not content:\n            continue\n        if \"final\" in header.lower() or \"answer\" in header.lower():\n            step_key = \"final\"\n        else:\n            step_key = f\"step_{i+1}\"\n        step_text = f\"{header} {content}\".strip()\n        steps[step_key] = step_text\n    \n    if len(steps) <= 1:\n        steps = heuristic_step_splitting(text)\n    \n    return steps\n\ndef heuristic_step_splitting(text):\n    steps = {}\n    lines = text.split('\\n')\n    step_count = 1\n    \n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        if (re.match(r'^\\d+\\.', line) or \n            re.match(r'^Step\\s*\\d+', line, re.I) or\n            re.match(r'^\\d+\\)', line) or\n            len(line) > 20 and any(keyword in line.lower() for keyword in ['first', 'then', 'next', 'after', 'finally'])):\n            if 'final' not in line.lower() and 'answer' not in line.lower():\n                steps[f'step_{step_count}'] = line\n                step_count += 1\n            else:\n                steps['final'] = line\n    \n    if len(steps) <= 1:\n        sentences = re.split(r'[.!?]+', text)\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if len(sentence) > 15 and not sentence.startswith(('So', 'Therefore', 'Thus')):\n                if step_count <= 4:\n                    steps[f'step_{step_count}'] = sentence\n                    step_count += 1\n    \n    return steps\n\n# =====================================================\n# è·³æ­¥è¯†åˆ«å‡½æ•°\n# =====================================================\ndef identify_skipped_steps(step_by_step_response, problem, expected_complete_reasoning):\n    judge_prompt = build_llama3_prompt(\n        \"You are a strict but reasonable reasoning analyzer. Focus on comparing actual reasoning with expected core logic step by step. Missing any expected core logic = skipped step. Wording differences are acceptable if core logic is fully included.\",\n        f\"\"\"Problem: {problem}\n\nExpected Complete Reasoning (EACH STEP'S CORE LOGIC IS MANDATORY):\n{chr(10).join([f\"Expected Step {i+1}: {step.strip()}\" for i, step in enumerate(expected_complete_reasoning)])}\n\nActual Reasoning:\n{step_by_step_response}\n\nStrict Rules:\n1. For Step 1: Only judge if the core logic matches the expected Step 1 (wording differences are acceptable as long as the key meaning is the same).\n2. For Steps 2+: Compare actual reasoning with EVERY expected step's core logic (ignore wording/format differences ONLY if core logic is fully included).\n3. If ANY expected core logic is not found in actual reasoning â†’ mark as skipped step.\n4. For incomplete problems (actual reasoning asks for more info), mark SKIPPED_STEPS as \"incomplete_problem\", not \"none\".\n5. Do NOT use \"none\" if actual reasoning is incomplete or missing expected logic.\n\nMust output in this EXACT format (no extra words):\nSKIPPED_STEPS: [step numbers separated by commas / \"incomplete_problem\" / \"none\" only if 100% complete]\nSKIPPED_CONTENT: [missing core logic / \"incomplete_problem\" / \"none\"]\nSKIPPING_LOCATION: [between step X and step Y / \"incomplete_problem\" / \"none\"]\"\"\"\n    )\n    \n    inputs = tokenizer(judge_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,  # åŠ é•¿è¾“å‡ºï¼Œé€‚é…å¤šè·³æ­¥æè¿°\n            num_return_sequences=1,\n            do_sample=False,\n            return_dict_in_generate=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    judge_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    judge_analysis = judge_response.split(\"assistant\")[-1].strip() if \"assistant\" in judge_response else judge_response\n    skipping_info = parse_skipping_analysis(judge_analysis)\n    return skipping_info, judge_analysis\n\ndef parse_skipping_analysis(analysis_text):\n    skipping_info = {\n        'skipped_steps': [],\n        'skipped_content': '',\n        'skipping_location': '',\n        'from_step': None,\n        'to_step': None\n    }\n    \n    # æå–æ‰€æœ‰è·³æ­¥åºå·ï¼ˆæ ¸å¿ƒä¿®å¤ï¼šæ”¯æŒå¤šè·³æ­¥è¯†åˆ«ï¼‰\n    steps_match = re.search(r'SKIPPED_STEPS:\\s*([^\\n]+)', analysis_text)\n    if steps_match:\n        steps_str = steps_match.group(1).strip()\n        if steps_str != \"none\" and steps_str != \"incomplete_problem\":\n            step_numbers = re.findall(r'\\d+', steps_str)\n            skipping_info['skipped_steps'] = [f\"step_{num}\" for num in step_numbers]\n    \n    # æå–ç¼ºå¤±å†…å®¹ï¼ˆé€‚é…å¤šè·³æ­¥çš„ç¼ºå¤±æè¿°ï¼‰\n    content_match = re.search(r'SKIPPED_CONTENT:\\s*([^\\n]+)', analysis_text)\n    if content_match:\n        skipping_info['skipped_content'] = content_match.group(1).strip()\n    \n    # æå–è·³æ­¥ä½ç½®\n    location_match = re.search(r'SKIPPING_LOCATION:\\s*([^\\n]+)', analysis_text)\n    if location_match:\n        location_str = location_match.group(1).strip()\n        skipping_info['skipping_location'] = location_str\n        step_matches = re.findall(r'step\\s*(\\d+)', location_str.lower())\n        if len(step_matches) >= 2:\n            skipping_info['from_step'] = f\"step_{step_matches[0]}\"\n            skipping_info['to_step'] = f\"step_{step_matches[1]}\"\n    \n    return skipping_info\n\n# =====================================================\n# LLMå¯¹æ¯”è·¯å¾„ç”Ÿæˆï¼ˆä¿®å¤ç‰ˆï¼‰\n# =====================================================\ndef generate_comparison_paths_with_llm_fixed(skipping_info, step_data, problem, tokenizer, model, outputs, prompt_length):\n    \"\"\"\n    ä¿®å¤ç‰ˆçš„å¯¹æ¯”è·¯å¾„ç”Ÿæˆå‡½æ•°ï¼Œæ­£ç¡®è®¡ç®—ç›¸ä¼¼åº¦\n    \"\"\"\n    # è·å–å½“å‰è·³æ­¥ä¿¡æ¯\n    skipped_step_key = skipping_info['current_skip_step']\n    skipped_num = int(skipped_step_key.split('_')[1])\n    \n    # å…³é”®ä¿®å¤ï¼šæ¥æ”¶hidden_stateå’Œlayer_idxä¸¤ä¸ªè¿”å›å€¼\n    actual_skip_hidden, _ = get_actual_skip_hidden_state(\n        skipped_step_key, step_data, skipping_info, outputs, prompt_length\n    )\n    \n    if actual_skip_hidden is None:\n        return {\n            \"status\": \"error\", \n            \"error\": \"æ— æ³•è·å–å®é™…è·³æ­¥çš„hidden state\",\n            \"similarities\": {},\n            \"paths\": {}\n        }\n    \n    # ä¸Šä¸‹æ–‡æ„å»º\n    prev_step_key = f\"step_{skipped_num - 1}\" if skipped_num > 1 else \"problem\"\n    target_step_key = f\"step_{skipped_num + 1}\" if skipped_num + 1 <= len(step_data) else None\n\n    if prev_step_key == \"problem\":\n        prev_text = f\"Problem: {problem}\"\n    else:\n        prev_text = next((step['step_text'].strip() for step in step_data if step['step_number'] == prev_step_key), \"\")\n    \n    target_text = next((step['step_text'].strip() for step in step_data if step['step_number'] == target_step_key), \"Final reasoning step\")\n\n    context_text = f\"\"\"You need toè¡¥å…… the skipped reasoning step (Step {skipped_num}) between:\nPrevious context: {prev_text}\nNext step (target): {target_text}\nMandatory missing content to include: {skipping_info['skipped_content']}\nPlease output ONLY the reasoning step, no extra words, no greetings, no explanations.\"\"\"\n\n    style_prompts = {\n        \"detailed\": \"Provide a detailed reasoning step (include all missing content):\",\n        \"shortcut\": \"Give a direct reasoning step (only core missing content, no extra words):\",\n        \"alternative\": \"Provide an alternative concise reasoning step (include missing content):\"\n    }\n\n    results = {\n        \"status\": \"ok\", \n        \"actual_skip_hidden_shape\": actual_skip_hidden.shape,\n        \"similarities\": {}, \n        \"paths\": {},\n        \"generated_hidden_states\": {}\n    }\n\n    for style, prompt_suffix in style_prompts.items():\n        sims = []\n        path_texts = []\n        generated_hiddens = []\n        \n        for _ in range(3):  # æ¯ä¸ªé£æ ¼ç”Ÿæˆ3ä¸ªæ ·æœ¬\n            user_msg = f\"{context_text}\\n\\n{prompt_suffix}\"\n            full_prompt = build_llama3_prompt(\n                \"You are a precise reasoning assistant. Only output the required reasoning step, no extra content.\", \n                user_msg\n            )\n\n            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n            with torch.no_grad():\n                path_outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=120,\n                    do_sample=True,\n                    output_hidden_states=True,\n                    return_dict_in_generate=True,\n                    pad_token_id=tokenizer.eos_token_id\n                )\n\n            # è§£ç è·¯å¾„æ–‡æœ¬\n            generated_ids = path_outputs.sequences[0, len(inputs.input_ids[0]):]\n            path_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n            \n            # è¿‡æ»¤æ— æ•ˆå›å¤\n            if any(word in path_text.lower() for word in [\"help\", \"context\", \"provide\", \"missing\", \"information\"]):\n                path_text = f\"[Invalid: Did not include missing content]\"\n            path_texts.append(path_text)\n\n            # è·å–ç”Ÿæˆè·¯å¾„çš„hidden stateï¼ˆæœ€åä¸€ä¸ªtokençš„hidden stateï¼‰\n            if len(path_outputs.hidden_states) > 0 and len(path_outputs.hidden_states[-1]) > 0:\n                path_hidden = path_outputs.hidden_states[-1][-1][0, -1, :].cpu().numpy()\n                generated_hiddens.append(path_hidden)\n            else:\n                generated_hiddens.append(None)\n\n        # è®¡ç®—ç›¸ä¼¼åº¦ï¼šå®é™…è·³æ­¥ vs ç”Ÿæˆè·¯å¾„\n        if generated_hiddens and any(h is not None for h in generated_hiddens):\n            sims = calculate_similarity_between_paths(actual_skip_hidden, generated_hiddens)\n        else:\n            sims = [0.0, 0.0, 0.0]\n\n        results[\"similarities\"][style] = {\n            \"mean\": float(np.mean(sims)),\n            \"std\": float(np.std(sims)),\n            \"all\": sims\n        }\n        results[\"paths\"][style] = path_texts\n        results[\"generated_hidden_states\"][style] = generated_hiddens\n\n    return results\n\n# =====================================================\n# å•ä¸ªé—®é¢˜å®éªŒæµç¨‹ï¼ˆåŒ…å«æ•°æ®ä¿å­˜ï¼‰\n# =====================================================\ndef main_experiment_with_save(test_sample):\n    \"\"\"ä¿®æ”¹ç‰ˆçš„å®éªŒä¸€æµç¨‹ï¼ŒåŒ…å«æ•°æ®ä¿å­˜\"\"\"\n    test_id = test_sample['id']\n    problem = test_sample['problem']\n    expected_steps = test_sample['expected_complete_reasoning']\n    \n    # ç”Ÿæˆåˆ†æ­¥æ¨ç†\n    step_by_step_prompt = build_llama3_prompt(\n        \"You solve reasoning problems in a FIXED FORMAT. Follow EXACTLY: 1. Start with 'Step 1: ...' 2. Each step has only core logic 3. End with 'Final Answer: ...' 4. No extra greetings/explanations 5. Do NOT ask for more information (try your best if problem is simple)\",\n        f\"Problem: {problem}\\n\\nSolve step by step in the FIXED FORMAT (only core logic, no extra words):\"\n    )\n    inputs = tokenizer(step_by_step_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=500,\n            num_return_sequences=1,\n            do_sample=True,\n            output_hidden_states=True,\n            return_dict_in_generate=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    full_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    step_by_step_response = full_response.split(\"assistant\")[-1].strip() if \"assistant\" in full_response else full_response\n    \n    # æå–æ­¥éª¤çº§hidden states\n    prompt_length = inputs.input_ids.shape[1]\n    step_data = extract_step_level_hidden_states(outputs, prompt_length, step_by_step_response)\n    \n    # è¯†åˆ«è·³æ­¥ä½ç½®\n    skipping_info, judge_analysis = identify_skipped_steps(step_by_step_response, problem, expected_steps)\n    \n    # ä¸ºæ¯ä¸ªè·³æ­¥ç”Ÿæˆå¯¹æ¯”è·¯å¾„\n    all_skip_results = []\n    if skipping_info['skipped_steps']:\n        for skip_step in skipping_info['skipped_steps']:\n            single_skip_info = {\n                'current_skip_step': skip_step,\n                'skipped_content': skipping_info['skipped_content'],\n                'skipping_location': skipping_info['skipping_location']\n            }\n            comparison_results = generate_comparison_paths_with_llm_fixed(\n                single_skip_info, \n                step_data, \n                problem,\n                tokenizer, \n                model,\n                outputs,\n                prompt_length\n            )\n            all_skip_results.append({\n                'skip_step': skip_step,\n                'similarity_results': comparison_results.get('similarities', {}),\n                'paths': comparison_results.get('paths', {}),\n                'actual_hidden_shape': comparison_results.get('actual_skip_hidden_shape', 'unknown'),\n                'status': comparison_results.get('status', 'unknown')\n            })\n    \n    # æ„å»ºå®Œæ•´ç»“æœ\n    result = {\n        'test_id': test_id,\n        'problem': problem,\n        'step_by_step_response': step_by_step_response,\n        'judge_analysis': judge_analysis,\n        'skipping_info': skipping_info,\n        'all_skip_results': all_skip_results,\n        'step_data': step_data,\n        'prompt_length': prompt_length\n    }\n    \n    # ä¿å­˜æ•°æ®\n    save_experiment1_data(result)\n    \n    return result\n\n# =====================================================\n# æ‰¹é‡è¿è¡Œæµ‹è¯•é›†ï¼ˆåŒ…å«æ•°æ®ä¿å­˜ï¼‰\n# =====================================================\ndef run_experiment1_with_save():\n    \"\"\"è¿è¡Œå®éªŒä¸€å¹¶ä¿å­˜æ‰€æœ‰æ•°æ®\"\"\"\n    testset = load_testset(\"/kaggle/input/testset/testset.csv\")\n    \n    print(\"=\" * 70)\n    print(f\"å®éªŒä¸€ï¼šè·³æ­¥è¯†åˆ«ä¸Hidden Statesä¿å­˜ - å…± {len(testset)} ä¸ªé—®é¢˜\")\n    print(\"=\" * 70)\n    \n    style_to_path = {\n        \"detailed\": \"A->C->B\",\n        \"shortcut\": \"A->B\",\n        \"alternative\": \"A->D->B\"\n    }\n    \n    all_results = []\n    for i, test_sample in enumerate(testset):\n        print(f\"\\nå¤„ç†è¿›åº¦: {i+1}/{len(testset)}\")\n        result = main_experiment_with_save(test_sample)\n        all_results.append(result)\n        \n        test_id = result['test_id']\n        skipping_info = result['skipping_info']\n        all_skip_results = result['all_skip_results']\n        step_by_step_response = result['step_by_step_response']\n        judge_analysis = result['judge_analysis']\n        \n        print(f\"\\nã€{test_id} è¯¦ç»†éªŒè¯ã€‘\")\n        print(f\"æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\\n{step_by_step_response[:500]}...\")\n        print(f\"\\nLLM Judgeåˆ†æï¼š\\n{judge_analysis}\")\n        \n        # è¾“å‡ºæ‰€æœ‰è·³æ­¥çš„å¯¹æ¯”è·¯å¾„\n        if all_skip_results:\n            for skip in all_skip_results:\n                skip_step = skip['skip_step']\n                skip_num = skip_step.split('_')[-1]\n                paths = skip['paths']\n                similarities = skip['similarity_results']\n                \n                print(f\"\\nã€è·³æ­¥{skip_num}çš„å¯¹æ¯”è·¯å¾„ã€‘\")\n                for style, path_texts in paths.items():\n                    print(f\"\\n{style_to_path[style]}ï¼ˆ{style}é£æ ¼ï¼‰ï¼š\")\n                    for i, text in enumerate(path_texts, 1):\n                        print(f\"  è·¯å¾„{i}ï¼š{text[:200]}...\")  # é™åˆ¶é•¿åº¦\n                \n                # è¾“å‡ºè¯¥è·³æ­¥çš„æœ€ä½³è·¯å¾„\n                if similarities:\n                    best_style = max(similarities.items(), key=lambda x: x[1]['mean'])[0]\n                    similar_path = style_to_path[best_style]\n                    best_similarity = round(similarities[best_style]['mean'], 4)\n                    print(f\"\\n{test_id} è·³æ­¥{skip_num}: ç›¸ä¼¼è·¯å¾„ï¼š{similar_path} ç›¸ä¼¼åº¦ï¼š{best_similarity}\")\n                else:\n                    print(f\"\\n{test_id} è·³æ­¥{skip_num}: ç›¸ä¼¼è·¯å¾„ï¼šæ—  ç›¸ä¼¼åº¦ï¼š0.0\")\n        else:\n            print(f\"\\n{test_id}: æ— è·³æ­¥ï¼ˆæ ¹æ®Judgeåˆ†æï¼‰\")\n    \n    # æ±‡æ€»æ•°æ®\n    summarize_experiment1_data()\n    \n    # ä¿å­˜å®Œæ•´æ•°æ®é›†\n    if save_complete_experiment1_data():\n        print(f\"\\nğŸ“Š å®éªŒä¸€æ•°æ®ç»Ÿè®¡:\")\n        print(f\"  æ€»æµ‹è¯•æ¡ˆä¾‹: {len(experiment1_data)}\")\n        total_steps = sum(len(data['step_data']) for data in experiment1_data.values())\n        total_skips = sum(len(data['skipping_info']['skipped_steps']) for data in experiment1_data.values())\n        print(f\"  æ€»æ¨ç†æ­¥éª¤: {total_steps}\")\n        print(f\"  æ€»è·³æ­¥æ•°é‡: {total_skips}\")\n    \n    print(f\"\\nğŸ‰ å®éªŒä¸€å®Œæˆï¼æ‰€æœ‰hidden statesæ•°æ®å·²ä¿å­˜\")\n    return all_results\n\n# =====================================================\n# æ‰§è¡Œå®éªŒä¸€ï¼ˆåŒ…å«æ•°æ®ä¿å­˜ï¼‰\n# =====================================================\nif __name__ == \"__main__\":\n    # è¿è¡Œå®éªŒä¸€å¹¶ä¿å­˜æ•°æ®\n    experiment1_results = run_experiment1_with_save()\n    \n    # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶\n    print(\"\\nğŸ“ ä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨:\")\n    for file in os.listdir('.'):\n        if file.startswith('experiment1_') and file.endswith('.pkl'):\n            file_size = os.path.getsize(file) / 1024  # KB\n            print(f\"  {file} ({file_size:.1f} KB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T14:54:08.616761Z","iopub.execute_input":"2025-11-05T14:54:08.616967Z","iopub.status.idle":"2025-11-05T14:58:55.052573Z","shell.execute_reply.started":"2025-11-05T14:54:08.616952Z","shell.execute_reply":"2025-11-05T14:58:55.051755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### æ–¹æ³•äºŒæµ‹è¯•","metadata":{}},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef load_experiment1_data(test_id):\n    \"\"\"Load experiment 1 data for specific test ID\"\"\"\n    try:\n        filename = f\"experiment1_hidden_states_{test_id}.pkl\"\n        with open(filename, 'rb') as f:\n            return pickle.load(f)\n    except FileNotFoundError:\n        print(f\"Not found saved data for {test_id}\")\n        return None\n\nclass LogitLensAnalyzer:\n    \"\"\"Logit Lens Analyzer - Refactored according to Method 2 requirements\"\"\"\n    \n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = model.device\n    \n    def get_concept_probability_curve(self, input_text, target_concept, target_position=-1):\n        \"\"\"\n        Get concept probability curve across model layers\n        \"\"\"\n        # Encode input\n        inputs = self.tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=2048).to(self.device)\n        \n        # Get target concept token IDs\n        if isinstance(target_concept, str):\n            target_token_ids = self.tokenizer.encode(target_concept, add_special_tokens=False)\n            print(f\"Target concept '{target_concept}' token IDs: {target_token_ids}\")\n        else:\n            target_token_ids = target_concept\n        \n        if not target_token_ids:\n            raise ValueError(f\"Cannot encode target concept: {target_concept}\")\n        \n        # Forward pass to get all layer hidden states\n        with torch.no_grad():\n            outputs = self.model(\n                **inputs,\n                output_hidden_states=True,\n                output_attentions=False,\n                return_dict=True\n            )\n        \n        hidden_states = outputs.hidden_states\n        layers = []\n        probabilities = []\n        \n        # Get LM head\n        lm_head = self.model.lm_head\n        \n        # Apply Logit Lens to each layer\n        for layer_idx, hidden_state in enumerate(hidden_states):\n            # Select target position hidden state\n            if target_position == -1:\n                target_hidden = hidden_state[0, -1, :]  # Last token\n            else:\n                target_hidden = hidden_state[0, target_position, :]\n            \n            # Calculate logits through LM head\n            layer_logits = lm_head(target_hidden.unsqueeze(0))\n            \n            # Calculate softmax probabilities\n            probs = torch.softmax(layer_logits, dim=-1)[0]\n            \n            # Calculate target concept probability\n            concept_probs = []\n            for token_id in target_token_ids:\n                if token_id < probs.shape[0]:\n                    concept_probs.append(probs[token_id].item())\n            \n            if concept_probs:\n                max_prob = max(concept_probs)\n                layers.append(layer_idx)\n                probabilities.append(max_prob)\n        \n        print(f\"Generated probability curve: {len(layers)} layers, probability range: {min(probabilities):.4f}-{max(probabilities):.4f}\")\n        return layers, probabilities, []\n    \n    def generate_standard_curves(self, xp, reasoning_paths, target_concept, question):\n        \"\"\"Generate standard curves - according to Method 2 requirements\"\"\"\n        standard_curves = {}\n        \n        for path_type, reasoning in reasoning_paths.items():\n            if reasoning:\n                # Build input: XP + reasoning path + question\n                input_text = f\"{xp}\\n\\n{reasoning}\\n\\n{question}\"\n                print(f\"Generating {path_type} path standard curve...\")\n                print(f\"Input text: {input_text[:200]}...\")\n                \n                try:\n                    layers, probs, _ = self.get_concept_probability_curve(input_text, target_concept)\n                    standard_curves[path_type] = {\n                        'layers': layers,\n                        'probabilities': probs,\n                        'input_text': input_text[:100] + \"...\"\n                    }\n                    print(f\"âœ“ Generated {path_type} path standard curve\")\n                except Exception as e:\n                    print(f\"âŒ Failed to generate {path_type} path standard curve: {e}\")\n        \n        return standard_curves\n    \n    def analyze_verification_case(self, test_id, skip_step_key, xp, question, target_concept):\n        \"\"\"\n        Analyze verification case (verification curve)\n        \"\"\"\n        # Load experiment 1 data\n        exp1_data = load_experiment1_data(test_id)\n        if not exp1_data:\n            return {\"error\": f\"Not found experiment 1 data for {test_id}\"}\n        \n        # Get skip step hidden state (hs_A)\n        skip_hidden = None\n        for step in exp1_data.get('step_data', []):\n            if step['step_number'] == skip_step_key:\n                skip_hidden = step['hidden_state']\n                break\n        \n        if skip_hidden is None:\n            return {\"error\": f\"Not found hidden state for {skip_step_key}\"}\n        \n        print(f\"Using skip hidden state (shape: {skip_hidden.shape}) + XP + Question to build verification curve\")\n        \n        # Build verification input\n        verification_input = f\"{xp}\\n\\n{question}\"\n        \n        # Get verification curve\n        try:\n            layers, probabilities, _ = self.get_concept_probability_curve(verification_input, target_concept)\n            \n            return {\n                'test_id': test_id,\n                'skip_step': skip_step_key,\n                'verification_curve': {\n                    'layers': layers,\n                    'probabilities': probabilities\n                },\n                'skip_hidden_shape': skip_hidden.shape,\n                'verification_input': verification_input[:100] + \"...\"\n            }\n        except Exception as e:\n            return {\"error\": f\"Failed to get verification curve: {e}\"}\n\ndef generate_multi_step_verification_scenarios(problem, skipped_steps, skipped_content, all_skip_results):\n    \"\"\"\n    Generate multi-step verification scenarios - based on experiment 1 results\n    \"\"\"\n    scenarios = []\n    \n    # Get similarity information from experiment 1 results\n    similarity_info = {}\n    if all_skip_results:\n        for skip_result in all_skip_results:\n            skip_step = skip_result['skip_step']\n            similarities = skip_result.get('similarity_results', {})\n            if similarities:\n                # Find the most similar path\n                best_style = max(similarities.items(), key=lambda x: x[1]['mean'])[0]\n                similarity_info[skip_step] = best_style\n    \n    for i, step in enumerate(skipped_steps):\n        step_num = step.split('_')[1] if '_' in step else str(i+1)\n        \n        # Determine intermediate step based on experiment 1 results\n        intermediate_step = \"D\"  # Default\n        if step in similarity_info:\n            style = similarity_info[step]\n            if style == \"detailed\":\n                intermediate_step = \"C\"\n            elif style == \"alternative\": \n                intermediate_step = \"D\"\n            elif style == \"shortcut\":\n                intermediate_step = \"direct\"  # No intermediate step\n        \n        # Generate scenarios based on problem content\n        problem_lower = problem.lower()\n        skipped_lower = skipped_content.lower()\n        \n        if \"building\" in problem_lower and \"east\" in problem_lower:\n            # Use more specific and less common target concepts\n            if intermediate_step == \"C\":\n                complete_path = \"You approach the building from east -> You rotate 180 degrees to face west -> You take steps in westward direction -> You are now inside the building\"\n                target_concept = \"rotate 180 degrees\"  # More specific concept\n            elif intermediate_step == \"D\":\n                complete_path = \"You stand at east entrance -> You orient yourself toward west -> You proceed forward into building -> You reach interior\" \n                target_concept = \"orient toward west\"  # More specific concept\n            else:\n                complete_path = \"You enter from east side -> You move in opposite direction -> You access building interior\"\n                target_concept = \"opposite direction\"  # Less common concept\n                \n            scenario = {\n                \"skipped_step\": step,\n                \"xp\": \"Context: Building entrance is on east side, interior is accessed from west\",\n                \"target_concept\": target_concept,\n                \"complete_path\": complete_path,\n                \"skip_path\": \"You enter building -> You are inside\",  # Make skip path more different\n                \"verification_question\": \"What directional change is needed when entering from east?\",\n                \"intermediate_step\": intermediate_step\n            }\n            \n        elif \"bachelor\" in problem_lower:\n            scenario = {\n                \"skipped_step\": step,\n                \"xp\": \"Formal definition: Bachelor refers to adult male without marital history\",\n                \"target_concept\": \"without marital history\",  # More specific\n                \"complete_path\": \"Bachelor definition -> Unmarried adult male -> Never entered marriage -> John meets criteria -> John is bachelor\",\n                \"skip_path\": \"Bachelor definition -> John is bachelor\",  # More different\n                \"verification_question\": \"What specific condition defines bachelor status?\",\n                \"intermediate_step\": \"C\"\n            }\n            \n        elif \"quadratic\" in problem_lower or \"xÂ²\" in problem:\n            if \"complete square\" in skipped_lower:\n                scenario = {\n                    \"skipped_step\": step,\n                    \"xp\": \"Mathematical expression: xÂ² - 4x + 7 requires completion of square method\",\n                    \"target_concept\": \"add subtract four\",  # More specific\n                    \"complete_path\": \"Quadratic expression -> Identify coefficient -4 -> Calculate (4/2)Â²=4 -> Add and subtract 4 -> Rewrite as (x-2)Â²+3 -> Analyze properties\",\n                    \"skip_path\": \"Quadratic expression -> Analyze properties\",  # More different\n                    \"verification_question\": \"What numerical operation completes the square for xÂ² - 4x?\",\n                    \"intermediate_step\": \"C\"\n                }\n            else:\n                scenario = {\n                    \"skipped_step\": step,\n                    \"xp\": \"Algebraic analysis of quadratic polynomial\",\n                    \"target_concept\": \"find factors\",  # More specific\n                    \"complete_path\": \"Quadratic polynomial -> Calculate discriminant -> Identify root pairs -> Factor expression -> Analyze solution set -> Draw conclusion\",\n                    \"skip_path\": \"Quadratic polynomial -> Draw conclusion\",\n                    \"verification_question\": \"What process finds factors of quadratic expression?\",\n                    \"intermediate_step\": \"C\"\n                }\n                \n        elif \"triangular\" in problem_lower or \"number\" in problem_lower:\n            scenario = {\n                \"skipped_step\": step,\n                \"xp\": \"Mathematical sequence of triangular numbers\",\n                \"target_concept\": \"summation formula\",  # More specific\n                \"complete_path\": \"Triangular sequence -> Observe pattern -> Derive summation -> Prove formula Tâ‚™=n(n+1)/2 -> Apply to problem -> Verify result\",\n                \"skip_path\": \"Triangular sequence -> Verify result\",\n                \"verification_question\": \"What formula calculates triangular numbers?\",\n                \"intermediate_step\": \"C\"\n            }\n                \n        else:\n            # Generic scenario\n            scenario = {\n                \"skipped_step\": step,\n                \"xp\": f\"Problem context: {problem[:80]}...\",\n                \"target_concept\": f\"intermediate_reasoning_{intermediate_step}\",\n                \"complete_path\": f\"Initial state -> Detailed step {intermediate_step}: {skipped_content[:50]}... -> Multiple verifications -> Final conclusion\",\n                \"skip_path\": \"Initial state -> Final conclusion\",\n                \"verification_question\": f\"What reasoning occurs in intermediate step {intermediate_step}?\",\n                \"intermediate_step\": intermediate_step\n            }\n        \n        scenarios.append(scenario)\n        print(f\"Generated step {step} verification scenario, intermediate step: {intermediate_step}\")\n    \n    return scenarios\n\ndef plot_logit_lens_curves_with_analysis(standard_curves, verification_curve, similarity_analysis, title=\"Logit Lens Analysis\"):\n    \"\"\"Plot Logit Lens curves with analysis results\"\"\"\n    plt.figure(figsize=(14, 8))\n    \n    # Plot standard curves\n    colors = {'complete': 'blue', 'skip': 'red'}\n    labels = {'complete': 'Complete Path (A->C->B)', 'skip': 'Skip Path (A->B)'}\n    \n    for curve_name, curve_data in standard_curves.items():\n        if curve_data and 'layers' in curve_data and 'probabilities' in curve_data:\n            plt.plot(curve_data['layers'], curve_data['probabilities'], \n                    label=labels.get(curve_name, curve_name), \n                    linewidth=2, marker='o', color=colors.get(curve_name, 'green'))\n    \n    # Plot verification curve\n    if verification_curve and 'layers' in verification_curve and 'probabilities' in verification_curve:\n        plt.plot(verification_curve['layers'], verification_curve['probabilities'],\n                label='Verification (hs_A + XP + Q)', linewidth=3, marker='s', linestyle='--', color='purple')\n    \n    plt.xlabel('Model Layer')\n    plt.ylabel('Concept Probability')\n    \n    # Add analysis conclusion to title\n    conclusion = similarity_analysis.get('conclusion', '')\n    full_title = f\"{title}\\n{conclusion}\"\n    plt.title(full_title)\n    \n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\ndef analyze_curve_similarity_detailed(standard_curves, verification_result, intermediate_step):\n    \"\"\"\n    Detailed curve similarity analysis - judge implicit reasoning based on Method 2 logic\n    \"\"\"\n    if 'verification_curve' not in verification_result:\n        return {\"error\": \"No verification curve data\"}\n    \n    verification_probs = verification_result['verification_curve']['probabilities']\n    \n    similarities = {}\n    for curve_name, curve_data in standard_curves.items():\n        if curve_data and 'probabilities' in curve_data:\n            std_probs = curve_data['probabilities']\n            # Simple cosine similarity calculation\n            if len(verification_probs) == len(std_probs) and len(verification_probs) > 0:\n                v_array = np.array(verification_probs).reshape(1, -1)\n                s_array = np.array(std_probs).reshape(1, -1)\n                similarity = cosine_similarity(v_array, s_array)[0][0]\n                similarities[curve_name] = float(similarity)\n    \n    # Judge implicit reasoning state based on Method 2 logic\n    if similarities:\n        complete_sim = similarities.get('complete', 0)\n        skip_sim = similarities.get('skip', 0)\n        \n        # Judgment logic: if verification curve is closer to complete path, implicit reasoning occurred\n        if complete_sim > skip_sim and complete_sim > 0.6:\n            reasoning_state = \"Implicitly reasoned\"\n            confidence = \"High\"\n        elif complete_sim > skip_sim:\n            reasoning_state = \"Likely implicitly reasoned\" \n            confidence = \"Medium\"\n        elif skip_sim > complete_sim and skip_sim > 0.6:\n            reasoning_state = \"Not implicitly reasoned\"\n            confidence = \"High\"\n        else:\n            reasoning_state = \"Uncertain\"\n            confidence = \"Low\"\n        \n        best_match = max(similarities.items(), key=lambda x: x[1])\n        conclusion = f\"Step {intermediate_step}: {reasoning_state} (Confidence: {confidence})\"\n        \n        return {\n            'similarities': similarities,\n            'best_match': best_match[0],\n            'best_similarity': best_match[1],\n            'reasoning_state': reasoning_state,\n            'confidence': confidence,\n            'conclusion': conclusion,\n            'complete_similarity': complete_sim,\n            'skip_similarity': skip_sim\n        }\n    else:\n        return {\"error\": \"Cannot calculate similarity\"}\n\ndef run_experiment2_analysis_enhanced():\n    \"\"\"\n    Enhanced Experiment 2 Analysis - Support multi-step reasoning analysis\n    \"\"\"\n    print(\"=\" * 70)\n    print(\"Experiment 2: Implicit Reasoning Verification via Logit Lens (Enhanced)\")\n    print(\"=\" * 70)\n    \n    # Initialize analyzer\n    analyzer = LogitLensAnalyzer(model, tokenizer)\n    \n    # Load experiment 1 data\n    try:\n        with open(\"experiment1_complete_data.pkl\", 'rb') as f:\n            exp1_data = pickle.load(f)\n        print(f\"âœ“ Loaded experiment 1 data, total {len(exp1_data)} test cases\")\n    except FileNotFoundError:\n        print(\"âŒ Experiment 1 data not found, please run experiment 1 first\")\n        return\n    \n    results = {}\n    \n    for test_id, data in exp1_data.items():\n        print(f\"\\n{'='*50}\")\n        print(f\"Analyzing {test_id}\")\n        print(f\"{'='*50}\")\n        \n        if not data['skipping_info']['skipped_steps']:\n            print(f\"  {test_id} no skipped steps, skipping\")\n            continue\n        \n        # Generate multi-step verification scenarios using experiment 1 results\n        scenarios = generate_multi_step_verification_scenarios(\n            data['problem'],\n            data['skipping_info']['skipped_steps'],\n            data['skipping_info']['skipped_content'],\n            data.get('all_skip_results', [])\n        )\n        \n        test_results = []\n        step_conclusions = []  # Store conclusions for each step\n        \n        for scenario in scenarios:\n            print(f\"\\n  Analyzing scenario: {scenario['skipped_step']}\")\n            print(f\"    Target concept: {scenario['target_concept']}\")\n            print(f\"    Intermediate step: {scenario['intermediate_step']}\")\n            \n            # Generate standard curves\n            reasoning_paths = {\n                'complete': scenario['complete_path'],  # A->C->B\n                'skip': scenario['skip_path']           # A->B\n            }\n            \n            standard_curves = analyzer.generate_standard_curves(\n                scenario['xp'],\n                reasoning_paths,\n                scenario['target_concept'],\n                scenario['verification_question']\n            )\n            \n            # Analyze verification case - using hidden states + xp + question\n            verification_result = analyzer.analyze_verification_case(\n                test_id,\n                scenario['skipped_step'],\n                scenario['xp'],\n                scenario['verification_question'],\n                scenario['target_concept']\n            )\n            \n            # Detailed similarity analysis\n            similarity_analysis = analyze_curve_similarity_detailed(\n                standard_curves, \n                verification_result,\n                scenario['intermediate_step']\n            )\n            \n            # Output detailed results\n            print(f\"    Similarity analysis:\")\n            print(f\"      - Complete path similarity: {similarity_analysis.get('complete_similarity', 0):.4f}\")\n            print(f\"      - Skip path similarity: {similarity_analysis.get('skip_similarity', 0):.4f}\")\n            print(f\"      - Conclusion: {similarity_analysis.get('conclusion', 'Analysis failed')}\")\n            \n            # Store step conclusion\n            step_conclusions.append(similarity_analysis.get('conclusion', 'Analysis failed'))\n            \n            # Plot enhanced curves\n            if 'verification_curve' in verification_result and standard_curves:\n                plot_title = f\"{test_id} - {scenario['skipped_step']}\\nTarget: {scenario['target_concept']}\"\n                plot_logit_lens_curves_with_analysis(\n                    standard_curves,\n                    verification_result['verification_curve'],\n                    similarity_analysis,\n                    title=plot_title\n                )\n            \n            test_results.append({\n                'scenario': scenario,\n                'standard_curves': standard_curves,\n                'verification_result': verification_result,\n                'similarity_analysis': similarity_analysis\n            })\n        \n        # Output multi-step comprehensive conclusions\n        print(f\"\\n  ğŸ“Š {test_id} Multi-step Reasoning Analysis Summary:\")\n        for i, conclusion in enumerate(step_conclusions):\n            print(f\"    Step {i+1}: {conclusion}\")\n        \n        results[test_id] = {\n            'test_results': test_results,\n            'step_conclusions': step_conclusions,\n            'problem': data['problem'],\n            'skipped_steps': data['skipping_info']['skipped_steps']\n        }\n    \n    # Save experiment 2 results\n    with open(\"experiment2_enhanced_results.pkl\", 'wb') as f:\n        pickle.dump(results, f)\n    \n    print(f\"\\nğŸ‰ Experiment 2 completed! Analyzed {len(results)} test cases\")\n    print(\"ğŸ“ Results saved to: experiment2_enhanced_results.pkl\")\n    \n    # Output overall statistics\n    print(f\"\\nğŸ“ˆ Overall Statistics:\")\n    total_steps = sum(len(data['skipped_steps']) for data in results.values())\n    print(f\"  Total analyzed steps: {total_steps}\")\n    \n    return results\n\n# Run enhanced experiment 2\nif __name__ == \"__main__\":\n    print(\"Running Experiment 2 - Implicit Reasoning Verification via Logit Lens (Enhanced)\")\n    experiment2_results = run_experiment2_analysis_enhanced()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T12:25:30.322421Z","iopub.execute_input":"2025-11-05T12:25:30.322709Z","iopub.status.idle":"2025-11-05T12:25:42.077843Z","shell.execute_reply.started":"2025-11-05T12:25:30.322684Z","shell.execute_reply":"2025-11-05T12:25:42.077157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T15:33:52.864991Z","iopub.execute_input":"2025-11-05T15:33:52.865702Z","iopub.status.idle":"2025-11-05T15:33:52.894739Z","shell.execute_reply.started":"2025-11-05T15:33:52.865675Z","shell.execute_reply":"2025-11-05T15:33:52.894206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### new version","metadata":{}},{"cell_type":"code","source":"# å…¨å±€å˜é‡ä¿å­˜å®éªŒä¸€æ•°æ®\nexperiment1_data = {}\n\ndef save_experiment1_data(detailed_result):\n    \"\"\"ä¿å­˜å®éªŒä¸€çš„hidden statesæ•°æ®\"\"\"\n    test_id = detailed_result['test_id']\n    \n    # æ„å»ºä¿å­˜çš„æ•°æ®ç»“æ„\n    saved_data = {\n        'test_id': test_id,\n        'problem': detailed_result.get('problem', ''),\n        'step_by_step_response': detailed_result['step_by_step_response'],\n        'step_data': [],\n        'skipping_info': detailed_result['skipping_info'],\n        'all_skip_results': detailed_result['all_skip_results'],\n        'hidden_states_info': {\n            'total_steps': len(detailed_result.get('step_data', [])),\n            'has_skipped_steps': len(detailed_result['skipping_info']['skipped_steps']) > 0,\n            'saved_timestamp': str(np.datetime64('now')),\n            'target_layer': -1\n        }\n    }\n    \n    # ä¿å­˜æ­¥éª¤æ•°æ®ï¼ˆåŒ…æ‹¬hidden stateså’Œlayer_idxï¼‰\n    for step in detailed_result.get('step_data', []):\n        step_data_copy = step.copy()\n        if 'hidden_state' in step_data_copy:\n            step_data_copy['hidden_state'] = step_data_copy['hidden_state']\n        step_data_copy['layer_idx'] = step.get('layer_idx', -1)\n        saved_data['step_data'].append(step_data_copy)\n    \n    # ä¿å­˜åˆ°å…¨å±€å˜é‡\n    experiment1_data[test_id] = saved_data\n    \n    # ä¿å­˜åˆ°æ–‡ä»¶\n    filename = f\"experiment1_hidden_states_{test_id}.pkl\"\n    with open(filename, 'wb') as f:\n        pickle.dump(saved_data, f)\n    \n    print(f\"âœ“ å·²ä¿å­˜ {test_id} çš„hidden statesæ•°æ®åˆ° {filename}\")\n    print(f\"  - æ­¥éª¤æ•°é‡: {len(saved_data['step_data'])}\")\n    print(f\"  - è·³æ­¥æ•°é‡: {len(saved_data['skipping_info']['skipped_steps'])}\")\n    if saved_data['step_data']:\n        print(f\"  - hidden stateå½¢çŠ¶: {saved_data['step_data'][0]['hidden_state'].shape}\")\n        print(f\"  - æå–å±‚ç´¢å¼•: {saved_data['step_data'][0]['layer_idx']}\")\n\ndef save_complete_experiment1_data():\n    \"\"\"ä¿å­˜å®Œæ•´çš„å®éªŒä¸€æ•°æ®é›†\"\"\"\n    if experiment1_data:\n        with open(\"experiment1_complete_data.pkl\", 'wb') as f:\n            pickle.dump(experiment1_data, f)\n        print(f\"\\nğŸ‰ å·²ä¿å­˜å®Œæ•´çš„å®éªŒä¸€æ•°æ®ï¼Œå…± {len(experiment1_data)} ä¸ªæµ‹è¯•æ¡ˆä¾‹\")\n        return True\n    else:\n        print(\"âŒ æ²¡æœ‰å®éªŒæ•°æ®å¯ä¿å­˜\")\n        return False\n\ndef summarize_experiment1_data():\n    \"\"\"æ±‡æ€»å®éªŒä¸€æ•°æ®\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"å®éªŒä¸€æ•°æ®æ±‡æ€»\")\n    print(\"=\"*60)\n    \n    if not experiment1_data:\n        print(\"æš‚æ— å®éªŒæ•°æ®\")\n        return\n    \n    for test_id, data in experiment1_data.items():\n        print(f\"\\n{test_id}:\")\n        print(f\"  é—®é¢˜: {data['problem'][:100]}...\")\n        print(f\"  æ¨ç†æ­¥éª¤: {len(data['step_data'])}\")\n        print(f\"  è·³æ­¥: {len(data['skipping_info']['skipped_steps'])}\")\n        print(f\"  è·³æ­¥è¯¦æƒ…: {data['skipping_info']['skipped_steps']}\")\n        if data['step_data']:\n            print(f\"  æå–å±‚ç´¢å¼•: {data['step_data'][0]['layer_idx']}\")\n\ndef calculate_similarity_between_paths(actual_skip_hidden, generated_paths_hidden_states):\n    \"\"\"\n    è®¡ç®—å®é™…è·³æ­¥hidden statesä¸ç”Ÿæˆè·¯å¾„hidden statesçš„ç›¸ä¼¼åº¦\n    \"\"\"\n    similarities = []\n    actual_vector = actual_skip_hidden.reshape(1, -1)\n    \n    for path_hidden in generated_paths_hidden_states:\n        if path_hidden is not None:\n            path_vector = path_hidden.reshape(1, -1)\n            sim = cosine_similarity(actual_vector, path_vector)[0][0]\n            similarities.append(float(sim))\n        else:\n            similarities.append(0.0)\n    \n    return similarities\n\ndef get_actual_skip_hidden_state(skip_step_key, step_data, skipping_info, outputs, prompt_length):\n    \"\"\"\n    è·å–å®é™…è·³æ­¥æ­¥éª¤çš„hidden state\n    \"\"\"\n    # æ–¹æ³•1: å°è¯•ä»step_dataä¸­ç›´æ¥æ‰¾åˆ°è·³æ­¥æ­¥éª¤\n    for step in step_data:\n        if step['step_number'] == skip_step_key:\n            return step['hidden_state'], step.get('layer_idx', -1)\n    \n    # æ–¹æ³•2: å¦‚æœæ‰¾ä¸åˆ°ï¼Œæ ¹æ®è·³æ­¥ä½ç½®æ¨æ–­\n    if step_data:\n        return step_data[-1]['hidden_state'], step_data[-1].get('layer_idx', -1)\n    \n    # æ–¹æ³•3: è¿”å›æ•´ä¸ªæ¨ç†çš„æœ€åä¸€ä¸ªhidden state\n    generated_ids = outputs.sequences[0, prompt_length:]\n    layer_idx = -1\n    if len(generated_ids) > 0:\n        actual_hidden_idx = prompt_length + len(generated_ids) - 1\n        if actual_hidden_idx < len(outputs.hidden_states[layer_idx]):\n            hidden_state = outputs.hidden_states[layer_idx][actual_hidden_idx][0, -1, :].cpu().numpy()\n            return hidden_state, layer_idx\n    \n    return None, -1\n\ndef build_llama3_prompt(system_msg, user_msg):\n    \"\"\"æ„å»ºç¬¦åˆLlama 3æ ¼å¼çš„prompt\"\"\"\n    return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_msg}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{user_msg}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n# =====================================================\n# è¯»å–CSVæµ‹è¯•é›†å‡½æ•°\n# =====================================================\ndef load_testset(csv_path=\"/kaggle/input/testset/testset.csv\"):\n    \"\"\"è¯»å–CSVæ ¼å¼çš„æµ‹è¯•é›†\"\"\"\n    testset = []\n    with open(csv_path, 'r', encoding='utf-8') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            expected_steps = row['expected_complete_reasoning'].split('||')\n            testset.append({\n                'id': row['id'],\n                'problem': row['problem'],\n                'expected_complete_reasoning': expected_steps\n            })\n    print(f\"âœ“ æˆåŠŸåŠ è½½ {len(testset)} ä¸ªæµ‹è¯•é—®é¢˜\")\n    return testset\n\n# =====================================================\n# æ­¥éª¤çº§hidden statesæå–ï¼ˆä¿®æ­£ç‰ˆï¼‰\n# =====================================================\ndef extract_step_level_hidden_states(outputs, prompt_length, generated_text):\n    steps = parse_step_structure(generated_text)\n    step_data = []\n    generated_ids = outputs.sequences[0, prompt_length:]\n    full_generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    total_hidden_length = len(generated_ids)\n\n    layer_idx = -1\n    \n    current_position = 0\n    for step_num, step_text in steps.items():\n        step_text_clean = step_text.strip()\n        step_start = full_generated_text.find(step_text_clean, current_position)\n        \n        if step_start == -1:\n            full_text_clean = \" \".join(full_generated_text.split())\n            step_text_clean_no_space = \" \".join(step_text_clean.split())\n            step_start = full_text_clean.find(step_text_clean_no_space, current_position)\n            if step_start == -1:\n                continue\n        \n        step_tokens = tokenizer.encode(step_text_clean, add_special_tokens=False)\n        step_token_length = len(step_tokens)\n        if step_token_length == 0:\n            continue\n        \n        text_before_step = full_generated_text[:step_start]\n        tokens_before_step = tokenizer.encode(text_before_step, add_special_tokens=False)\n        step_start_token_idx = len(tokens_before_step)\n        step_end_token_idx = step_start_token_idx + step_token_length - 1\n        \n        if step_end_token_idx < step_start_token_idx:\n            current_position = step_start + len(step_text_clean)\n            continue\n        \n        if step_end_token_idx >= total_hidden_length:\n            step_end_token_idx = total_hidden_length - 1\n        \n        actual_hidden_idx = prompt_length + step_end_token_idx\n        if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n            actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n        step_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n        \n        step_data.append({\n            'step_number': step_num,\n            'step_text': step_text,\n            'hidden_state': step_hidden,\n            'layer_idx': layer_idx, \n            'token_start': step_start_token_idx,\n            'token_end': step_end_token_idx,\n            'token_length': step_token_length,\n            'text_position': step_start\n        })\n        \n        current_position = step_start + len(step_text_clean)\n    \n    if not step_data and len(generated_ids) > 0:\n        actual_hidden_idx = prompt_length + len(generated_ids) - 1\n        if actual_hidden_idx >= len(outputs.hidden_states[-1]):\n            actual_hidden_idx = len(outputs.hidden_states[-1]) - 1\n        last_hidden = outputs.hidden_states[-1][actual_hidden_idx][0, -1, :].cpu().numpy()\n        step_data.append({\n            'step_number': 'step_1',\n            'step_text': generated_text,\n            'hidden_state': last_hidden,\n            'layer_idx': layer_idx, \n            'token_start': 0,\n            'token_end': len(generated_ids) - 1,\n            'token_length': len(generated_ids),\n            'text_position': 0\n        })\n    \n    return step_data\n\ndef parse_step_structure(text):\n    steps = {}\n    step_patterns = [\n        r'(Step\\s*\\d+\\s*:)(.*?)(?=Step\\s*\\d+\\s*:|Final Answer:|$)',\n        r'(\\d+\\.\\s*)(.*?)(?=\\d+\\.\\s*|Final Answer:|$)',\n        r'(\\d+\\)\\s*)(.*?)(?=\\d+\\)\\s*|Final Answer:|$)',\n        r'(Final Answer:\\s*)(.*?)(?=$)'\n    ]\n    \n    all_matches = []\n    for pattern in step_patterns:\n        matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)\n        for match in matches:\n            header, content = match[0], match[1]\n            start_pos = text.find(header)\n            if start_pos != -1:\n                all_matches.append((start_pos, header.strip(), content.strip()))\n    \n    all_matches.sort(key=lambda x: x[0])\n    \n    for i, (pos, header, content) in enumerate(all_matches):\n        if not content:\n            continue\n        if \"final\" in header.lower() or \"answer\" in header.lower():\n            step_key = \"final\"\n        else:\n            step_key = f\"step_{i+1}\"\n        step_text = f\"{header} {content}\".strip()\n        steps[step_key] = step_text\n    \n    if len(steps) <= 1:\n        steps = heuristic_step_splitting(text)\n    \n    return steps\n\ndef heuristic_step_splitting(text):\n    steps = {}\n    lines = text.split('\\n')\n    step_count = 1\n    \n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        if (re.match(r'^\\d+\\.', line) or \n            re.match(r'^Step\\s*\\d+', line, re.I) or\n            re.match(r'^\\d+\\)', line) or\n            len(line) > 20 and any(keyword in line.lower() for keyword in ['first', 'then', 'next', 'after', 'finally'])):\n            if 'final' not in line.lower() and 'answer' not in line.lower():\n                steps[f'step_{step_count}'] = line\n                step_count += 1\n            else:\n                steps['final'] = line\n    \n    if len(steps) <= 1:\n        sentences = re.split(r'[.!?]+', text)\n        for sentence in sentences:\n            sentence = sentence.strip()\n            if len(sentence) > 15 and not sentence.startswith(('So', 'Therefore', 'Thus')):\n                if step_count <= 4:\n                    steps[f'step_{step_count}'] = sentence\n                    step_count += 1\n    \n    return steps\n\n# =====================================================\n# è·³æ­¥è¯†åˆ«å‡½æ•°ï¼ˆä¼˜åŒ–é€»è¾‘è‡ªæ´½æ€§ï¼‰\n# =====================================================\ndef identify_skipped_steps(step_by_step_response, problem, expected_complete_reasoning):\n    judge_prompt = build_llama3_prompt(\n        \"You are a strict but reasonable reasoning analyzer. Focus on comparing actual reasoning with expected core logic step by step. Missing any expected core logic = skipped step. Wording differences are acceptable if core logic is fully included.\",\n        f\"\"\"Problem: {problem}\n\nExpected Complete Reasoning (EACH STEP'S CORE LOGIC IS MANDATORY):\n{chr(10).join([f\"Expected Step {i+1}: {step.strip()}\" for i, step in enumerate(expected_complete_reasoning)])}\n\nActual Reasoning:\n{step_by_step_response}\n\nStrict Rules:\n1. First, count how many complete reasoning steps the actual response has (excluding final answer).\n2. For each expected step (1,2,...n), check if its core logic is present in ANY of the actual steps.\n3. If an expected step's core logic is not found in any actual step â†’ mark as skipped.\n4. If the actual response has steps but misses expected core logic â†’ list the skipped expected step numbers.\n5. If the actual response has NO reasoning steps (only final answer) â†’ mark all expected steps as skipped.\n6. For incomplete problems (actual reasoning asks for more info), mark SKIPPED_STEPS as \"incomplete_problem\".\n7. Do NOT mark a step as skipped if its core logic is included in another actual step.\n\nMust output in this EXACT format (no extra words):\nSKIPPED_STEPS: [step numbers separated by commas / \"incomplete_problem\" / \"none\" only if 100% complete]\nSKIPPED_CONTENT: [missing core logic (list expected steps' core content) / \"incomplete_problem\" / \"none\"]\nSKIPPING_LOCATION: [between step X and step Y / \"start\" (if first step skipped) / \"end\" / \"incomplete_problem\" / \"none\"]\"\"\"\n    )\n    \n    inputs = tokenizer(judge_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            num_return_sequences=1,\n            do_sample=False,\n            return_dict_in_generate=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    judge_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    judge_analysis = judge_response.split(\"assistant\")[-1].strip() if \"assistant\" in judge_response else judge_response\n    skipping_info = parse_skipping_analysis(judge_analysis)\n    return skipping_info, judge_analysis\n\ndef parse_skipping_analysis(analysis_text):\n    skipping_info = {\n        'skipped_steps': [],\n        'skipped_content': '',\n        'skipping_location': '',\n        'from_step': None,\n        'to_step': None\n    }\n    \n    steps_match = re.search(r'SKIPPED_STEPS:\\s*([^\\n]+)', analysis_text)\n    if steps_match:\n        steps_str = steps_match.group(1).strip()\n        if steps_str != \"none\" and steps_str != \"incomplete_problem\":\n            step_numbers = re.findall(r'\\d+', steps_str)\n            skipping_info['skipped_steps'] = [f\"step_{num}\" for num in step_numbers]\n    \n    content_match = re.search(r'SKIPPED_CONTENT:\\s*([^\\n]+)', analysis_text)\n    if content_match:\n        skipping_info['skipped_content'] = content_match.group(1).strip()\n    \n    location_match = re.search(r'SKIPPING_LOCATION:\\s*([^\\n]+)', analysis_text)\n    if location_match:\n        location_str = location_match.group(1).strip()\n        skipping_info['skipping_location'] = location_str\n        step_matches = re.findall(r'step\\s*(\\d+)', location_str.lower())\n        if len(step_matches) >= 2:\n            skipping_info['from_step'] = f\"step_{step_matches[0]}\"\n            skipping_info['to_step'] = f\"step_{step_matches[1]}\"\n    \n    return skipping_info\n\n# =====================================================\n# é‡æ„çš„è·¯å¾„ç”Ÿæˆå‡½æ•°ï¼ˆä¼˜åŒ–æ ¼å¼çº¦æŸ+æ— æ•ˆå†…å®¹è¿‡æ»¤ï¼‰\n# =====================================================\ndef generate_explicit_reference_paths(skipping_info, step_data, problem, tokenizer, model, outputs, prompt_length):\n    \"\"\"\n    æŒ‰ç…§æ–¹æ³•ä¸€è¦æ±‚ï¼šæ„å»ºæ˜¾å¼å‚è€ƒè·¯å¾„\n    å…³é”®ä¼˜åŒ–ï¼šå¼ºåˆ¶è¾“å‡ºæ ¼å¼ã€è¿‡æ»¤æ— æ•ˆå†…å®¹ã€ç¡®ä¿ä¸Šä¸‹æ–‡ä¸€è‡´\n    \"\"\"\n    # è·å–å½“å‰è·³æ­¥ä¿¡æ¯\n    skipped_step_key = skipping_info['skipped_steps'][0] if skipping_info['skipped_steps'] else None\n    if not skipped_step_key or not skipped_step_key.startswith('step_'):\n        return {\n            \"status\": \"error\", \n            \"error\": \"æ— æœ‰æ•ˆè·³æ­¥æ­¥éª¤\",\n            \"similarities\": {},\n            \"paths\": {}\n        }\n    \n    skipped_num = int(skipped_step_key.split('_')[1])\n    \n    # è·å–å®é™…è·³æ­¥çš„hidden state\n    actual_skip_hidden, _ = get_actual_skip_hidden_state(\n        skipped_step_key, step_data, skipping_info, outputs, prompt_length\n    )\n    \n    if actual_skip_hidden is None:\n        return {\n            \"status\": \"error\", \n            \"error\": \"æ— æ³•è·å–å®é™…è·³æ­¥çš„hidden state\",\n            \"similarities\": {},\n            \"paths\": {}\n        }\n    \n    # æ„å»ºå®Œæ•´çš„æ¨ç†å†å²ï¼ˆå‰ç½®æ¡ä»¶Xpï¼‰\n    # æ”¶é›†è·³æ­¥ä¹‹å‰çš„æ‰€æœ‰æ­¥éª¤\n    previous_steps = []\n    for step in step_data:\n        step_num = int(step['step_number'].split('_')[1]) if step['step_number'].startswith('step_') else 0\n        if step_num < skipped_num:\n            previous_steps.append(step['step_text'].strip())\n    \n    # æ„å»ºå®Œæ•´çš„å‰ç½®æ¡ä»¶Xpï¼ˆç¡®ä¿ä¸Šä¸‹æ–‡æ˜ç¡®ï¼‰\n    if previous_steps:\n        Xp = f\"Problem: {problem}\\n\\nPrevious reasoning steps (already completed):\\n\" + \"\\n\".join([f\"{step}\" for step in previous_steps])\n    else:\n        Xp = f\"Problem: {problem}\\n\\nNo previous reasoning steps yet. You need to generate the first reasoning step.\"\n    \n    # è·å–ç¼ºå¤±çš„æ ¸å¿ƒå†…å®¹ï¼ˆç¡®ä¿å¼•å¯¼ç²¾å‡†ï¼‰\n    skipped_content = skipping_info['skipped_content'] if skipping_info['skipped_content'] != 'none' else \"complete reasoning for this step\"\n    \n    # æ„å»ºä¸åŒçš„å¼•å¯¼å¼æç¤ºè¯Q'ï¼ˆå¼ºåˆ¶æ ¼å¼+å†…å®¹çº¦æŸï¼‰\n    guided_prompts = {\n        \"path_detailed_calculations\": f\"\"\"{Xp}\n\nNow you must generate Step {skipped_num} (the missing step).\nMissing core content to include: {skipped_content}\n\nREQUIREMENTS (MUST FOLLOW EXACTLY):\n1. ONLY output Step {skipped_num} content. NO greetings, explanations, or extra text.\n2. Format: \"Step {skipped_num}: [detailed reasoning with calculations/intermediate steps]\"\n3. Must be relevant to the problem. NEVER say \"missing context\" or \"can't help\".\n4. Keep it concise (1-3 sentences), focus on explicit calculations/quantitative reasoning.\n\nExample Output (correct format):\nStep {skipped_num}: Calculate the distance by speed Ã— time (5km/h Ã— 2h = 10km). Then confirm it matches the required route length.\n\"\"\",\n        \n        \"path_logical_deduction\": f\"\"\"{Xp}\n\nNow you must generate Step {skipped_num} (the missing step).\nMissing core content to include: {skipped_content}\n\nREQUIREMENTS (MUST FOLLOW EXACTLY):\n1. ONLY output Step {skipped_num} content. NO extra explanations or greetings.\n2. Format: \"Step {skipped_num}: [logical deduction with if-then reasoning]\"\n3. Must connect to the problem/previous steps. NEVER mention missing context.\n4. Focus on logical connections (e.g., cause-effect, necessary conditions), 1-3 sentences.\n\nExample Output (correct format):\nStep {skipped_num}: Since the building's east side is the entry point, and the question asks for other directions, we can eliminate east and focus on north/south/west.\n\"\"\",\n        \n        \"path_conceptual_analysis\": f\"\"\"{Xp}\n\nNow you must generate Step {skipped_num} (the missing step).\nMissing core content to include: {skipped_content}\n\nREQUIREMENTS (MUST FOLLOW EXACTLY):\n1. ONLY output Step {skipped_num} content. NO extra text or greetings.\n2. Format: \"Step {skipped_num}: [conceptual analysis with underlying principles]\"\n3. Must relate to the problem's core concepts. NEVER say \"incomplete context\".\n4. Concise (1-3 sentences), explain the principle guiding this reasoning step.\n\nExample Output (correct format):\nStep {skipped_num}: This step relies on the principle of direction eliminationâ€”removing known directions (east) leaves the remaining possible directions to analyze.\n\"\"\"\n    }\n    \n    results = {\n        \"status\": \"ok\", \n        \"actual_skip_hidden_shape\": actual_skip_hidden.shape,\n        \"similarities\": {}, \n        \"paths\": {},\n        \"generated_hidden_states\": {},\n        \"variance_analysis\": {}\n    }\n    \n    # ä¸ºæ¯æ¡è·¯å¾„ç”Ÿæˆ3ä¸ªæ ·æœ¬ï¼ˆN-best generationsï¼‰\n    for path_name, guided_prompt in guided_prompts.items():\n        individual_sims = []\n        valid_path_texts = []\n        valid_generated_hiddens = []\n        \n        for sample_num in range(3):  # 3æ¬¡é‡‡æ ·ï¼Œä¿è¯ç¨³å®šæ€§\n            full_prompt = build_llama3_prompt(\n                \"You are a precise reasoning assistant. Generate only the required step following the exact format and requirements. Do NOT add any extra content.\",\n                guided_prompt\n            )\n            \n            inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n            with torch.no_grad():\n                path_outputs = model.generate(\n                    **inputs,\n                    max_new_tokens=100,  # é™åˆ¶é•¿åº¦ï¼Œé¿å…å†—ä½™\n                    do_sample=True,\n                    temperature=0.6,  # å¹³è¡¡éšæœºæ€§å’Œä¸€è‡´æ€§\n                    top_p=0.9,  # æå‡ç”Ÿæˆè´¨é‡\n                    output_hidden_states=True,\n                    return_dict_in_generate=True,\n                    pad_token_id=tokenizer.eos_token_id,\n                    eos_token_id=tokenizer.eos_token_id\n                )\n            \n            # è§£ç å¹¶è¿‡æ»¤æ— æ•ˆå†…å®¹\n            generated_ids = path_outputs.sequences[0, len(inputs.input_ids[0]):]\n            raw_text = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n            \n            # ä¸¥æ ¼è¿‡æ»¤ï¼šåªä¿ç•™ç¬¦åˆæ ¼å¼çš„Step Nå†…å®¹\n            filtered_text = \"\"\n            if f\"Step {skipped_num}:\" in raw_text:\n                # æå–Step Nå¼€å¤´çš„å†…å®¹ï¼Œç›´åˆ°æ¢è¡Œæˆ–ç»“æŸ\n                step_start = raw_text.find(f\"Step {skipped_num}:\")\n                step_end = raw_text.find(\"\\n\", step_start) if \"\\n\" in raw_text[step_start:] else len(raw_text)\n                filtered_text = raw_text[step_start:step_end].strip()\n                # ç§»é™¤å¯èƒ½çš„å¤šä½™æ ‡è®°ï¼ˆå¦‚æ¨¡å‹æ·»åŠ çš„åºå·ï¼‰\n                filtered_text = re.sub(r'^Step\\s*\\d+:\\s*', f'Step {skipped_num}: ', filtered_text)\n            \n            # ä»…ä¿ç•™æœ‰æ•ˆæ–‡æœ¬ï¼ˆéç©ºä¸”ç¬¦åˆæ ¼å¼ï¼‰\n            if filtered_text and filtered_text.startswith(f\"Step {skipped_num}:\"):\n                valid_path_texts.append(filtered_text)\n                \n                # æå–è¯¥æ ·æœ¬çš„hidden stateï¼ˆå–ç”Ÿæˆåºåˆ—çš„æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€ï¼‰\n                if hasattr(path_outputs, 'hidden_states') and path_outputs.hidden_states:\n                    # ä¿®æ­£ï¼šLLaMA3çš„hidden_statesç»“æ„æ˜¯ tuple(ç”Ÿæˆæ­¥éª¤) â†’ tuple(æ¨¡å‹å±‚) â†’ tensor\n                    # 1. å–æœ€åä¸€æ­¥ç”Ÿæˆçš„éšè—çŠ¶æ€ï¼ˆtuple(æ¨¡å‹å±‚)ï¼‰\n                    last_step_hiddens = path_outputs.hidden_states[-1]  # tuple: (layer_0, layer_1, ..., layer_n)\n                    # 2. å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€ï¼ˆtensor: (batch, seq_len, hidden_size)ï¼‰\n                    if isinstance(last_step_hiddens, tuple) and len(last_step_hiddens) > 0:\n                        last_layer_hiddens = last_step_hiddens[-1]  # æœ€åä¸€å±‚çš„tensor\n                        # 3. éªŒè¯tensorå½¢çŠ¶ï¼Œå–æœ€åä¸€ä¸ªtokençš„éšè—çŠ¶æ€\n                        if last_layer_hiddens.ndim == 3 and last_layer_hiddens.shape[1] > 0:\n                            path_hidden = last_layer_hiddens[0, -1, :].cpu().numpy()  # (hidden_size,)\n                            valid_generated_hiddens.append(path_hidden)\n            else:\n                print(f\"âš ï¸  è·¯å¾„ {path_name} æ ·æœ¬ {sample_num+1} ç”Ÿæˆæ— æ•ˆå†…å®¹ï¼Œå·²è¿‡æ»¤ï¼š{raw_text[:50]}...\")\n        \n        # å¤„ç†æ— æœ‰æ•ˆæ ·æœ¬çš„æƒ…å†µ\n        if not valid_generated_hiddens:\n            results[\"similarities\"][path_name] = {\n                \"individual\": [0.0, 0.0, 0.0],\n                \"mean\": 0.0,\n                \"std\": 0.0,\n                \"variance\": 0.0\n            }\n            results[\"paths\"][path_name] = [\"æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹\"]\n            results[\"generated_hidden_states\"][path_name] = [None, None, None]\n            continue\n        \n        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆå®é™…è·³æ­¥ vs æœ‰æ•ˆç”Ÿæˆè·¯å¾„ï¼‰\n        sims = calculate_similarity_between_paths(actual_skip_hidden, valid_generated_hiddens)\n        # è¡¥å……ç©ºå€¼ä»¥ä¿è¯3ä¸ªæ ·æœ¬çš„ç»“æ„ä¸€è‡´æ€§\n        while len(sims) < 3:\n            sims.append(0.0)\n        \n        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n        mean_sim = float(np.mean(sims[:3]))  # å–å‰3ä¸ªï¼ˆæœ€å¤š3ä¸ªï¼‰\n        std_sim = float(np.std(sims[:3]))\n        var_sim = float(np.var(sims[:3]))\n        \n        results[\"similarities\"][path_name] = {\n            \"individual\": sims[:3],  # ç¡®ä¿é•¿åº¦ä¸º3\n            \"mean\": mean_sim,\n            \"std\": std_sim,\n            \"variance\": var_sim\n        }\n        results[\"paths\"][path_name] = valid_path_texts + [\"æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹\"] * (3 - len(valid_path_texts))  # è¡¥å…¨3ä¸ª\n        results[\"generated_hidden_states\"][path_name] = valid_generated_hiddens + [None] * (3 - len(valid_generated_hiddens))  # è¡¥å…¨3ä¸ª\n    \n    # è®¡ç®—è·¨è·¯å¾„çš„æ–¹å·®åˆ†æï¼ˆæ‰€æœ‰è·¯å¾„çš„å¹³å‡ç›¸ä¼¼åº¦çš„æ–¹å·®ï¼‰\n    all_path_means = []\n    for path_name in guided_prompts.keys():\n        path_mean = results[\"similarities\"][path_name][\"mean\"]\n        all_path_means.append(path_mean)\n    \n    cross_path_variance = float(np.var(all_path_means)) if len(all_path_means) > 1 else 0.0\n    results[\"variance_analysis\"][\"cross_path_variance\"] = cross_path_variance\n    results[\"variance_analysis\"][\"path_means\"] = all_path_means\n    results[\"variance_analysis\"][\"path_names\"] = list(guided_prompts.keys())\n    \n    return results\n\ndef print_detailed_similarity_analysis(skip_step, similarity_results, paths):\n    \"\"\"\n    æ‰“å°è¯¦ç»†çš„ç›¸ä¼¼åº¦åˆ†æï¼ŒåŒ…æ‹¬æ¯æ¬¡ç›¸ä¼¼åº¦å’Œæ–¹å·®åˆ†æ\n    \"\"\"\n    print(f\"\\nğŸ” ã€è·³æ­¥ {skip_step} è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\")\n    print(\"-\" * 60)\n    \n    # æ‰“å°å„è·¯å¾„ç›¸ä¼¼åº¦\n    for path_name, sim_data in similarity_results.items():\n        individual_sims = sim_data[\"individual\"]\n        mean_sim = sim_data[\"mean\"]\n        variance = sim_data[\"variance\"]\n        \n        print(f\"\\nğŸ“Š {path_name.replace('_', ' ').title()}:\")\n        print(f\"   å•æ¬¡ç›¸ä¼¼åº¦: {[f'{sim:.4f}' for sim in individual_sims]}\")\n        print(f\"   å¹³å‡ç›¸ä¼¼åº¦: {mean_sim:.4f}\")\n        print(f\"   æ ·æœ¬å†…æ–¹å·®: {variance:.6f}\")\n        \n        # æ˜¾ç¤ºå¯¹åº”çš„è·¯å¾„æ–‡æœ¬ï¼ˆåªæ˜¾ç¤ºæœ‰æ•ˆå†…å®¹ï¼‰\n        path_texts = paths[path_name]\n        for i, text in enumerate(path_texts):\n            if text != \"æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹\":\n                print(f\"   æ ·æœ¬{i+1}: {text}\")\n            else:\n                print(f\"   æ ·æœ¬{i+1}: {text}\")\n    \n    # è·¨è·¯å¾„æ–¹å·®åˆ†æ\n    cross_variance = similarity_results.get(\"variance_analysis\", {}).get(\"cross_path_variance\", 0.0)\n    path_means = similarity_results.get(\"variance_analysis\", {}).get(\"path_means\", [])\n    path_names = similarity_results.get(\"variance_analysis\", {}).get(\"path_names\", [])\n    \n    print(f\"\\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\")\n    print(f\"   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {dict(zip(path_names, [f'{m:.4f}' for m in path_means]))}\")\n    print(f\"   è·¨è·¯å¾„æ–¹å·®: {cross_variance:.6f}\")\n    \n    # æ–¹å·®è§£è¯»ï¼ˆç»“åˆå®é™…åœºæ™¯ä¼˜åŒ–ï¼‰\n    if cross_variance < 0.005:\n        print(\"   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\")\n    elif 0.005 <= cross_variance < 0.02:\n        print(\"   ğŸ“ è§£è¯»: ä½æ–¹å·® - è·³æ­¥çŠ¶æ€å¯¹å¤šæ¡è·¯å¾„æœ‰å¼±åå¥½ï¼Œæ— æ˜æ˜¾å€¾å‘æ€§\")\n    elif 0.02 <= cross_variance < 0.05:\n        print(\"   ğŸ“ è§£è¯»: ä¸­ç­‰æ–¹å·® - è·³æ­¥çŠ¶æ€å¯¹æŸæ¡è·¯å¾„æœ‰æ˜æ˜¾åå¥½ï¼Œæ”¯æŒæ½œåœ¨æ¨ç†è·¯å¾„å‡è®¾\")\n    else:\n        print(\"   ğŸ“ è§£è¯»: é«˜æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸ç‰¹å®šè·¯å¾„é«˜åº¦å»åˆï¼Œå¼ºçƒˆæ”¯æŒæ½œåœ¨æ¨ç†è·¯å¾„å‡è®¾\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:12:22.753229Z","iopub.execute_input":"2025-11-10T12:12:22.753887Z","iopub.status.idle":"2025-11-10T12:12:22.815795Z","shell.execute_reply.started":"2025-11-10T12:12:22.753866Z","shell.execute_reply":"2025-11-10T12:12:22.815043Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =====================================================\n# å•ä¸ªé—®é¢˜å®éªŒæµç¨‹\n# =====================================================\ndef main_experiment_with_save(test_sample):\n    \"\"\"æŒ‰ç…§æ–¹æ³•ä¸€è¦æ±‚ä¿®æ”¹çš„å®éªŒæµç¨‹\"\"\"\n    test_id = test_sample['id']\n    problem = test_sample['problem']\n    expected_steps = test_sample['expected_complete_reasoning']\n    \n    # ç”Ÿæˆåˆ†æ­¥æ¨ç†ï¼ˆå…³é”®ï¼šåœ¨generateä¸­è®¾ç½®output_hidden_states=Trueï¼‰\n    step_by_step_prompt = build_llama3_prompt(\n        \"You solve reasoning problems in a FIXED FORMAT. Follow EXACTLY: 1. Start with 'Step 1: ...' 2. Each step has only core logic 3. End with 'Final Answer: ...' 4. No extra greetings/explanations 5. Do NOT ask for more information (try your best if problem is simple)\",\n        f\"Problem: {problem}\\n\\nSolve step by step in the FIXED FORMAT (only core logic, no extra words):\"\n    )\n    inputs = tokenizer(step_by_step_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).to(model.device)\n    with torch.no_grad():\n        # å…³é”®ä¿®æ­£ï¼šåœ¨generateä¸­è®¾ç½®output_hidden_states=True\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=500,\n            num_return_sequences=1,\n            do_sample=True,\n            output_hidden_states=True,  # æ­£ç¡®çš„ä½ç½®\n            return_dict_in_generate=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    full_response = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n    step_by_step_response = full_response.split(\"assistant\")[-1].strip() if \"assistant\" in full_response else full_response\n    \n    # æå–æ­¥éª¤çº§hidden states\n    prompt_length = inputs.input_ids.shape[1]\n    step_data = extract_step_level_hidden_states(outputs, prompt_length, step_by_step_response)\n    \n    # è¯†åˆ«è·³æ­¥ä½ç½®\n    skipping_info, judge_analysis = identify_skipped_steps(step_by_step_response, problem, expected_steps)\n    \n    # ä¸ºæ¯ä¸ªè·³æ­¥ç”Ÿæˆæ˜¾å¼å‚è€ƒè·¯å¾„å¹¶è®¡ç®—ç›¸ä¼¼åº¦\n    all_skip_results = []\n    if skipping_info['skipped_steps']:\n        for skip_step in skipping_info['skipped_steps']:\n            # å…³é”®ä¿®æ”¹ï¼šç›´æ¥ä¼ é€’å®Œæ•´çš„skipping_infoï¼Œä¸å†æ„å»ºsingle_skip_info\n            comparison_results = generate_explicit_reference_paths(\n                skipping_info,  # ç›´æ¥ä¼ é€’å®Œæ•´çš„è·³æ­¥ä¿¡æ¯\n                step_data, \n                problem,\n                tokenizer, \n                model,\n                outputs,\n                prompt_length\n            )\n            all_skip_results.append({\n                'skip_step': skip_step,\n                'similarity_results': comparison_results.get('similarities', {}),\n                'variance_analysis': comparison_results.get('variance_analysis', {}),\n                'paths': comparison_results.get('paths', {}),\n                'actual_hidden_shape': comparison_results.get('actual_skip_hidden_shape', 'unknown'),\n                'status': comparison_results.get('status', 'unknown')\n            })\n    \n    # æ„å»ºå®Œæ•´ç»“æœ\n    result = {\n        'test_id': test_id,\n        'problem': problem,\n        'step_by_step_response': step_by_step_response,\n        'judge_analysis': judge_analysis,\n        'skipping_info': skipping_info,\n        'all_skip_results': all_skip_results,\n        'step_data': step_data,\n        'prompt_length': prompt_length\n    }\n    \n    # ä¿å­˜æ•°æ®\n    save_experiment1_data(result)\n    \n    return result\n\n# =====================================================\n# ä¿®æ­£çš„è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æå‡½æ•°\n# =====================================================\ndef print_detailed_similarity_analysis(skip_step, skip_result):\n    \"\"\"\n    ä¿®æ­£ç‰ˆï¼šæ‰“å°è¯¦ç»†çš„ç›¸ä¼¼åº¦åˆ†æï¼ŒåŒ…æ‹¬æ¯æ¬¡ç›¸ä¼¼åº¦å’Œæ–¹å·®åˆ†æ\n    å…³é”®ä¿®å¤ï¼šæ­£ç¡®è·å–è·¨è·¯å¾„æ–¹å·®æ•°æ®\n    \"\"\"\n    similarity_results = skip_result.get('similarity_results', {})\n    paths = skip_result.get('paths', {})\n    variance_analysis = skip_result.get('variance_analysis', {})\n    \n    print(f\"\\nğŸ” ã€è·³æ­¥ {skip_step} è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\")\n    print(\"-\" * 60)\n    \n    # æ‰“å°å„è·¯å¾„ç›¸ä¼¼åº¦\n    for path_name, sim_data in similarity_results.items():\n        individual_sims = sim_data[\"individual\"]\n        mean_sim = sim_data[\"mean\"]\n        variance = sim_data[\"variance\"]\n        \n        print(f\"\\nğŸ“Š {path_name.replace('_', ' ').title()}:\")\n        print(f\"   å•æ¬¡ç›¸ä¼¼åº¦: {[f'{sim:.4f}' for sim in individual_sims]}\")\n        print(f\"   å¹³å‡ç›¸ä¼¼åº¦: {mean_sim:.4f}\")\n        print(f\"   æ ·æœ¬å†…æ–¹å·®: {variance:.6f}\")\n        \n        # æ˜¾ç¤ºå¯¹åº”çš„è·¯å¾„æ–‡æœ¬ï¼ˆåªæ˜¾ç¤ºæœ‰æ•ˆå†…å®¹ï¼‰\n        path_texts = paths.get(path_name, [])\n        for i, text in enumerate(path_texts):\n            if text != \"æ— æœ‰æ•ˆç”Ÿæˆå†…å®¹\":\n                print(f\"   æ ·æœ¬{i+1}: {text}\")\n            else:\n                print(f\"   æ ·æœ¬{i+1}: {text}\")\n    \n    # è·¨è·¯å¾„æ–¹å·®åˆ†æ - å…³é”®ä¿®å¤ï¼šç›´æ¥ä»variance_analysisè·å–\n    cross_variance = variance_analysis.get(\"cross_path_variance\", 0.0)\n    path_means = variance_analysis.get(\"path_means\", [])\n    path_names = variance_analysis.get(\"path_names\", [])\n    \n    print(f\"\\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\")\n    path_mean_dict = dict(zip(path_names, [f'{m:.4f}' for m in path_means]))\n    print(f\"   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {path_mean_dict}\")\n    print(f\"   è·¨è·¯å¾„æ–¹å·®: {cross_variance:.6f}\")\n    \n    # æ–¹å·®è§£è¯»ï¼ˆç»“åˆå®é™…åœºæ™¯ä¼˜åŒ–ï¼‰\n    if cross_variance < 0.005:\n        print(\"   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\")\n    elif 0.005 <= cross_variance < 0.02:\n        print(\"   ğŸ“ è§£è¯»: ä½æ–¹å·® - è·³æ­¥çŠ¶æ€å¯¹å¤šæ¡è·¯å¾„æœ‰å¼±åå¥½ï¼Œæ— æ˜æ˜¾å€¾å‘æ€§\")\n    elif 0.02 <= cross_variance < 0.05:\n        print(\"   ğŸ“ è§£è¯»: ä¸­ç­‰æ–¹å·® - è·³æ­¥çŠ¶æ€å¯¹æŸæ¡è·¯å¾„æœ‰æ˜æ˜¾åå¥½ï¼Œæ”¯æŒæ½œåœ¨æ¨ç†è·¯å¾„å‡è®¾\")\n    else:\n        print(\"   ğŸ“ è§£è¯»: é«˜æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸ç‰¹å®šè·¯å¾„é«˜åº¦å»åˆï¼Œå¼ºçƒˆæ”¯æŒæ½œåœ¨æ¨ç†è·¯å¾„å‡è®¾\")\n\n# =====================================================\n# æ‰¹é‡è¿è¡Œæµ‹è¯•é›†ï¼ˆæ›´æ–°æ˜¾ç¤ºé€»è¾‘ï¼‰\n# =====================================================\ndef run_experiment1_with_save():\n    \"\"\"è¿è¡Œå®éªŒä¸€å¹¶ä¿å­˜æ‰€æœ‰æ•°æ®\"\"\"\n    testset = load_testset(\"/kaggle/input/testset/testset.csv\")\n    \n    print(\"=\" * 70)\n    print(f\"å®éªŒä¸€ï¼šåŸºäºéšè—çŠ¶æ€ç›¸ä¼¼æ€§çš„è·¯å¾„åŒ¹é…åˆ†æ - å…± {len(testset)} ä¸ªé—®é¢˜\")\n    print(\"=\" * 70)\n    \n    # è·¯å¾„æè¿°æ˜ å°„\n    path_descriptions = {\n        \"path_detailed_calculations\": \"è¯¦ç»†è®¡ç®—è·¯å¾„\",\n        \"path_logical_deduction\": \"é€»è¾‘æ¨ç†è·¯å¾„\", \n        \"path_conceptual_analysis\": \"æ¦‚å¿µåˆ†æè·¯å¾„\"\n    }\n    \n    all_results = []\n    for i, test_sample in enumerate(testset):\n        print(f\"\\nå¤„ç†è¿›åº¦: {i+1}/{len(testset)}\")\n        result = main_experiment_with_save(test_sample)\n        all_results.append(result)\n        \n        test_id = result['test_id']\n        skipping_info = result['skipping_info']\n        all_skip_results = result['all_skip_results']\n        step_by_step_response = result['step_by_step_response']\n        judge_analysis = result['judge_analysis']\n        \n        print(f\"\\nã€{test_id} éªŒè¯ç»“æœã€‘\")\n        print(f\"æ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\\n{step_by_step_response[:300]}...\")\n        print(f\"\\nLLMè·³æ­¥åˆ†æï¼š\\n{judge_analysis}\")\n        \n        # è¾“å‡ºæ‰€æœ‰è·³æ­¥çš„è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æ\n        if all_skip_results:\n            for skip_result in all_skip_results:\n                skip_step = skip_result['skip_step']\n                similarities = skip_result['similarity_results']\n                \n                # å…³é”®ä¿®å¤ï¼šä¼ é€’å®Œæ•´çš„skip_resultè€Œä¸æ˜¯åˆ†å¼€çš„å‚æ•°\n                print_detailed_similarity_analysis(skip_step, skip_result)\n                \n                # æ‰¾å‡ºæœ€ä½³åŒ¹é…è·¯å¾„\n                if similarities:\n                    best_path = max(similarities.items(), key=lambda x: x[1]['mean'])[0]\n                    best_similarity = similarities[best_path]['mean']\n                    best_path_desc = path_descriptions.get(best_path, best_path)\n                    print(f\"\\nğŸ¯ {test_id} {skip_step} æœ€ä½³åŒ¹é…: {best_path_desc} (ç›¸ä¼¼åº¦: {best_similarity:.4f})\")\n        else:\n            print(f\"\\n{test_id}: æ— è·³æ­¥ï¼ˆæ ¹æ®Judgeåˆ†æï¼‰\")\n    \n    # æ±‡æ€»æ•°æ®\n    summarize_experiment1_data()\n    \n    # ä¿å­˜å®Œæ•´æ•°æ®é›†\n    if save_complete_experiment1_data():\n        print(f\"\\nğŸ“Š å®éªŒä¸€æ•°æ®ç»Ÿè®¡:\")\n        print(f\"  æ€»æµ‹è¯•æ¡ˆä¾‹: {len(experiment1_data)}\")\n        total_steps = sum(len(data['step_data']) for data in experiment1_data.values())\n        total_skips = sum(len(data['skipping_info']['skipped_steps']) for data in experiment1_data.values())\n        print(f\"  æ€»æ¨ç†æ­¥éª¤: {total_steps}\")\n        print(f\"  æ€»è·³æ­¥æ•°é‡: {total_skips}\")\n        \n        # æ·»åŠ ç›¸ä¼¼åº¦ç»Ÿè®¡\n        valid_comparisons = 0\n        total_similarity = 0.0\n        variance_data = []\n        \n        for test_id, data in experiment1_data.items():\n            if data.get('all_skip_results'):\n                for skip_result in data['all_skip_results']:\n                    similarities = skip_result.get('similarity_results', {})\n                    variance_analysis = skip_result.get('variance_analysis', {})\n                    \n                    # æ”¶é›†è·¨è·¯å¾„æ–¹å·®æ•°æ®\n                    cross_variance = variance_analysis.get(\"cross_path_variance\", 0.0)\n                    variance_data.append(cross_variance)\n                    \n                    for path_data in similarities.values():\n                        if path_data['mean'] > 0:\n                            valid_comparisons += 1\n                            total_similarity += path_data['mean']\n        \n        if valid_comparisons > 0:\n            avg_similarity = total_similarity / valid_comparisons\n            print(f\"  æœ‰æ•ˆå¯¹æ¯”æ¬¡æ•°: {valid_comparisons}\")\n            print(f\"  å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f}\")\n        \n        if variance_data:\n            avg_variance = np.mean(variance_data)\n            print(f\"  å¹³å‡è·¨è·¯å¾„æ–¹å·®: {avg_variance:.6f}\")\n            print(f\"  æ–¹å·®æ•°æ®èŒƒå›´: [{min(variance_data):.6f}, {max(variance_data):.6f}]\")\n    \n    print(f\"\\nğŸ‰ å®éªŒä¸€å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²ä¿å­˜\")\n    return all_results\n\n# =====================================================\n# æ‰§è¡Œå®éªŒä¸€\n# =====================================================\nif __name__ == \"__main__\":\n    # è¿è¡Œå®éªŒä¸€å¹¶ä¿å­˜æ•°æ®\n    experiment1_results = run_experiment1_with_save()\n    \n    # æ˜¾ç¤ºä¿å­˜çš„æ–‡ä»¶\n    print(\"\\nğŸ“ ä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨:\")\n    for file in os.listdir('.'):\n        if file.startswith('experiment1_') and file.endswith('.pkl'):\n            file_size = os.path.getsize(file) / 1024\n            print(f\"  {file} ({file_size:.1f} KB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-10T12:12:22.818804Z","iopub.execute_input":"2025-11-10T12:12:22.820107Z","iopub.status.idle":"2025-11-10T12:20:50.192226Z","shell.execute_reply.started":"2025-11-10T12:12:22.820081Z","shell.execute_reply":"2025-11-10T12:20:50.191482Z"}},"outputs":[{"name":"stdout","text":"âœ“ æˆåŠŸåŠ è½½ 8 ä¸ªæµ‹è¯•é—®é¢˜\n======================================================================\nå®éªŒä¸€ï¼šåŸºäºéšè—çŠ¶æ€ç›¸ä¼¼æ€§çš„è·¯å¾„åŒ¹é…åˆ†æ - å…± 8 ä¸ªé—®é¢˜\n======================================================================\n\nå¤„ç†è¿›åº¦: 1/8\n","output_type":"stream"},{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test1 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test1.pkl\n  - æ­¥éª¤æ•°é‡: 5\n  - è·³æ­¥æ•°é‡: 1\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test1 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: You are facing north.\n\nStep 2: If you walk 10 meters, you will be 10 meters north.\n\nStep 3: If you then turn 90 degrees to your right, you will be facing east.\n\nStep 4: If you walk 10 meters, you will be 10 meters east.\n\nFinal Answer: You are 10 meters east and 10 meters north of the entranc...\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: 1\nSKIPPED_CONTENT: walking west\nSKIPPING_LOCATION: start\n\nğŸ” ã€è·³æ­¥ step_1 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8102', '0.8408', '0.8048']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8186\n   æ ·æœ¬å†…æ–¹å·®: 0.000250\n   æ ·æœ¬1: Step 1: Walk 10 meters west to reach the entrance.\n   æ ·æœ¬2: Step 1: Walk west for 10 meters to reach the entrance, since you are currently 10 meters east of the entrance.\n   æ ·æœ¬3: Step 1: Since you are 10 meters east of the entrance, walk west by 10 meters to reach the entrance.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8428', '0.8742', '0.8846']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8672\n   æ ·æœ¬å†…æ–¹å·®: 0.000314\n   æ ·æœ¬1: Step 1: Since you are currently 10 meters east of the entrance, walking west would move you closer to the entrance, thus reducing the east distance by some amount.\n   æ ·æœ¬2: Step 1: Since you are 10 meters east of the entrance, walking west would mean moving 10 meters closer to the entrance.\n   æ ·æœ¬3: Step 1: Since you have already entered the building from the east side and are 10 meters east and 10 meters north of the entrance, you must walk west to change your eastward distance.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7645', '0.7742', '0.7754']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7714\n   æ ·æœ¬å†…æ–¹å·®: 0.000024\n   æ ·æœ¬1: Step 1: This step relies on the principle of direction elimination, where we eliminate the direction from which we entered the building (east) to focus on the remaining directions, allowing us to determine the next logical movement.\n   æ ·æœ¬2: Step 1: Since you have entered the building from the east side, this step relies on the principle of direction elimination, which involves removing the known direction (east) to focus on the remaining possible directions for further analysis.\n   æ ·æœ¬3: Step 1: This step applies the principle of direction modification, as walking west from the east side of the building reverses the initial direction, allowing us to reassess the position relative to the entrance.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.8186', 'path_logical_deduction': '0.8672', 'path_conceptual_analysis': '0.7714'}\n   è·¨è·¯å¾„æ–¹å·®: 0.001530\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test1 step_1 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.8672)\n\nå¤„ç†è¿›åº¦: 2/8\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test2 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test2.pkl\n  - æ­¥éª¤æ•°é‡: 4\n  - è·³æ­¥æ•°é‡: 0\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test2 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: All bachelors have never married.\nStep 2: John is a man who has never been married.\nStep 3: Since John has never been married, he is a bachelor.\nFinal Answer: Yes....\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: none\nSKIPPED_CONTENT: none\nSKIPPING_LOCATION: none\n\nTest2: æ— è·³æ­¥ï¼ˆæ ¹æ®Judgeåˆ†æï¼‰\n\nå¤„ç†è¿›åº¦: 3/8\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test3 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test3.pkl\n  - æ­¥éª¤æ•°é‡: 8\n  - è·³æ­¥æ•°é‡: 3\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test3 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: Factor the quadratic expression: xÂ² â€“ 4x + 7 = (x - 1)(x - 7)\n\nStep 2: Determine the product of the two factors: (x - 1)(x - 7) â‰¥ 0\n\nStep 3: The product of two numbers is non-negative if and only if both numbers have the same sign. So, either both factors are positive or both are negative.\n\n...\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: 2, 4, 5\nSKIPPED_CONTENT: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4 (add and subtract (4/2)Â² = 4 to complete the square), Step 4: Analyze the squared term: (x â€“ 2)Â² â‰¥ 0 for all real x (square of any real number is non-negative), Step 5: Deduce the entire expression: (x â€“ 2)Â² + 3 â‰¥ 0 + 3 = 3 > 0\nSKIPPING_LOCATION: between Step 1 and Step 2\n\nğŸ” ã€è·³æ­¥ step_2 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7584', '0.7585', '0.7689']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7620\n   æ ·æœ¬å†…æ–¹å·®: 0.000024\n   æ ·æœ¬1: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬2: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, add and subtract (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬3: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8009', '0.7862', '0.8009']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7960\n   æ ·æœ¬å†…æ–¹å·®: 0.000048\n   æ ·æœ¬1: Step 2: Since we have factored the quadratic expression as (x - 1)(x - 7), we can rewrite xÂ² - 4x as (xÂ² - 4x + 4) - 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬2: Step 2: Since xÂ² â€“ 4x + 7 can be rewritten as (xÂ² â€“ 4x + 4) â€“ 4, we add and subtract (4/2)Â² = 4 to complete the square, resulting in (x â€“ 2)Â² â€“ 4.\n   æ ·æœ¬3: Step 2: Since we have factored the quadratic expression as (x - 1)(x - 7), we can rewrite xÂ² - 4x as (xÂ² - 4x + 4) - 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7619', '0.7620', '0.7620']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7620\n   æ ·æœ¬å†…æ–¹å·®: 0.000000\n   æ ·æœ¬1: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬2: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4 by adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬3: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, adding and subtracting (4/2)Â² = 4 to complete the square.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.7620', 'path_logical_deduction': '0.7960', 'path_conceptual_analysis': '0.7620'}\n   è·¨è·¯å¾„æ–¹å·®: 0.000257\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test3 step_2 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.7960)\n\nğŸ” ã€è·³æ­¥ step_4 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7584', '0.7585', '0.7584']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7584\n   æ ·æœ¬å†…æ–¹å·®: 0.000000\n   æ ·æœ¬1: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬2: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, add and subtract (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬3: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7929', '0.8009', '0.8115']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8018\n   æ ·æœ¬å†…æ–¹å·®: 0.000058\n   æ ·æœ¬1: Step 2: Since xÂ² â€“ 4x + 7 can be rewritten as (xÂ² â€“ 4x + 4) â€“ 4, we add and subtract (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬2: Step 2: Since we have factored the quadratic expression as (x - 1)(x - 7), we can rewrite xÂ² - 4x as (xÂ² - 4x + 4) - 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬3: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, adding and subtracting (4/2)Â² = 4 to complete the square.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7657', '0.7707', '0.7620']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7661\n   æ ·æœ¬å†…æ–¹å·®: 0.000013\n   æ ·æœ¬1: Step 2: This step relies on the principle of completing the square, where we add and subtract (4/2)Â² = 4 to the quadratic expression xÂ² â€“ 4x to transform it into a perfect square, allowing us to utilize its non-negativity property.\n   æ ·æœ¬2: Step 2: This step relies on the principle of completing the square, where we add and subtract a perfect square (4) to rewrite the quadratic expression xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, which enables us to manipulate the expression into a more tractable form.\n   æ ·æœ¬3: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, adding and subtracting (4/2)Â² = 4 to complete the square.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.7584', 'path_logical_deduction': '0.8018', 'path_conceptual_analysis': '0.7661'}\n   è·¨è·¯å¾„æ–¹å·®: 0.000356\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test3 step_4 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.8018)\n\nğŸ” ã€è·³æ­¥ step_5 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7689', '0.7689', '0.7584']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7654\n   æ ·æœ¬å†…æ–¹å·®: 0.000025\n   æ ·æœ¬1: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4.\n   æ ·æœ¬2: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4.\n   æ ·æœ¬3: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8009', '0.7996', '0.8046']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8017\n   æ ·æœ¬å†…æ–¹å·®: 0.000004\n   æ ·æœ¬1: Step 2: Since we have factored the quadratic expression as (x - 1)(x - 7), we can rewrite xÂ² - 4x as (xÂ² - 4x + 4) - 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬2: Step 2: Since we have factored the quadratic expression as (x - 1)(x - 7), we can rewrite xÂ² - 4x as (xÂ² - 4x + 4) - 4 by adding and subtracting (4/2)Â² = 4 to complete the square.\n   æ ·æœ¬3: Step 2: Since we have factored the quadratic expression as (x - 1)(x - 7), we can rewrite xÂ² â€“ 4x + 7 as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4 to complete the square.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7613', '0.7637', '0.7683']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7645\n   æ ·æœ¬å†…æ–¹å·®: 0.000008\n   æ ·æœ¬1: Step 2: This step applies the principle of completing the square, which allows us to rewrite the quadratic expression xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4 by adding and subtracting (4/2)Â² = 4 to the expression.\n   æ ·æœ¬2: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4, by adding and subtracting (4/2)Â² = 4 to complete the square, utilizing the concept of algebraic manipulation to transform the expression into a more tractable form.\n   æ ·æœ¬3: Step 2: Rewrite xÂ² â€“ 4x as (xÂ² â€“ 4x + 4) â€“ 4 by adding and subtracting (4/2)Â² = 4 to complete the square, leveraging the algebraic property of commutativity and associativity of addition and subtraction.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.7654', 'path_logical_deduction': '0.8017', 'path_conceptual_analysis': '0.7645'}\n   è·¨è·¯å¾„æ–¹å·®: 0.000300\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test3 step_5 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.8017)\n\nå¤„ç†è¿›åº¦: 4/8\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test4 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test4.pkl\n  - æ­¥éª¤æ•°é‡: 10\n  - è·³æ­¥æ•°é‡: 5\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test4 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: Consider the first few triangular numbers: 1, 3, 6, 10, 15,...\n\nStep 2: Observe that each triangular number is formed by adding consecutive integers: 1 = 1, 3 = 1 + 2, 6 = 1 + 2 + 3, 10 = 1 + 2 + 3 + 4,...\n\nStep 3: Notice that the nth triangular number can be written as the sum of the first ...\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: 2, 3, 5, 6, 7\nSKIPPED_CONTENT: Expected Step 2: Write the sum in reverse, Expected Step 3: Add the two equations, Expected Step 5: Recall the identity that sum of first n cubes = (sum of first n integers)Â², Expected Step 6: Substitute sum of first n integers = Tâ‚™ = n(n+1)/2, Expected Step 7: Square the sum\nSKIPPING_LOCATION: between step 1 and step 4\n\nğŸ” ã€è·³æ­¥ step_2 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3893', '0.4313', '0.3654']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3953\n   æ ·æœ¬å†…æ–¹å·®: 0.000743\n   æ ·æœ¬1: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =? (reverse the order of the terms) =?Â³ +?Â³ +?Â³ +... + 1Â³.\n   æ ·æœ¬2: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ = (nÂ³) + (n-1)Â³ +... + 3Â³ + 2Â³ + 1Â³\n   æ ·æœ¬3: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =?... + nÂ³ + 3Â³ + 2Â³ + 1Â³.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3633', '0.3703', '0.3699']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3678\n   æ ·æœ¬å†…æ–¹å·®: 0.000010\n   æ ·æœ¬1: Step 2: Write the sum in reverse as Î£(nÂ³) = Î£(n+1)Â³ - 1Â³, then simplify the right side to get Î£(n+1)Â³ - Î£(1Â³).\n   æ ·æœ¬2: Step 2: The sum of the first n triangular numbers can be written as the sum of the sum of the first n integers multiplied by the first n integers, since each triangular number is the sum of consecutive integers.\n   æ ·æœ¬3: Step 2: The sum of the first n triangular numbers can be written as the sum of the first n terms of the arithmetic sequence 1, 3, 6, 10,... in reverse order, which is 1 + 3 + 6 +... + n(n-1).\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3010', '0.3431', '0.3526']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3322\n   æ ·æœ¬å†…æ–¹å·®: 0.000503\n   æ ·æœ¬1: Step 2: Write the sum in reverse, which is equivalent to reversing the order of addition, allowing us to combine like terms and simplify the expression.\n   æ ·æœ¬2: Step 2: The sum of the first n triangular numbers can be written as the sum of the first n numbers multiplied by the sum of the first n numbers.\n   æ ·æœ¬3: Step 2: The sum of the first n triangular numbers can be written as the sum of the first n terms of the arithmetic sequence 1, 3, 6, 10,..., which can be written in reverse as n, (n-1) + 1, (n-2) + 2,..., 2 + 3, 1 + 4.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.3953', 'path_logical_deduction': '0.3678', 'path_conceptual_analysis': '0.3322'}\n   è·¨è·¯å¾„æ–¹å·®: 0.000667\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test4 step_2 æœ€ä½³åŒ¹é…: è¯¦ç»†è®¡ç®—è·¯å¾„ (ç›¸ä¼¼åº¦: 0.3953)\n\nğŸ” ã€è·³æ­¥ step_3 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3702', '0.3601', '0.3736']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3679\n   æ ·æœ¬å†…æ–¹å·®: 0.000033\n   æ ·æœ¬1: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =? Write the sum in reverse order:... + nÂ³ + (n-1)Â³ + nÂ² +... + 2Â³ + 1Â³.\n   æ ·æœ¬2: Step 2: Write the sum in reverse, Tâ‚™ = n(n+1)/2 = 1 + 3 + 6 +... + nÂ².\n   æ ·æœ¬3: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =? Write the sum in reverse order: nÂ³ + (n-1)Â³ +... + 2Â³ + 1Â³.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3693', '0.3601', '0.3778']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3691\n   æ ·æœ¬å†…æ–¹å·®: 0.000052\n   æ ·æœ¬1: Step 2: The sum of the first n triangular numbers can be written as the sum of the first n numbers, in reverse order, multiplied by the first n numbers.\n   æ ·æœ¬2: Step 2: The sum of the first n triangular numbers can be written as the sum of the first n numbers, but in reverse order, since each term is the sum of consecutive integers starting from 1.\n   æ ·æœ¬3: Step 2: The sum of the first n triangular numbers can be written in reverse as (1 + 2 +... + n) + (1 + 3 +... + n) +... + (1 + n) = 1(1 + n) + 2(1 + n - 1) +... + n(1).\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.2925', '0.2999', '0.2811']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.2912\n   æ ·æœ¬å†…æ–¹å·®: 0.000060\n   æ ·æœ¬1: Step 2: Write the sum in reverse, which allows us to pair the terms and cancel out the variables, facilitating the subsequent simplification of the expression.\n   æ ·æœ¬2: Step 2: The sum of the cubes can be written in reverse by reversing the order of the terms, which allows us to group the terms in a way that facilitates the calculation.\n   æ ·æœ¬3: Step 2: This step relies on the principle of reversing the order of terms, which is a fundamental property of algebraic expressions, allowing us to explore alternative representations and relationships.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.3679', 'path_logical_deduction': '0.3691', 'path_conceptual_analysis': '0.2912'}\n   è·¨è·¯å¾„æ–¹å·®: 0.001329\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test4 step_3 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.3691)\n\nğŸ” ã€è·³æ­¥ step_5 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3891', '0.3648', '0.3805']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3781\n   æ ·æœ¬å†…æ–¹å·®: 0.000101\n   æ ·æœ¬1: Step 2: Write the sum in reverse, which is 1Â³ + (n-1)Â³ + (n-2)Â³ +... + 2Â³ + 1Â³, and simplify it to (nÂ³ + (n-1)Â³) + ((n-1)Â³ + (n-2)Â³) +... + 2Â³ + 1Â³.\n   æ ·æœ¬2: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =?... + nÂ³ + 3Â³ + 2Â³ + 1Â³.\n   æ ·æœ¬3: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =? Write the sum in reverse order: nÂ³ + (n-1)Â³ + (n-2)Â³ +... + 1Â³.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3698', '0.3632', '0.3749']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3693\n   æ ·æœ¬å†…æ–¹å·®: 0.000023\n   æ ·æœ¬1: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =... + nÂ³ + 3Â³ + 2Â³ + 1Â³.\n   æ ·æœ¬2: Step 2: The sum of the first n triangular numbers is the sum of the first n terms of the arithmetic series 1 + 2 + 3 +... + n, which is equal to n(n+1)/2.\n   æ ·æœ¬3: Step 2: Write the sum in reverse, 1Â³ + 2Â³ + 3Â³ +... + nÂ³ = (nÂ³ + (n-1)Â³ +... + 1Â³).\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.2850', '0.3040', '0.3473']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3121\n   æ ·æœ¬å†…æ–¹å·®: 0.000680\n   æ ·æœ¬1: Step 2: This step relies on the principle of rearrangement, where the original sum is rewritten in reverse to identify patterns and relationships between terms.\n   æ ·æœ¬2: Step 2: This step relies on the principle of reversing the order of terms, which allows us to reveal the underlying pattern and structure of the triangular numbers. By writing the sum in reverse, we can potentially uncover a connection between the triangular numbers and the sequence of integers.\n   æ ·æœ¬3: Step 2: Write the sum in reverse, Tâ‚™ = 1Â³ + 2Â³ + 3Â³ +... + nÂ³, by reversing the order of the terms, Tâ‚™ = nÂ³ + (n-1)Â³ + (n-2)Â³ +... + 1Â³.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.3781', 'path_logical_deduction': '0.3693', 'path_conceptual_analysis': '0.3121'}\n   è·¨è·¯å¾„æ–¹å·®: 0.000856\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test4 step_5 æœ€ä½³åŒ¹é…: è¯¦ç»†è®¡ç®—è·¯å¾„ (ç›¸ä¼¼åº¦: 0.3781)\n\nğŸ” ã€è·³æ­¥ step_6 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3588', '0.3666', '0.3556']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3604\n   æ ·æœ¬å†…æ–¹å·®: 0.000021\n   æ ·æœ¬1: Step 2: Write the sum in reverse, i.e., 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =?Â³ + (n-1)Â³ + (n-2)Â³ +... + 2Â³ + 1Â³.\n   æ ·æœ¬2: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =?Â³ +... + nÂ³ + 3Â³ + 2Â³ + 1Â³.\n   æ ·æœ¬3: Step 2: Write the sum in reverse, that is, 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =?Â³ +?Â² +... +?Â³.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3635', '0.3629', '0.3703']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3656\n   æ ·æœ¬å†…æ–¹å·®: 0.000011\n   æ ·æœ¬1: Step 2: The sum of the first n triangular numbers can be written as the sum of the first n numbers in reverse, since the triangular numbers are formed by adding consecutive integers.\n   æ ·æœ¬2: Step 2: The sum of the first n triangular numbers is equal to the sum of the first n terms of the sequence 1Â³, 2Â³, 3Â³,..., nÂ³, when written in reverse.\n   æ ·æœ¬3: Step 2: The sum of the first n triangular numbers can be written as the sum of the first n integers, Tâ‚ + Tâ‚‚ +... + Tâ‚™, and the sum of the first n integers can be written in reverse order as (1 + 2 +... + n) + (n + (n-1) +... + 1).\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.2944', '0.3542', '0.3232']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3239\n   æ ·æœ¬å†…æ–¹å·®: 0.000596\n   æ ·æœ¬1: Step 2: Write the sum in reverse, which is a technique used to reveal patterns and symmetries in the sequence, allowing for the identification of a closed-form expression.\n   æ ·æœ¬2: Step 2: Write the sum of the cubes in reverse order, using the commutative property of addition to reorder the terms without changing their sum.\n   æ ·æœ¬3: Step 2: Write the sum of cubes in reverse, utilizing the commutative property of addition, which states that the order of addends does not change the sum, allowing us to rearrange the terms for a more revealing analysis.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.3604', 'path_logical_deduction': '0.3656', 'path_conceptual_analysis': '0.3239'}\n   è·¨è·¯å¾„æ–¹å·®: 0.000343\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test4 step_6 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.3656)\n\nğŸ” ã€è·³æ­¥ step_7 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3829', '0.3554', '0.3744']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3709\n   æ ·æœ¬å†…æ–¹å·®: 0.000132\n   æ ·æœ¬1: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ = (nÂ³ + (n-1)Â³ +... + 2Â³ + 1Â³).\n   æ ·æœ¬2: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =?Â³ +?Â³ +... + nÂ³ + 1Â³.\n   æ ·æœ¬3: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =? â†’ nÂ³ + (n-1)Â³ + (n-2)Â³ +... + 1Â³.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.3645', '0.3619', '0.3674']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.3646\n   æ ·æœ¬å†…æ–¹å·®: 0.000005\n   æ ·æœ¬1: Step 2: The sum in reverse is 15 + 10 + 6 + 3 + 1, which can be written as (1 + 2 + 3 +... + n) + (1 + 2 + 3 +... + (n-1)).\n   æ ·æœ¬2: Step 2: Write the sum in reverse: 1Â³ + 2Â³ + 3Â³ +... + nÂ³ =? +... + 3Â³ + 2Â³ + 1Â³.\n   æ ·æœ¬3: Step 2: Write the sum in reverse as (n + 1)Â³ + (n + 2)Â³ +... + 1Â³.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.2843', '0.3170', '0.2872']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.2962\n   æ ·æœ¬å†…æ–¹å·®: 0.000218\n   æ ·æœ¬1: Step 2: This step relies on the principle of reversing the order of summation, allowing us to rewrite the sum of cubes as a product of sums.\n   æ ·æœ¬2: Step 2: Write the sum in reverse, specifically the sum of the cubes of integers from n to 1, and recognize that this is equivalent to the original sum.\n   æ ·æœ¬3: Step 2: This step involves reversing the order of the summation to facilitate the algebraic manipulation, utilizing the associative property of addition to regroup the terms.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.3709', 'path_logical_deduction': '0.3646', 'path_conceptual_analysis': '0.2962'}\n   è·¨è·¯å¾„æ–¹å·®: 0.001145\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test4 step_7 æœ€ä½³åŒ¹é…: è¯¦ç»†è®¡ç®—è·¯å¾„ (ç›¸ä¼¼åº¦: 0.3709)\n\nå¤„ç†è¿›åº¦: 5/8\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test5 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test5.pkl\n  - æ­¥éª¤æ•°é‡: 4\n  - è·³æ­¥æ•°é‡: 0\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test5 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: Find the total amount Jane spent on the first three days.\n$1.50 + $3 + $4.50 = $9\n\nStep 2: To find the amount Jane spent on the last day, subtract the total amount spent from the initial amount.\nLet x be the initial amount. Then, x - $9 is the amount Jane spent on the last day.\n\nStep 3: Find...\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: none\nSKIPPED_CONTENT: none\nSKIPPING_LOCATION: none\n\nTest5: æ— è·³æ­¥ï¼ˆæ ¹æ®Judgeåˆ†æï¼‰\n\nå¤„ç†è¿›åº¦: 6/8\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test6 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test6.pkl\n  - æ­¥éª¤æ•°é‡: 4\n  - è·³æ­¥æ•°é‡: 1\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test6 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: Alabama's capital is Montgomery.\n\nStep 2: Alabama's state bird is Yellowhammer.\n\nStep 3: The Yellowhammer is a type of Warbler.\n\nFinal Answer: The state capital is Montgomery....\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: 1\nSKIPPED_CONTENT: Juneau\nSKIPPING_LOCATION: start\n\nğŸ” ã€è·³æ­¥ step_1 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7549', '0.8276', '0.8089']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7972\n   æ ·æœ¬å†…æ–¹å·®: 0.000949\n   æ ·æœ¬1: Step 1: Determine the state by analyzing the given information, noting that Juneau is the capital of Alaska, and Montgomery is the capital of Alabama.\n   æ ·æœ¬2: Step 1: Identify the state associated with the capital Juneau, which is Alaska, and note that Montgomery is not the capital of Alaska.\n   æ ·æœ¬3: Step 1: Identify the state with capital Juneau, which is Alaska, and note that it is not the given state capital Montgomery.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8715', '0.8767', '0.8755']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8746\n   æ ·æœ¬å†…æ–¹å·®: 0.000005\n   æ ·æœ¬1: Step 1: The given state capital is Montgomery, and Juneau is mentioned, so we can deduce that the state is Alabama, as Juneau is the capital of Alaska, which is a different state from Alabama.\n   æ ·æœ¬2: Step 1: Since Montgomery is the state capital, and Juneau is not mentioned as the capital, we can conclude that Juneau is not the state capital.\n   æ ·æœ¬3: Step 1: If the previous answer is Montgomery, and Juneau is not mentioned as the state capital, then it is likely that Juneau is the state capital of a different state.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8398', '0.7386', '0.8265']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8016\n   æ ·æœ¬å†…æ–¹å·®: 0.002017\n   æ ·æœ¬1: Step 1: This step employs the principle of spatial relationshipsâ€”by considering the location of Juneau, a city in the Pacific Northwest, we can deduce that the state capital being searched is not located in the eastern United States.\n   æ ·æœ¬2: Step 1: This step applies the principle of geographic association, leveraging the fact that Juneau is a city located in Alaska, which is known for its proximity to the Pacific Ocean.\n   æ ·æœ¬3: Step 1: This step applies the principle of pattern recognition, noting that the given state capital \"Montgomery\" is located in the southeastern United States, which increases the likelihood that the state capital with the name \"Juneau\" is located in a region with a similar climate or geographical feature.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.7972', 'path_logical_deduction': '0.8746', 'path_conceptual_analysis': '0.8016'}\n   è·¨è·¯å¾„æ–¹å·®: 0.001260\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test6 step_1 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.8746)\n\nå¤„ç†è¿›åº¦: 7/8\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test7 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test7.pkl\n  - æ­¥éª¤æ•°é‡: 10\n  - è·³æ­¥æ•°é‡: 1\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test7 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: Write the given expression as (33â¿ - 16â¿) - (28â¿ - 11â¿).\n\nStep 2: Factor the first term as (33â¿ - 16â¿) = (33â¿ - 16â¿ + 0).\n\nStep 3: Factor the second term as (28â¿ - 11â¿) = (28â¿ - 16â¿ + 8â¿).\n\nStep 4: Combine the two factored terms as (33â¿ - 16â¿) - (28â¿ - 11â¿) = (33â¿ - 16â¿ + 0) - (28â¿ - 16â¿ + 8...\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: 1\nSKIPPED_CONTENT: Expected Step 1: Factor 85 into prime factors: 85 = 5 Ã— 17 (coprime\nSKIPPING_LOCATION: start\n\nğŸ” ã€è·³æ­¥ step_1 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8565', '0.9048', '0.8162']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8592\n   æ ·æœ¬å†…æ–¹å·®: 0.001313\n   æ ·æœ¬1: Step 1: 85 = 5 Ã— 17, so 85 is the product of two coprime numbers, which means they share no common factors other than 1.\n   æ ·æœ¬2: Step 1: Factor 85 into prime factors: 85 = 5 Ã— 17, which are coprime.\n   æ ·æœ¬3: Step 1: Factor 85 into prime factors: 85 = 5 Ã— 17.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8411', '0.9156', '0.9080']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8882\n   æ ·æœ¬å†…æ–¹å·®: 0.001120\n   æ ·æœ¬1: Step 1: 85 can be factored into prime factors, 85 = 5 Ã— 17, which are coprime numbers.\n   æ ·æœ¬2: Step 1: 85 can be factorized into 5 Ã— 17, which are coprime, implying that for the expression 33â¿ - 16â¿ - 28â¿ + 11â¿ to be divisible by 85, it suffices to show that each term is divisible by 5 and 17 separately.\n   æ ·æœ¬3: Step 1: 85 can be factored into 5 Ã— 17, which are coprime, so we can analyze the divisibility of the expression by considering the divisibility by 5 and 17 separately.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8125', '0.8021', '0.8217']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.8121\n   æ ·æœ¬å†…æ–¹å·®: 0.000064\n   æ ·æœ¬1: Step 1: Factor 85 into its prime factors: 85 = 5 Ã— 17, which are coprime.\n   æ ·æœ¬2: Step 1: Factor 85 into prime factors: 85 = 5 Ã— 17, which are coprime numbers, meaning they do not share any common factors.\n   æ ·æœ¬3: Step 1: Factor 85 into prime factors: 85 = 5 Ã— 17, which are coprime.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.8592', 'path_logical_deduction': '0.8882', 'path_conceptual_analysis': '0.8121'}\n   è·¨è·¯å¾„æ–¹å·®: 0.000984\n   ğŸ“ è§£è¯»: æä½æ–¹å·® - è·³æ­¥çŠ¶æ€ä¸æ‰€æœ‰è·¯å¾„ç›¸ä¼¼åº¦æ¥è¿‘ï¼Œå¯èƒ½æ˜¯æ¨ç†ç©ºé—´ç‹­çª„æˆ–è·³æ­¥çŠ¶æ€ä¿¡æ¯æ³›åŒ–\n\nğŸ¯ Test7 step_1 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.8882)\n\nå¤„ç†è¿›åº¦: 8/8\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"âœ“ å·²ä¿å­˜ Test8 çš„hidden statesæ•°æ®åˆ° experiment1_hidden_states_Test8.pkl\n  - æ­¥éª¤æ•°é‡: 6\n  - è·³æ­¥æ•°é‡: 1\n  - hidden stateå½¢çŠ¶: (4096,)\n  - æå–å±‚ç´¢å¼•: -1\n\nã€Test8 éªŒè¯ç»“æœã€‘\næ¨¡å‹æ¨ç†è¿‡ç¨‹ï¼š\nStep 1: I am driving.\n\nStep 2: I am moving my vehicle.\n\nStep 3: I am using a mode of transportation.\n\nStep 4: I am likely on a road.\n\nStep 5: I am probably using a vehicle.\n\nFinal Answer: I am driving a vehicle on a road....\n\nLLMè·³æ­¥åˆ†æï¼š\nSKIPPED_STEPS: 1\nSKIPPED_CONTENT: hit on the brakes\nSKIPPING_LOCATION: start\n\nğŸ” ã€è·³æ­¥ step_1 è¯¦ç»†ç›¸ä¼¼åº¦åˆ†æã€‘\n------------------------------------------------------------\n\nğŸ“Š Path Detailed Calculations:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.7966', '0.6269', '0.8211']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7482\n   æ ·æœ¬å†…æ–¹å·®: 0.007458\n   æ ·æœ¬1: Step 1: As I am driving a vehicle on a road, I hit on the brakes to slow down or stop.\n   æ ·æœ¬2: Step 1: As I am driving a vehicle on a road, I need to adjust my speed to match the road conditions.\n   æ ·æœ¬3: Step 1: As I am driving a vehicle on a road, I hit the brakes to slow down or stop, which indicates I am controlling the vehicle's speed and movement.\n\nğŸ“Š Path Logical Deduction:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.9454', '0.9300', '0.9461']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.9405\n   æ ·æœ¬å†…æ–¹å·®: 0.000055\n   æ ·æœ¬1: Step 1: As I am driving a vehicle on a road, if I need to stop or slow down, then I will hit on the brakes.\n   æ ·æœ¬2: Step 1: If I am driving a vehicle on a road, then I may need to hit on the brakes to adjust my speed or stop at an intersection or traffic signal.\n   æ ·æœ¬3: Step 1: As I am driving a vehicle on a road, if I need to stop suddenly, then I would hit on the brakes.\n\nğŸ“Š Path Conceptual Analysis:\n   å•æ¬¡ç›¸ä¼¼åº¦: ['0.8198', '0.7094', '0.8198']\n   å¹³å‡ç›¸ä¼¼åº¦: 0.7830\n   æ ·æœ¬å†…æ–¹å·®: 0.002709\n   æ ·æœ¬1: Step 1: This step relies on the principle of control and agencyâ€”since I am driving, I must be exerting control over the vehicle's movement, which implies hitting on the brakes as a possible action.\n   æ ·æœ¬2: Step 1: This step relies on the principle of action observationâ€”focusing on the physical action of operating a vehicle, which includes controlling speed and trajectory, to identify the next step in the reasoning process.\n   æ ·æœ¬3: Step 1: This step relies on the principle of action sequencingâ€”identifying the subsequent action that typically follows a vehicle's operation, which is to control its speed, resulting in the need to hit on the brakes.\n\nğŸ¯ è·¨è·¯å¾„æ–¹å·®åˆ†æ:\n   å„è·¯å¾„å¹³å‡ç›¸ä¼¼åº¦: {'path_detailed_calculations': '0.7482', 'path_logical_deduction': '0.9405', 'path_conceptual_analysis': '0.7830'}\n   è·¨è·¯å¾„æ–¹å·®: 0.007001\n   ğŸ“ è§£è¯»: ä½æ–¹å·® - è·³æ­¥çŠ¶æ€å¯¹å¤šæ¡è·¯å¾„æœ‰å¼±åå¥½ï¼Œæ— æ˜æ˜¾å€¾å‘æ€§\n\nğŸ¯ Test8 step_1 æœ€ä½³åŒ¹é…: é€»è¾‘æ¨ç†è·¯å¾„ (ç›¸ä¼¼åº¦: 0.9405)\n\n============================================================\nå®éªŒä¸€æ•°æ®æ±‡æ€»\n============================================================\n\nTest1:\n  é—®é¢˜: If you enter a building from the east side...\n  æ¨ç†æ­¥éª¤: 5\n  è·³æ­¥: 1\n  è·³æ­¥è¯¦æƒ…: ['step_1']\n  æå–å±‚ç´¢å¼•: -1\n\nTest2:\n  é—®é¢˜: All bachelors have never married. John is a man who has never been married. Is John a bachelor?...\n  æ¨ç†æ­¥éª¤: 4\n  è·³æ­¥: 0\n  è·³æ­¥è¯¦æƒ…: []\n  æå–å±‚ç´¢å¼•: -1\n\nTest3:\n  é—®é¢˜: Prove that xÂ² â€“ 4x + 7 is positive for all values of x...\n  æ¨ç†æ­¥éª¤: 8\n  è·³æ­¥: 3\n  è·³æ­¥è¯¦æƒ…: ['step_2', 'step_4', 'step_5']\n  æå–å±‚ç´¢å¼•: -1\n\nTest4:\n  é—®é¢˜: Triangular numbers are numbers that can make a triangular dot pattern. Prove that the n-th triangula...\n  æ¨ç†æ­¥éª¤: 10\n  è·³æ­¥: 5\n  è·³æ­¥è¯¦æƒ…: ['step_2', 'step_3', 'step_5', 'step_6', 'step_7']\n  æå–å±‚ç´¢å¼•: -1\n\nTest5:\n  é—®é¢˜: Jane won the lottery and decided to spend some of the money. She spent $1.50 on the first day. She s...\n  æ¨ç†æ­¥éª¤: 4\n  è·³æ­¥: 0\n  è·³æ­¥è¯¦æƒ…: []\n  æå–å±‚ç´¢å¼•: -1\n\nTest6:\n  é—®é¢˜: Given the following state capitols Montgomery...\n  æ¨ç†æ­¥éª¤: 4\n  è·³æ­¥: 1\n  è·³æ­¥è¯¦æƒ…: ['step_1']\n  æå–å±‚ç´¢å¼•: -1\n\nTest7:\n  é—®é¢˜: Prove that 33â¿ - 16â¿ - 28â¿ + 11â¿ is divisible by 85 for all positive integers â‰¥ 2. Give me a step-by...\n  æ¨ç†æ­¥éª¤: 10\n  è·³æ­¥: 1\n  è·³æ­¥è¯¦æƒ…: ['step_1']\n  æå–å±‚ç´¢å¼•: -1\n\nTest8:\n  é—®é¢˜: If I am driving...\n  æ¨ç†æ­¥éª¤: 6\n  è·³æ­¥: 1\n  è·³æ­¥è¯¦æƒ…: ['step_1']\n  æå–å±‚ç´¢å¼•: -1\n\nğŸ‰ å·²ä¿å­˜å®Œæ•´çš„å®éªŒä¸€æ•°æ®ï¼Œå…± 8 ä¸ªæµ‹è¯•æ¡ˆä¾‹\n\nğŸ“Š å®éªŒä¸€æ•°æ®ç»Ÿè®¡:\n  æ€»æµ‹è¯•æ¡ˆä¾‹: 8\n  æ€»æ¨ç†æ­¥éª¤: 51\n  æ€»è·³æ­¥æ•°é‡: 12\n  æœ‰æ•ˆå¯¹æ¯”æ¬¡æ•°: 36\n  å¹³å‡ç›¸ä¼¼åº¦: 0.6168\n  å¹³å‡è·¨è·¯å¾„æ–¹å·®: 0.001336\n  æ–¹å·®æ•°æ®èŒƒå›´: [0.000257, 0.007001]\n\nğŸ‰ å®éªŒä¸€å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²ä¿å­˜\n\nğŸ“ ä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨:\n  experiment1_hidden_states_Test6.pkl (35.4 KB)\n  experiment1_hidden_states_Test1.pkl (43.5 KB)\n  experiment1_complete_data.pkl (443.1 KB)\n  experiment1_hidden_states_Test4.pkl (92.3 KB)\n  experiment1_hidden_states_Test8.pkl (51.3 KB)\n  experiment1_hidden_states_Test2.pkl (33.3 KB)\n  experiment1_hidden_states_Test7.pkl (84.9 KB)\n  experiment1_hidden_states_Test3.pkl (72.2 KB)\n  experiment1_hidden_states_Test5.pkl (34.2 KB)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}